{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAH_TheGuild final submission - Public .538 Private .532"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Bowl 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T22:51:18.277065Z",
     "start_time": "2020-01-24T22:51:18.273066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleaned up redundant or unused code from Seals-final.ipynb final submission. See that notebook for full executed notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/data-science-bowl-2019/train.csv\n",
      "/kaggle/input/data-science-bowl-2019/train_labels.csv\n",
      "/kaggle/input/data-science-bowl-2019/specs.csv\n",
      "/kaggle/input/data-science-bowl-2019/test.csv\n",
      "/kaggle/input/data-science-bowl-2019/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "#import mlflow              #MLFlow only used locally for tracking test results\n",
    "import sklearn\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    ")\n",
    "from catboost import CatBoostRegressor\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression,\n",
    "    SGDRegressor,\n",
    "    Ridge,\n",
    "    LinearRegression,\n",
    "    Lasso\n",
    ")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\"\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/data-science-bowl-2019/train.csv\n",
      "/kaggle/input/data-science-bowl-2019/train_labels.csv\n",
      "/kaggle/input/data-science-bowl-2019/specs.csv\n",
      "/kaggle/input/data-science-bowl-2019/test.csv\n",
      "/kaggle/input/data-science-bowl-2019/sample_submission.csv\n",
      "Reading train.csv file....\n",
      "Training.csv file have 11341042 rows and 11 columns\n",
      "Reading test.csv file....\n",
      "Test.csv file have 1156414 rows and 11 columns\n",
      "Reading train_labels.csv file....\n",
      "Train_labels.csv file have 17690 rows and 7 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b9280221af425e9168f3d0407059c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3614.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6b7b42056b43619dddaf3aa59a517a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "['categoricals.txt']"
      ],
      "text/plain": [
       "['categoricals.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "# %% [code]\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from catboost import CatBoostRegressor\n",
    "from matplotlib import pyplot\n",
    "import shap\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import gc\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 1000)\n",
    "\n",
    "# %% [markdown]\n",
    "# # Notes\n",
    "# * Check the distribution of the target variable of the out of folds score and the prediction distribution. A good model should more or less have the same distribution.\n",
    "\n",
    "# %% [code]\n",
    "def read_data():\n",
    "    print(\"Reading train.csv file....\")\n",
    "    train = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/train.csv\")\n",
    "    print(\n",
    "        \"Training.csv file have {} rows and {} columns\".format(\n",
    "            train.shape[0], train.shape[1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"Reading test.csv file....\")\n",
    "    test = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/test.csv\")\n",
    "    print(\n",
    "        \"Test.csv file have {} rows and {} columns\".format(test.shape[0], test.shape[1])\n",
    "    )\n",
    "\n",
    "    print(\"Reading train_labels.csv file....\")\n",
    "    train_labels = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/train_labels.csv\")\n",
    "    print(\n",
    "        \"Train_labels.csv file have {} rows and {} columns\".format(\n",
    "            train_labels.shape[0], train_labels.shape[1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return train, test, train_labels\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "def encode_title(train, test, train_labels):\n",
    "    # encode title\n",
    "    train[\"title_event_code\"] = list(\n",
    "        map(lambda x, y: str(x) + \"_\" + str(y), train[\"title\"], train[\"event_code\"])\n",
    "    )\n",
    "    test[\"title_event_code\"] = list(\n",
    "        map(lambda x, y: str(x) + \"_\" + str(y), test[\"title\"], test[\"event_code\"])\n",
    "    )\n",
    "    all_title_event_code = list(\n",
    "        set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique())\n",
    "    )\n",
    "    # make a list with all the unique 'titles' from the train and test set\n",
    "    list_of_user_activities = list(\n",
    "        set(train[\"title\"].unique()).union(set(test[\"title\"].unique()))\n",
    "    )\n",
    "    # make a list with all the unique 'event_code' from the train and test set\n",
    "    list_of_event_code = list(\n",
    "        set(train[\"event_code\"].unique()).union(set(test[\"event_code\"].unique()))\n",
    "    )\n",
    "    list_of_event_id = list(\n",
    "        set(train[\"event_id\"].unique()).union(set(test[\"event_id\"].unique()))\n",
    "    )\n",
    "    # make a list with all the unique worlds from the train and test set\n",
    "    list_of_worlds = list(\n",
    "        set(train[\"world\"].unique()).union(set(test[\"world\"].unique()))\n",
    "    )\n",
    "    # create a dictionary numerating the titles\n",
    "    activities_map = dict(\n",
    "        zip(list_of_user_activities, np.arange(len(list_of_user_activities)))\n",
    "    )\n",
    "    activities_labels = dict(\n",
    "        zip(np.arange(len(list_of_user_activities)), list_of_user_activities)\n",
    "    )\n",
    "    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "    assess_titles = list(\n",
    "        set(train[train[\"type\"] == \"Assessment\"][\"title\"].value_counts().index).union(\n",
    "            set(test[test[\"type\"] == \"Assessment\"][\"title\"].value_counts().index)\n",
    "        )\n",
    "    )\n",
    "    # replace the text titles with the number titles from the dict\n",
    "    train[\"title\"] = train[\"title\"].map(activities_map)\n",
    "    test[\"title\"] = test[\"title\"].map(activities_map)\n",
    "    train[\"world\"] = train[\"world\"].map(activities_world)\n",
    "    test[\"world\"] = test[\"world\"].map(activities_world)\n",
    "    train_labels[\"title\"] = train_labels[\"title\"].map(activities_map)\n",
    "    win_code = dict(\n",
    "        zip(\n",
    "            activities_map.values(), (4100 * np.ones(len(activities_map))).astype(\"int\")\n",
    "        )\n",
    "    )\n",
    "    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n",
    "    win_code[activities_map[\"Bird Measurer (Assessment)\"]] = 4110\n",
    "    # convert text into datetime\n",
    "    train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\n",
    "    test[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])\n",
    "\n",
    "    return (\n",
    "        train,\n",
    "        test,\n",
    "        train_labels,\n",
    "        win_code,\n",
    "        list_of_user_activities,\n",
    "        list_of_event_code,\n",
    "        activities_labels,\n",
    "        assess_titles,\n",
    "        list_of_event_id,\n",
    "        all_title_event_code,\n",
    "    )\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "# this is the function that convert the raw data into processed features\n",
    "def get_data(user_sample, test_set=False):\n",
    "    \"\"\"\n",
    "    The user_sample is a DataFrame from train or test where the only one \n",
    "    installation_id is filtered\n",
    "    And the test_set parameter is related with the labels processing, that is only requered\n",
    "    if test_set=False\n",
    "    \"\"\"\n",
    "    # Constants and parameters declaration\n",
    "    last_activity = 0\n",
    "\n",
    "    user_activities_count = {\"Clip\": 0, \"Activity\": 0, \"Assessment\": 0, \"Game\": 0}\n",
    "\n",
    "    # new features: time spent in each activity\n",
    "    last_session_time_sec = 0\n",
    "    accuracy_groups = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "    all_assessments = []\n",
    "    accumulated_accuracy_group = 0\n",
    "    accumulated_accuracy = 0\n",
    "    accumulated_correct_attempts = 0\n",
    "    accumulated_uncorrect_attempts = 0\n",
    "    accumulated_actions = 0\n",
    "    counter = 0\n",
    "    time_first_activity = float(user_sample[\"timestamp\"].values[0])\n",
    "    durations = []\n",
    "    last_accuracy_title = {\"acc_\" + title: -1 for title in assess_titles}\n",
    "    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n",
    "    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n",
    "    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()}\n",
    "    title_event_code_count: Dict[str, int] = {\n",
    "        t_eve: 0 for t_eve in all_title_event_code\n",
    "    }\n",
    "\n",
    "    # last features\n",
    "    sessions_count = 0\n",
    "\n",
    "    # itarates through each session of one instalation_id\n",
    "    for i, session in user_sample.groupby(\"game_session\", sort=False):\n",
    "        # i = game_session_id\n",
    "        # session is a DataFrame that contain only one game_session\n",
    "\n",
    "        # get some sessions information\n",
    "        session_type = session[\"type\"].iloc[0]\n",
    "        session_title = session[\"title\"].iloc[0]\n",
    "        session_title_text = activities_labels[session_title]\n",
    "        game_session = session[\"game_session\"].iloc[0]\n",
    "\n",
    "        # for each assessment, and only this kind off session, the features below are processed\n",
    "        # and a register are generated\n",
    "        if (session_type == \"Assessment\") & (test_set or len(session) > 1):\n",
    "            # search for event_code 4100, that represents the assessments trial\n",
    "            all_attempts = session.query(f\"event_code == {win_code[session_title]}\")\n",
    "            # then, check the numbers of wins and the number of losses\n",
    "            true_attempts = all_attempts[\"event_data\"].str.contains(\"true\").sum()\n",
    "            false_attempts = all_attempts[\"event_data\"].str.contains(\"false\").sum()\n",
    "            # copy a dict to use as feature template, it's initialized with some itens:\n",
    "            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "            features = user_activities_count.copy()\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            features.update(event_code_count.copy())\n",
    "            features.update(event_id_count.copy())\n",
    "            features.update(title_count.copy())\n",
    "            features.update(title_event_code_count.copy())\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            features[\"installation_session_count\"] = sessions_count\n",
    "\n",
    "            variety_features = [\n",
    "                (\"var_event_code\", event_code_count),\n",
    "                (\"var_event_id\", event_id_count),\n",
    "                (\"var_title\", title_count),\n",
    "                (\"var_title_event_code\", title_event_code_count),\n",
    "            ]\n",
    "\n",
    "            for name, dict_counts in variety_features:\n",
    "                arr = np.array(list(dict_counts.values()))\n",
    "                features[name] = np.count_nonzero(arr)\n",
    "\n",
    "            # get installation_id for aggregated features\n",
    "            features[\"installation_id\"] = session[\"installation_id\"].iloc[-1]\n",
    "            features[\"game_session\"] = game_session\n",
    "            # add title as feature, remembering that title represents the name of the game\n",
    "            features[\"session_title\"] = session[\"title\"].iloc[0]\n",
    "            # the 4 lines below add the feature of the history of the trials of this player\n",
    "            # this is based on the all time attempts so far, at the moment of this assessment\n",
    "            features[\"accumulated_correct_attempts\"] = accumulated_correct_attempts\n",
    "            features[\"accumulated_uncorrect_attempts\"] = accumulated_uncorrect_attempts\n",
    "            accumulated_correct_attempts += true_attempts\n",
    "            accumulated_uncorrect_attempts += false_attempts\n",
    "            # the time spent in the app so far\n",
    "            if durations == []:\n",
    "                features[\"duration_mean\"] = 0\n",
    "                features[\"duration_std\"] = 0\n",
    "            else:\n",
    "                features[\"duration_mean\"] = np.mean(durations)\n",
    "                features[\"duration_std\"] = np.std(durations)\n",
    "            durations.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n",
    "            # the accurace is the all time wins divided by the all time attempts\n",
    "            features[\"accumulated_accuracy\"] = (\n",
    "                accumulated_accuracy / counter if counter > 0 else 0\n",
    "            )\n",
    "            accuracy = (\n",
    "                true_attempts / (true_attempts + false_attempts)\n",
    "                if (true_attempts + false_attempts) != 0\n",
    "                else 0\n",
    "            )\n",
    "            accumulated_accuracy += accuracy\n",
    "            last_accuracy_title[\"acc_\" + session_title_text] = accuracy\n",
    "            # a feature of the current accuracy categorized\n",
    "            # it is a counter of how many times this player was in each accuracy group\n",
    "            if accuracy == 0:\n",
    "                features[\"accuracy_group\"] = 0\n",
    "            elif accuracy == 1:\n",
    "                features[\"accuracy_group\"] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features[\"accuracy_group\"] = 2\n",
    "            else:\n",
    "                features[\"accuracy_group\"] = 1\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[features[\"accuracy_group\"]] += 1\n",
    "            # mean of the all accuracy groups of this player\n",
    "            features[\"accumulated_accuracy_group\"] = (\n",
    "                accumulated_accuracy_group / counter if counter > 0 else 0\n",
    "            )\n",
    "            accumulated_accuracy_group += features[\"accuracy_group\"]\n",
    "            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n",
    "            features[\"accumulated_actions\"] = accumulated_actions\n",
    "\n",
    "            # there are some conditions to allow this features to be inserted in the datasets\n",
    "            # if it's a test set, all sessions belong to the final dataset\n",
    "            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n",
    "            # that means, must exist an event_code 4100 or 4110\n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "            elif true_attempts + false_attempts > 0:\n",
    "                all_assessments.append(features)\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "        sessions_count += 1\n",
    "        # this piece counts how many actions was made in each event_code so far\n",
    "        def update_counters(counter: dict, col: str):\n",
    "            num_of_session_count = Counter(session[col])\n",
    "            for k in num_of_session_count.keys():\n",
    "                x = k\n",
    "                if col == \"title\":\n",
    "                    x = activities_labels[k]\n",
    "                counter[x] += num_of_session_count[k]\n",
    "            return counter\n",
    "\n",
    "        event_code_count = update_counters(event_code_count, \"event_code\")\n",
    "        event_id_count = update_counters(event_id_count, \"event_id\")\n",
    "        title_count = update_counters(title_count, \"title\")\n",
    "        title_event_code_count = update_counters(\n",
    "            title_event_code_count, \"title_event_code\"\n",
    "        )\n",
    "\n",
    "        # counts how many actions the player has done so far, used in the feature of the same name\n",
    "        accumulated_actions += len(session)\n",
    "        if last_activity != session_type:\n",
    "            user_activities_count[session_type] += 1\n",
    "            last_activitiy = session_type\n",
    "\n",
    "    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n",
    "    if test_set:\n",
    "        return all_assessments[-1]\n",
    "    # in the train_set, all assessments goes to the dataset\n",
    "    return all_assessments\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "# Compile the training and testing data \n",
    "def get_train_and_test(train, test):\n",
    "    compiled_train = []\n",
    "    compiled_test = []\n",
    "    assessment_sessions_by_instid = {}\n",
    "    # Loop through each train installation id\n",
    "    for ins_id, user_sample in tqdm(\n",
    "        train.groupby(\"installation_id\", sort=False),\n",
    "        total=train[\"installation_id\"].nunique(),\n",
    "    ):\n",
    "        compiled_train += get_data(user_sample, test_set=False)\n",
    "    reduce_train = pd.DataFrame(compiled_train)\n",
    "    del compiled_train\n",
    "    # Loop through each test installation id\n",
    "    for ins_id, user_sample in tqdm(\n",
    "        test.groupby(\"installation_id\", sort=False),\n",
    "        total=test[\"installation_id\"].nunique(),\n",
    "    ):\n",
    "        test_data = get_data(user_sample, test_set=True)\n",
    "        compiled_test.append(test_data)\n",
    "    reduce_test = pd.DataFrame(compiled_test)\n",
    "    del compiled_test\n",
    "    categoricals = [\"session_title\"]\n",
    "    return reduce_train, reduce_test, categoricals\n",
    "\n",
    "\n",
    "# Remove elements that aren't needed\n",
    "def remove_dead_weight(df, train_labels, test_set=False):\n",
    "    data_df = pd.DataFrame(df).copy()\n",
    "    data_df = data_df[data_df.world != \"NONE\"]\n",
    "\n",
    "    # Filter out only the installation ids with assessments\n",
    "    keep_id = data_df[data_df.type == \"Assessment\"][\n",
    "        [\"installation_id\"]\n",
    "    ].drop_duplicates()\n",
    "    data_df = pd.merge(data_df, keep_id, on=\"installation_id\", how=\"inner\")\n",
    "\n",
    "    # Filter out installation ids with more than 4000 event code counts\n",
    "    #df_grouped = data_df.groupby(\"installation_id\")[\"event_id\"].count()\n",
    "    #keep_count_ids = df_grouped[df_grouped < 6000]\n",
    "\n",
    "    #data_df = data_df[data_df.installation_id.isin(keep_count_ids.index)]\n",
    "\n",
    "    # If training set then make sure the installation ids are in the labels and remove assements not in the labels\n",
    "    if test_set == False:\n",
    "        data_df.reset_index()\n",
    "        data_df = data_df[\n",
    "            data_df.installation_id.isin(train_labels.installation_id.unique())\n",
    "        ]\n",
    "        assessments = data_df[data_df.type == \"Assessment\"]\n",
    "        assessments = assessments[\n",
    "            ~assessments.game_session.isin(train_labels.game_session)\n",
    "        ]\n",
    "        data_df = data_df[~data_df.game_session.isin(assessments.game_session)]\n",
    "        data_df.reset_index()\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "# read data\n",
    "train, test, train_labels = read_data()\n",
    "\n",
    "# %% [code]\n",
    "# remove unwanted data\n",
    "train = remove_dead_weight(train, train_labels, test_set=False)\n",
    "test = remove_dead_weight(test, train_labels, test_set=True)\n",
    "\n",
    "# %% [code]\n",
    "# get usefull dict with maping encode\n",
    "train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(\n",
    "    train, test, train_labels\n",
    ")\n",
    "\n",
    "# %% [code]\n",
    "# tranform function to get the train and test set\n",
    "reduce_train, reduce_test, categoricals = get_train_and_test(train, test)\n",
    "\n",
    "# %% [code]\n",
    "# Delete train and terst to free up resources\n",
    "del train\n",
    "del test\n",
    "\n",
    "# Save off the train/test data so local testing can skip the long compilation process\n",
    "reduce_train.to_csv(\"reduce_train.csv\")\n",
    "reduce_test.to_csv(\"reduce_test.csv\")\n",
    "joblib.dump(categoricals, 'categoricals.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop alphanumeric and target columns\n",
    "cols_to_drop = [\"game_session\", \"installation_id\", \"accuracy_group\"]\n",
    "\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "kappa_scorer = make_scorer(cohen_kappa_score, weights=\"quadratic\")\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score, cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# get prediction\n",
    "# this takes regression outputs and rounds them to integers by identifying distribution of accuracy groups in training data\n",
    "def get_class_pred(pred, train_t):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for regression outputs\n",
    "    \"\"\"\n",
    "    dist = Counter(train_t[\"accuracy_group\"])\n",
    "    for k in dist:\n",
    "        dist[k] /= len(train_t)\n",
    "\n",
    "    acum = 0\n",
    "    bound = {}\n",
    "    for i in range(3):\n",
    "        acum += dist[i]\n",
    "        bound[i] = np.percentile(pred, acum * 100)\n",
    "\n",
    "    def classify(x):\n",
    "        if x <= bound[0]:\n",
    "            return 0\n",
    "        elif x <= bound[1]:\n",
    "            return 1\n",
    "        elif x <= bound[2]:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    y_pred = np.array(list(map(classify, pred)))\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def make_submission(preds, train_df):\n",
    "    \"\"\" Format final predictions in Kaggle-acceptable submission format. \"\"\"\n",
    "    preds = get_class_pred(preds, train_df)\n",
    "    # assert len(preds)==1000\n",
    "    sample = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/sample_submission.csv\")\n",
    "    submission = pd.DataFrame()\n",
    "    submission[\"installation_id\"] = sample[\"installation_id\"]\n",
    "    submission[\"accuracy_group\"] = preds\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create metric function to use with vecstack - https://github.com/vecxoz/vecstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_metric(y_true, y_pred):\n",
    "    \"\"\" Create function to use with vecstack library - allows for kappa scoring as cross-validation metric\"\"\"\n",
    "    y_pred = get_class_pred(y_pred, reduce_train)\n",
    "    qwk = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "    return qwk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate variety of regression models for use in vecstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMRegressor()\n",
    "rfr = RandomForestRegressor()\n",
    "cbr = CatBoostRegressor(**{\n",
    "            'loss_function': 'MultiRMSE',\n",
    "        'task_type': \"CPU\",\n",
    "        'iterations': 1860,\n",
    "        'depth': 6,\n",
    "        'early_stopping_rounds': 300,\n",
    "        'l2_leaf_reg': 2,\n",
    "        'rsm': 1,\n",
    "        'bootstrap_type': 'Bayesian',\n",
    "        'bagging_temperature': 1,\n",
    "        'random_seed': 42,\n",
    "        'learning_rate': 0.04,\n",
    "        'eval_metric': 'MultiRMSE',\n",
    "        'silent':True}\n",
    ")\n",
    "gbr = GradientBoostingRegressor()\n",
    "abr = AdaBoostRegressor()\n",
    "lvr = LinearSVR()\n",
    "lr = LinearRegression()\n",
    "rr = Ridge()\n",
    "\n",
    "models = [cbr, lgbm, rfr, gbr, abr, lr, rr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vecstack import stacking\n",
    "\n",
    "mms = MinMaxScaler()   #scaling wasn't used in final... only seemed to impact a neural net that didn't get used in the end\n",
    "\n",
    "#  Read in and scale data\n",
    "\n",
    "# If running for submission use these 3 lines:\n",
    "X_train = reduce_train.drop(cols_to_drop, axis=1, errors=\"ignore\")\n",
    "y_train = reduce_train.accuracy_group\n",
    "X_test = reduce_test.drop(cols_to_drop, axis=1, errors=\"ignore\")\n",
    "\n",
    "# If testing locally, use these line:\n",
    "# X = reduce_train.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "# y = reduce_train.accuracy_group\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#    X, y, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "#X_train, X_test = mms.fit_transform(X_train), mms.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a stack train, and stack test with our initial list of models. Cross-val scores are printed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [regression]\n",
      "metric:       [my_metric]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [7]\n",
      "\n",
      "model  0:     [CatBoostRegressor]\n",
      "    fold  0:  [0.60177028]\n",
      "    fold  1:  [0.58580674]\n",
      "    fold  2:  [0.57356928]\n",
      "    fold  3:  [0.60488308]\n",
      "    ----\n",
      "    MEAN:     [0.59150735] + [0.01263437]\n",
      "    FULL:     [0.59130900]\n",
      "\n",
      "model  1:     [LGBMRegressor]\n",
      "    fold  0:  [0.59773612]\n",
      "    fold  1:  [0.58647548]\n",
      "    fold  2:  [0.57084494]\n",
      "    fold  3:  [0.59935598]\n",
      "    ----\n",
      "    MEAN:     [0.58860313] + [0.01138986]\n",
      "    FULL:     [0.59025280]\n",
      "\n",
      "model  2:     [RandomForestRegressor]\n",
      "    fold  0:  [0.55619342]\n",
      "    fold  1:  [0.52471821]\n",
      "    fold  2:  [0.50179736]\n",
      "    fold  3:  [0.54523689]\n",
      "    ----\n",
      "    MEAN:     [0.53198647] + [0.02077113]\n",
      "    FULL:     [0.53415615]\n",
      "\n",
      "model  3:     [GradientBoostingRegressor]\n",
      "    fold  0:  [0.58909148]\n",
      "    fold  1:  [0.57145990]\n",
      "    fold  2:  [0.56065809]\n",
      "    fold  3:  [0.58475876]\n",
      "    ----\n",
      "    MEAN:     [0.57649206] + [0.01121518]\n",
      "    FULL:     [0.57663492]\n",
      "\n",
      "model  4:     [AdaBoostRegressor]\n",
      "    fold  0:  [0.51146129]\n",
      "    fold  1:  [0.47207703]\n",
      "    fold  2:  [0.44310152]\n",
      "    fold  3:  [0.50386440]\n",
      "    ----\n",
      "    MEAN:     [0.48262606] + [0.02718460]\n",
      "    FULL:     [0.49965643]\n",
      "\n",
      "model  5:     [LinearRegression]\n",
      "    fold  0:  [0.37268734]\n",
      "    fold  1:  [0.36601315]\n",
      "    fold  2:  [0.38989148]\n",
      "    fold  3:  [0.40817495]\n",
      "    ----\n",
      "    MEAN:     [0.38419173] + [0.01635920]\n",
      "    FULL:     [0.38447606]\n",
      "\n",
      "model  6:     [Ridge]\n",
      "    fold  0:  [0.37254327]\n",
      "    fold  1:  [0.36773477]\n",
      "    fold  2:  [0.38989148]\n",
      "    fold  3:  [0.40945043]\n",
      "    ----\n",
      "    MEAN:     [0.38490499] + [0.01639339]\n",
      "    FULL:     [0.38494134]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "S_train, S_test = stacking(\n",
    "    models,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    regression=True,\n",
    "    mode=\"oof_pred_bag\",\n",
    "    needs_proba=False,\n",
    "    save_dir=None,\n",
    "    metric=my_metric,\n",
    "    n_folds=4,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit final model on S_train, y_train. This second-level model trains on all the former model outputs and creates the final prediction on S_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "model = model.fit(S_train, y_train)\n",
    "y_pred = model.predict(S_test)\n",
    "#y_pred = get_class_pred(y_pred, reduce_train)\n",
    "#print('Final prediction score: [%.8f]' % my_metric(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>installation_id</th>\n",
       "      <th>accuracy_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00abaee7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01242218</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>017c5718</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01a44906</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01bc6cb6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>fee254cf</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>ff57e602</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>ffc73fb2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>ffe00ca8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>ffe774cc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    installation_id  accuracy_group\n",
       "0          00abaee7               3\n",
       "1          01242218               3\n",
       "2          017c5718               3\n",
       "3          01a44906               3\n",
       "4          01bc6cb6               3\n",
       "..              ...             ...\n",
       "995        fee254cf               3\n",
       "996        ff57e602               0\n",
       "997        ffc73fb2               3\n",
       "998        ffe00ca8               0\n",
       "999        ffe774cc               1\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_submission(y_pred, reduce_train)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "06774d2e28c74775aa0da1fdf35d1248": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e36ae2312abb4585a8580b4d7b613dbb",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_72424b6a16b04000a2698057e102597d",
       "value": 1000
      }
     },
     "10b9280221af425e9168f3d0407059c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_63cee6984eaf47c592f3729960b757ef",
        "IPY_MODEL_ad27a1cb7b6d4e7fa24700a9a3dad7b3"
       ],
       "layout": "IPY_MODEL_ebc2fdcda3994b509d96be96f96828c4"
      }
     },
     "139cbd5ebbf049a591747591f488eac5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1a6b7b42056b43619dddaf3aa59a517a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_06774d2e28c74775aa0da1fdf35d1248",
        "IPY_MODEL_49bc798fee4e4144b7d2e7a81fc3872a"
       ],
       "layout": "IPY_MODEL_436a6d613c7848b0aab3ccb596462b5d"
      }
     },
     "4131aa1b4da84758838aad7d9dac2175": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "436a6d613c7848b0aab3ccb596462b5d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "49bc798fee4e4144b7d2e7a81fc3872a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_94ce58db3b4f4d58a846bab391ccece4",
       "placeholder": "​",
       "style": "IPY_MODEL_139cbd5ebbf049a591747591f488eac5",
       "value": " 1000/1000 [01:23&lt;00:00, 12.04it/s]"
      }
     },
     "5548b75bb19a492cbdc16a10c6801779": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "63cee6984eaf47c592f3729960b757ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_efe84a30a53b4d54893d80b339f76cf8",
       "max": 3614,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e06d50d3d21440f2bfd6511ddb94054b",
       "value": 3614
      }
     },
     "72424b6a16b04000a2698057e102597d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "94ce58db3b4f4d58a846bab391ccece4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ad27a1cb7b6d4e7fa24700a9a3dad7b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4131aa1b4da84758838aad7d9dac2175",
       "placeholder": "​",
       "style": "IPY_MODEL_5548b75bb19a492cbdc16a10c6801779",
       "value": " 3614/3614 [06:09&lt;00:00,  9.77it/s]"
      }
     },
     "e06d50d3d21440f2bfd6511ddb94054b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "e36ae2312abb4585a8580b4d7b613dbb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ebc2fdcda3994b509d96be96f96828c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "efe84a30a53b4d54893d80b339f76cf8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
