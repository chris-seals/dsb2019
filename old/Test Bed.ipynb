{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from matplotlib import pyplot\n",
    "import shap\n",
    "import random\n",
    "from collections import Counter\n",
    "from random import choice\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from pprint import pprint\n",
    "from bayes_opt import BayesianOptimization\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "import gc\n",
    "import json\n",
    "pd.set_option('display.max_columns', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_qwk_lgb_regr(y_true, y_pred, reduce_train, is_classifier):\n",
    "\n",
    "    if not is_classifier:\n",
    "        dist = Counter(reduce_train['accuracy_group'])\n",
    "        for k in dist:\n",
    "            dist[k] /= len(reduce_train)\n",
    "\n",
    "        acum = 0\n",
    "        bound = {}\n",
    "        for i in range(3):\n",
    "            acum += dist[i]\n",
    "            bound[i] = np.percentile(y_pred, acum * 100)\n",
    "\n",
    "        def classify(x):\n",
    "            if x <= bound[0]:\n",
    "                return 0\n",
    "            elif x <= bound[1]:\n",
    "                return 1\n",
    "            elif x <= bound[2]:\n",
    "                return 2\n",
    "            else:\n",
    "                return 3\n",
    "\n",
    "        y_pred = np.array(list(map(classify, y_pred))).reshape(y_true.shape)\n",
    "\n",
    "    return 'cappa', cohen_kappa_score(y_true, y_pred, weights='quadratic'), True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_title(train, test, train_labels):\n",
    "    # encode title\n",
    "    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n",
    "    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n",
    "    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n",
    "    # make a list with all the unique 'titles' from the train and test set\n",
    "    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n",
    "    # make a list with all the unique 'event_code' from the train and test set\n",
    "    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n",
    "    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n",
    "    # make a list with all the unique worlds from the train and test set\n",
    "    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n",
    "    # create a dictionary numerating the titles\n",
    "    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
    "    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
    "    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(\n",
    "        set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n",
    "    # replace the text titles with the number titles from the dict\n",
    "    train['title'] = train['title'].map(activities_map)\n",
    "    test['title'] = test['title'].map(activities_map)\n",
    "    train['world'] = train['world'].map(activities_world)\n",
    "    test['world'] = test['world'].map(activities_world)\n",
    "    train_labels['title'] = train_labels['title'].map(activities_map)\n",
    "    win_code = dict(zip(activities_map.values(), (4100 * np.ones(len(activities_map))).astype('int')))\n",
    "    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n",
    "    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n",
    "    # convert text into datetime\n",
    "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "\n",
    "    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test(train, test, assess_titles, list_of_event_code, list_of_event_id, activities_labels,\n",
    "                       all_title_event_code, win_code):\n",
    "    compiled_train = []\n",
    "    compiled_test = []\n",
    "    assessment_sessions_by_instid = {}\n",
    "\n",
    "    # Loop through each train installation id\n",
    "    for ins_id, user_sample in tqdm(train.groupby('installation_id', sort=False),\n",
    "                                    total=train['installation_id'].nunique()):\n",
    "        compiled_train += get_data(user_sample, assess_titles, list_of_event_code, list_of_event_id, activities_labels,\n",
    "                                   all_title_event_code, win_code, test_set=False)\n",
    "\n",
    "    reduce_train = pd.DataFrame(compiled_train)\n",
    "    del compiled_train\n",
    "\n",
    "    # Loop through each test installation id\n",
    "    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False),\n",
    "                                    total=test['installation_id'].nunique()):\n",
    "        test_data = get_data(user_sample, assess_titles, list_of_event_code, list_of_event_id, activities_labels,\n",
    "                             all_title_event_code, win_code, test_set=True)\n",
    "        compiled_test.append(test_data)\n",
    "\n",
    "    reduce_test = pd.DataFrame(compiled_test)\n",
    "    del compiled_test\n",
    "\n",
    "    categoricals = ['session_title']\n",
    "\n",
    "    return reduce_train, reduce_test, categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(user_sample, assess_titles, list_of_event_code, list_of_event_id, activities_labels, all_title_event_code,\n",
    "             win_code, test_set=False):\n",
    "    '''\n",
    "    The user_sample is a DataFrame from train or test where the only one\n",
    "    installation_id is filtered\n",
    "    And the test_set parameter is related with the labels processing, that is only requered\n",
    "    if test_set=False\n",
    "    '''\n",
    "    # Constants and parameters declaration\n",
    "    last_activity = 0\n",
    "\n",
    "    user_activities_count = {'Clip': 0, 'Activity': 0, 'Assessment': 0, 'Game': 0}\n",
    "\n",
    "    # new features: time spent in each activity\n",
    "    last_session_time_sec = 0\n",
    "    accuracy_groups = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "    all_assessments = []\n",
    "    accumulated_accuracy_group = 0\n",
    "    accumulated_accuracy = 0\n",
    "    accumulated_correct_attempts = 0\n",
    "    accumulated_uncorrect_attempts = 0\n",
    "    accumulated_actions = 0\n",
    "    counter = 0\n",
    "    time_first_activity = float(user_sample['timestamp'].values[0])\n",
    "    durations = []\n",
    "    durations_game = []\n",
    "    durations_activity = []\n",
    "    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n",
    "    last_game_time_title = {'lgt_' + title: 0 for title in assess_titles}\n",
    "    ac_game_time_title = {'agt_' + title: 0 for title in assess_titles}\n",
    "    ac_true_attempts_title = {'ata_' + title: 0 for title in assess_titles}\n",
    "    ac_false_attempts_title = {'afa_' + title: 0 for title in assess_titles}\n",
    "    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n",
    "    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n",
    "    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()}\n",
    "    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n",
    "    session_count = 0\n",
    "\n",
    "    # itarates through each session of one instalation_id\n",
    "    for i, session in user_sample.groupby('game_session', sort=False):\n",
    "        # i = game_session_id\n",
    "        # session is a DataFrame that contain only one game_session\n",
    "\n",
    "        # get some sessions information\n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_title_text = activities_labels[session_title]\n",
    "        game_session = session['game_session'].iloc[0]\n",
    "\n",
    "        # for each assessment, and only this kind off session, the features below are processed\n",
    "        # and a register are generated\n",
    "        if (session_type == 'Assessment') & (test_set or len(session) > 1):\n",
    "            # search for event_code 4100, that represents the assessments trial\n",
    "            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n",
    "            # then, check the numbers of wins and the number of losses\n",
    "            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n",
    "            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n",
    "            # copy a dict to use as feature template, it's initialized with some itens:\n",
    "            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "            features = user_activities_count.copy()\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            features.update(event_code_count.copy())\n",
    "            features.update(title_count.copy())\n",
    "            features.update(event_id_count.copy())\n",
    "            features.update(title_event_code_count.copy())\n",
    "            features.update(last_game_time_title.copy())\n",
    "            features.update(ac_game_time_title.copy())\n",
    "            features.update(ac_true_attempts_title.copy())\n",
    "            features.update(ac_false_attempts_title.copy())\n",
    "            features['installation_session_count'] = session_count\n",
    "\n",
    "            variety_features = [('var_event_code', event_code_count),\n",
    "                                ('var_event_id', event_id_count),\n",
    "                                ('var_title', title_count),\n",
    "                                ('var_title_event_code', title_event_code_count)]\n",
    "\n",
    "            for name, dict_counts in variety_features:\n",
    "                arr = np.array(list(dict_counts.values()))\n",
    "                features[name] = np.count_nonzero(arr)\n",
    "\n",
    "            # get installation_id for aggregated features\n",
    "            features['installation_id'] = session['installation_id'].iloc[-1]\n",
    "            features['game_session'] = game_session\n",
    "            # add title as feature, remembering that title represents the name of the game\n",
    "            features['session_title'] = session['title'].iloc[0]\n",
    "            # the 4 lines below add the feature of the history of the trials of this player\n",
    "            # this is based on the all time attempts so far, at the moment of this assessment\n",
    "            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
    "            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n",
    "            accumulated_correct_attempts += true_attempts\n",
    "            accumulated_uncorrect_attempts += false_attempts\n",
    "\n",
    "            # ----------------------------------------------\n",
    "            ac_true_attempts_title['ata_' + session_title_text] += true_attempts\n",
    "            ac_false_attempts_title['afa_' + session_title_text] += false_attempts\n",
    "\n",
    "            last_game_time_title['lgt_' + session_title_text] = session['game_time'].iloc[-1]\n",
    "            ac_game_time_title['agt_' + session_title_text] += session['game_time'].iloc[-1]\n",
    "            # ----------------------------------------------\n",
    "\n",
    "            # the time spent in the app so far\n",
    "            if durations == []:\n",
    "                features['duration_mean'] = 0\n",
    "                features['duration_std'] = 0\n",
    "                features['last_duration'] = 0\n",
    "                features['duration_max'] = 0\n",
    "            else:\n",
    "                features['duration_mean'] = np.mean(durations)\n",
    "                features['duration_std'] = np.std(durations)\n",
    "                features['last_duration'] = durations[-1]\n",
    "                features['duration_max'] = np.max(durations)\n",
    "            durations.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n",
    "\n",
    "            if durations_game == []:\n",
    "                features['duration_game_mean'] = 0\n",
    "                features['duration_game_std'] = 0\n",
    "                features['game_last_duration'] = 0\n",
    "                features['game_max_duration'] = 0\n",
    "            else:\n",
    "                features['duration_game_mean'] = np.mean(durations_game)\n",
    "                features['duration_game_std'] = np.std(durations_game)\n",
    "                features['game_last_duration'] = durations_game[-1]\n",
    "                features['game_max_duration'] = np.max(durations_game)\n",
    "\n",
    "            if durations_activity == []:\n",
    "                features['duration_activity_mean'] = 0\n",
    "                features['duration_activity_std'] = 0\n",
    "                features['game_activity_duration'] = 0\n",
    "                features['game_activity_max'] = 0\n",
    "            else:\n",
    "                features['duration_activity_mean'] = np.mean(durations_activity)\n",
    "                features['duration_activity_std'] = np.std(durations_activity)\n",
    "                features['game_activity_duration'] = durations_activity[-1]\n",
    "                features['game_activity_max'] = np.max(durations_activity)\n",
    "\n",
    "            # the accuracy is the all time wins divided by the all time attempts\n",
    "            features['accumulated_accuracy'] = accumulated_accuracy / counter if counter > 0 else 0\n",
    "            accuracy = true_attempts / (true_attempts + false_attempts) if (true_attempts + false_attempts) != 0 else 0\n",
    "            accumulated_accuracy += accuracy\n",
    "            last_accuracy_title['acc_' + session_title_text] = accuracy\n",
    "            # a feature of the current accuracy categorized\n",
    "            # it is a counter of how many times this player was in each accuracy group\n",
    "            if accuracy == 0:\n",
    "                features['accuracy_group'] = 0\n",
    "            elif accuracy == 1:\n",
    "                features['accuracy_group'] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features['accuracy_group'] = 2\n",
    "            else:\n",
    "                features['accuracy_group'] = 1\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[features['accuracy_group']] += 1\n",
    "            # mean of the all accuracy groups of this player\n",
    "            features['accumulated_accuracy_group'] = accumulated_accuracy_group / counter if counter > 0 else 0\n",
    "            accumulated_accuracy_group += features['accuracy_group']\n",
    "            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n",
    "            features['accumulated_actions'] = accumulated_actions\n",
    "\n",
    "            # there are some conditions to allow this features to be inserted in the datasets\n",
    "            # if it's a test set, all sessions belong to the final dataset\n",
    "            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n",
    "            # that means, must exist an event_code 4100 or 4110\n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "            elif true_attempts + false_attempts > 0:\n",
    "                all_assessments.append(features)\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "        if session_type == 'Game':\n",
    "            durations_game.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n",
    "\n",
    "        if session_type == 'Activity':\n",
    "            durations_activity.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n",
    "\n",
    "        session_count += 1\n",
    "\n",
    "        # this piece counts how many actions was made in each event_code so far\n",
    "        def update_counters(counter: dict, col: str):\n",
    "            num_of_session_count = Counter(session[col])\n",
    "            for k in num_of_session_count.keys():\n",
    "                x = k\n",
    "                if col == 'title':\n",
    "                    x = activities_labels[k]\n",
    "                counter[x] += num_of_session_count[k]\n",
    "            return counter\n",
    "\n",
    "        event_code_count = update_counters(event_code_count, \"event_code\")\n",
    "        event_id_count = update_counters(event_id_count, \"event_id\")\n",
    "        title_count = update_counters(title_count, 'title')\n",
    "        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n",
    "\n",
    "        # counts how many actions the player has done so far, used in the feature of the same name\n",
    "        accumulated_actions += len(session)\n",
    "        if last_activity != session_type:\n",
    "            user_activities_count[session_type] += 1\n",
    "            last_activitiy = session_type\n",
    "\n",
    "            # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n",
    "    if test_set:\n",
    "        if len(all_assessments) > 0:\n",
    "            return all_assessments[-1]\n",
    "        else:\n",
    "            return all_assessments[0]\n",
    "    # in the train_set, all assessments goes to the dataset\n",
    "    return all_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dead_weight(df, train_labels, test_set=False):\n",
    "    df = df[df['world'] != 'NONE']\n",
    "\n",
    "    # filtering by ids that took assessments\n",
    "    ids_w_assessments = df[df['type'] == 'Assessment']['installation_id'].drop_duplicates()\n",
    "    df = df[df['installation_id'].isin(ids_w_assessments)]\n",
    "\n",
    "    # If training set then make sure the installation ids are in the labels and remove assements not in the labels\n",
    "    if test_set == False:\n",
    "        # drop data whose installation does not contain any scored assessments in train_labels\n",
    "        df = df[df['installation_id'].isin(train_labels['installation_id'].unique())]\n",
    "\n",
    "        assessments = df[df.type == 'Assessment']\n",
    "        assessments = assessments[~assessments.game_session.isin(train_labels.game_session)]\n",
    "        df = df[~df.game_session.isin(assessments.game_session)]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stract_hists(feature, train, test, adjust=False, plot=False):\n",
    "    n_bins = 10\n",
    "    train_data = train[feature]\n",
    "    test_data = test[feature]\n",
    "    if adjust:\n",
    "        test_data *= train_data.mean() / test_data.mean()\n",
    "    perc_90 = np.percentile(train_data, 95)\n",
    "    train_data = np.clip(train_data, 0, perc_90)\n",
    "    test_data = np.clip(test_data, 0, perc_90)\n",
    "    train_hist = np.histogram(train_data, bins=n_bins)[0] / len(train_data)\n",
    "    test_hist = np.histogram(test_data, bins=n_bins)[0] / len(test_data)\n",
    "    msre = mean_squared_error(train_hist, test_hist)\n",
    "    if plot:\n",
    "        print(msre)\n",
    "        plt.bar(range(n_bins), train_hist, color='blue', alpha=0.5)\n",
    "        plt.bar(range(n_bins), test_hist, color='red', alpha=0.5)\n",
    "        plt.show()\n",
    "    return msre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction\n",
    "def get_class_pred(pred, train_t):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for lgb.\n",
    "    \"\"\"\n",
    "    dist = Counter(train_t['accuracy_group'])\n",
    "    for k in dist:\n",
    "        dist[k] /= len(train_t)\n",
    "\n",
    "    acum = 0\n",
    "    bound = {}\n",
    "    for i in range(3):\n",
    "        acum += dist[i]\n",
    "        bound[i] = np.percentile(pred, acum * 100)\n",
    "\n",
    "    def classify(x):\n",
    "        if x <= bound[0]:\n",
    "            return 0\n",
    "        elif x <= bound[1]:\n",
    "            return 1\n",
    "        elif x <= bound[2]:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    y_pred = np.array(list(map(classify, pred)))\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(reduce_train, reduce_test, cols_to_drop):\n",
    "    for df in [reduce_train, reduce_test]:\n",
    "        df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count')\n",
    "        df['installation_duration_mean'] = df.groupby(['installation_id'])['duration_mean'].transform('mean')\n",
    "        # df['installation_duration_std'] = df.groupby(['installation_id'])['duration_mean'].transform('std')\n",
    "        df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n",
    "\n",
    "        df['sum_event_code_count'] = df[\n",
    "            [2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020,\n",
    "             4021,\n",
    "             4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080,\n",
    "             2035,\n",
    "             2040, 4090, 4220, 4095]].sum(axis=1)\n",
    "\n",
    "        df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform(\n",
    "            'mean')\n",
    "        # df['installation_event_code_count_std'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('std')\n",
    "\n",
    "    features = reduce_train.loc[\n",
    "        (reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns  # delete useless columns\n",
    "    features = [x for x in features if x not in cols_to_drop]\n",
    "\n",
    "    return reduce_train, reduce_test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Catagoricals\n",
    "def create_cats(train, test, categoricals, activities_labels):\n",
    "    tmp_train = train.copy()\n",
    "    tmp_test = test.copy()\n",
    "    tmp_train['session_title'] = tmp_train['session_title'].astype(CategoricalDtype(categories=activities_labels))\n",
    "    tmp_test['session_title'] = tmp_test['session_title'].astype(CategoricalDtype(categories=activities_labels))\n",
    "    # tmp_train['world'] = tmp_train['world'].astype(CategoricalDtype(categories=[0, 1, 2]))\n",
    "    # tmp_test['world'] = tmp_test['world'].astype(CategoricalDtype(categories=[0, 1, 2]))\n",
    "    train_cats = pd.get_dummies(tmp_train[categoricals], prefix=categoricals)\n",
    "    test_cats = pd.get_dummies(tmp_test[categoricals], prefix=categoricals)\n",
    "\n",
    "    tmp_train = tmp_train.drop(categoricals, axis=1)\n",
    "    tmp_test = tmp_test.drop(categoricals, axis=1)\n",
    "\n",
    "    tmp_train = pd.concat([tmp_train, train_cats], axis=1, sort=False)\n",
    "    tmp_test = pd.concat([tmp_test, test_cats], axis=1, sort=False)\n",
    "\n",
    "    return tmp_train, tmp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min Max\n",
    "def create_min_max(train, test, categoricals, cols_to_drop):\n",
    "    tmp_train = train.copy()\n",
    "    tmp_test = test.copy()\n",
    "    scalars = [x for x in tmp_train.columns if x not in (categoricals + cols_to_drop)]\n",
    "    tmp_train[scalars] = tmp_train[scalars].apply(lambda x: (x - x.min()) / (x.max() - x.min())).fillna(0)\n",
    "    tmp_test[scalars] = tmp_test[scalars].apply(lambda x: (x - x.min()) / (x.max() - x.min())).fillna(0)\n",
    "\n",
    "    return tmp_train, tmp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Model(object):\n",
    "\n",
    "    def __init__(self, train_df, test_df, features, params, reduce_train, reduce_test, \n",
    "                 categoricals=[], n_splits=5, verbose=True, is_classifier=False, is_lgb=False):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.features = features\n",
    "        self.n_splits = n_splits\n",
    "        self.categoricals = categoricals\n",
    "        self.target = 'accuracy_group'\n",
    "        self.cv = self.get_cv()\n",
    "        self.verbose = verbose\n",
    "        self.params = params\n",
    "        self.is_classifier = is_classifier\n",
    "        self.is_lgb = is_lgb\n",
    "        self.y_pred, self.score, self.model, self.oof_pred = self.fit(reduce_train, reduce_test)\n",
    "\n",
    "    def train_model(self, train_set, val_set):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_cv(self):\n",
    "        cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
    "        return cv.split(self.train_df, self.train_df[self.target])\n",
    "\n",
    "    def get_params(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def convert_x(self, x):\n",
    "        return x\n",
    "\n",
    "    def fit(self, reduce_train, reduce_test):\n",
    "        oof_pred = np.zeros((len(reduce_train),))\n",
    "        y_pred = np.zeros((len(reduce_test),))\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv):\n",
    "            try:\n",
    "                x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[\n",
    "                    val_idx]\n",
    "                y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n",
    "                train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n",
    "                model = self.train_model(train_set, val_set)\n",
    "                conv_x_val = self.convert_x(x_val)\n",
    "                tmp_pred = model.predict(conv_x_val)\n",
    "                if self.is_lgb and self.is_classifier:\n",
    "                    tmp_pred = np.argmax(tmp_pred, axis=1)\n",
    "                oof_pred[val_idx] = tmp_pred.reshape(oof_pred[val_idx].shape)\n",
    "                x_test = self.convert_x(self.test_df[self.features])\n",
    "                tmp_y_pred = model.predict(x_test)\n",
    "                if self.is_lgb and self.is_classifier:\n",
    "                    tmp_y_pred = np.argmax(tmp_y_pred, axis=1)\n",
    "                y_pred += tmp_y_pred.reshape(y_pred.shape) / self.n_splits\n",
    "                print('Partial score of fold {} is: {}'.format(fold, eval_qwk_lgb_regr(y_val, tmp_pred, \n",
    "                                                                                       reduce_train, self.is_classifier)[1]))\n",
    "                _, loss_score, _ = eval_qwk_lgb_regr(self.train_df[self.target], oof_pred, \n",
    "                                                     reduce_train, self.is_classifier)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('Error training: val_idx = ', val_idx)\n",
    "        if self.verbose:\n",
    "            print('Our oof cohen kappa score is: ', loss_score)\n",
    "        del self.train_df, self.test_df, self.cv\n",
    "        gc.collect()\n",
    "        return y_pred, loss_score, model, oof_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xgb_Model(Base_Model):\n",
    "\n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        return xgb.train(self.params, train_set,\n",
    "                         num_boost_round=667, evals=[(train_set, 'train'), (val_set, 'val')],\n",
    "                         verbose_eval=verbosity, early_stopping_rounds=100)\n",
    "\n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = xgb.DMatrix(x_train, y_train)\n",
    "        val_set = xgb.DMatrix(x_val, y_val)\n",
    "        return train_set, val_set\n",
    "\n",
    "    def convert_x(self, x):\n",
    "        return xgb.DMatrix(x)\n",
    "\n",
    "\n",
    "class Catb_Model(Base_Model):\n",
    "\n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        clf = CatBoostRegressor(**self.params)\n",
    "        clf.fit(train_set['X'],\n",
    "                train_set['y'],\n",
    "                eval_set=(val_set['X'], val_set['y']),\n",
    "                verbose=verbosity, )\n",
    "        return clf\n",
    "\n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "    \n",
    "class Catb_Class_Model(Base_Model):\n",
    "\n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        clf = CatBoostClassifier(**self.params)\n",
    "        clf.fit(train_set['X'],\n",
    "                train_set['y'],\n",
    "                eval_set=(val_set['X'], val_set['y']),\n",
    "                verbose=verbosity)\n",
    "        return clf\n",
    "\n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "\n",
    "\n",
    "class RF_Model(Base_Model):\n",
    "\n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        if self.is_classifier:\n",
    "            rf = RandomForestClassifier(**self.params)\n",
    "        else:\n",
    "            rf = RandomForestRegressor(**self.params)\n",
    "            \n",
    "        rf.fit(train_set['X'], train_set['y'])\n",
    "        return rf\n",
    "\n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "    \n",
    "\n",
    "class KNN_Model(Base_Model):\n",
    "\n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        if self.is_classifier:\n",
    "            knn = KNeighborsClassifier(**self.params)\n",
    "        else:\n",
    "            knn = KNeighborsRegressor(**self.params)\n",
    "        knn.fit(train_set['X'], train_set['y'])\n",
    "        return knn\n",
    "\n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "    \n",
    "\n",
    "class Lgb_Model(Base_Model):\n",
    "\n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        return lgb.train(self.params, train_set, valid_sets=[train_set, val_set], verbose_eval=verbosity)\n",
    "\n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = lgb.Dataset(x_train, y_train)\n",
    "        val_set = lgb.Dataset(x_val, y_val)\n",
    "        return train_set, val_set\n",
    "\n",
    "class Nn_Model(Base_Model):\n",
    "\n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(train_set['X'].shape[1],)),\n",
    "            tf.keras.layers.Dense(200, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(100, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(50, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(25, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1, activation='relu')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-4), loss='mse')\n",
    "        print(model.summary())\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True,\n",
    "                                                       verbose=1)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "        model.fit(train_set['X'],\n",
    "                  train_set['y'],\n",
    "                  validation_data=(val_set['X'], val_set['y']),\n",
    "                  epochs=100,\n",
    "                  callbacks=[save_best, early_stop])\n",
    "        model.load_weights('nn_model.w8')\n",
    "        return model\n",
    "\n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "\n",
    "    def get_params(self):\n",
    "        return None\n",
    "    \n",
    "class Nn_Class_Model(Base_Model):\n",
    "\n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(train_set['X'].shape[1],)),\n",
    "            tf.keras.layers.Dense(100, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(50, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(25, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(4, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "        print(model.summary())\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True,\n",
    "                                                       verbose=1)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "        model.fit(train_set['X'],\n",
    "                  train_set['y'],\n",
    "                  validation_data=(val_set['X'], val_set['y']),\n",
    "                  epochs=100,\n",
    "                  callbacks=[save_best, early_stop])\n",
    "        model.load_weights('nn_model.w8')\n",
    "        return model\n",
    "\n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "\n",
    "    def get_params(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    print('Reading train.csv file....')\n",
    "    train = pd.read_csv('./data/train.csv')\n",
    "    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n",
    "\n",
    "    print('Reading test.csv file....')\n",
    "    test = pd.read_csv('./data/test.csv')\n",
    "    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n",
    "\n",
    "    print('Reading train_labels.csv file....')\n",
    "    train_labels = pd.read_csv('./data/train_labels.csv')\n",
    "    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n",
    "\n",
    "    return train, test, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train.csv file....\n",
      "Training.csv file have 11341042 rows and 11 columns\n",
      "Reading test.csv file....\n",
      "Test.csv file have 1156414 rows and 11 columns\n",
      "Reading train_labels.csv file....\n",
      "Train_labels.csv file have 17690 rows and 7 columns\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "train, test, train_labels = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unwanted data\n",
    "train = remove_dead_weight(train, train_labels, test_set=False)\n",
    "test = remove_dead_weight(test, train_labels, test_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get usefull dict with maping encode\n",
    "train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(train, test, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c8a406ed4945439d170ad70f95dc85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3614.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de64ff057d2c4557911f38a498a8adf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tranform function to get the train and test set\n",
    "reduce_train, reduce_test, categoricals = get_train_and_test(train, test, assess_titles, list_of_event_code, list_of_event_id, activities_labels, all_title_event_code, win_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2223"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete train and test to free up resources\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the accuracy group vals\n",
    "reduce_train = reduce_train.set_index('game_session')\n",
    "train_labels = train_labels.set_index('game_session')\n",
    "reduce_train.update(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "reduce_train = reduce_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cols to drop for training\n",
    "cols_to_drop = ['accuracy_group', 'game_session', 'installation_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "os = SMOTE(random_state = 0)\n",
    "\n",
    "reduce_train_x, reduce_train_y =  os.fit_sample(reduce_train.drop(cols_to_drop, axis=1), reduce_train['accuracy_group'])\n",
    "\n",
    "# create dataframes from SMOTE analysis\n",
    "reduce_train_x = pd.DataFrame(data = reduce_train_x, columns = reduce_train.drop(cols_to_drop, axis=1).columns)\n",
    "reduce_train_y = pd.DataFrame(data = reduce_train_y, columns = ['accuracy_group'])\n",
    "reduce_train_x['accuracy_group'] = reduce_train_y\n",
    "reduce_train = reduce_train_x.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del reduce_train_x, reduce_train_y, train_labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_train['installation_id'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_train, reduce_test, features = preprocess(reduce_train, reduce_test, cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Cats\n",
    "reduce_train, reduce_test = create_cats(reduce_train, reduce_test, categoricals, activities_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_train, reduce_test = create_min_max(reduce_train, reduce_test, categoricals, cols_to_drop)\n",
    "reduce_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_train.columns]\n",
    "reduce_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n",
    "features = [x for x in features if x not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through and find high correlations. Add to remove list if so\n",
    "counter = 0\n",
    "to_remove = []\n",
    "for feat_a in features:\n",
    "    for feat_b in features:\n",
    "        if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n",
    "            c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]\n",
    "            if c > 0.995:\n",
    "                counter += 1\n",
    "                to_remove.append(feat_b)\n",
    "                #print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through again a look for features to remove\n",
    "to_exclude = [] \n",
    "ajusted_test = reduce_test.copy()\n",
    "for feature in ajusted_test.columns:\n",
    "    if feature not in (cols_to_drop + categoricals):\n",
    "        try:\n",
    "            data = reduce_train[feature]\n",
    "            train_mean = data.mean()\n",
    "            data = ajusted_test[feature] \n",
    "            test_mean = data.mean()\n",
    "            error = stract_hists(feature, train=reduce_train, test=reduce_test, adjust=True)\n",
    "            ajust_factor = train_mean / test_mean\n",
    "            if ajust_factor > 10 or ajust_factor < 0.1:# or error > 0.01:\n",
    "                to_exclude.append(feature)\n",
    "                #print(feature, train_mean, test_mean, error)\n",
    "            else:\n",
    "                ajusted_test[feature] *= ajust_factor\n",
    "        except:\n",
    "            to_exclude.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final feature list removing the unwanted ones\n",
    "features = [x for x in features if x not in (to_exclude + to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cols to drop for training\n",
    "cols_to_drop = ['accuracy_group', 'game_session', 'installation_id']\n",
    "categoricals = ['session_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial score of fold 0 is: 0.6947427925381571\n",
      "Partial score of fold 1 is: 0.6922555115884681\n",
      "Partial score of fold 2 is: 0.6914292175486205\n",
      "Partial score of fold 3 is: 0.6902204635387225\n",
      "Partial score of fold 4 is: 0.6809496890898813\n"
     ]
    }
   ],
   "source": [
    "# Random Forrest Classifier\n",
    "\n",
    "params = {'bootstrap': False, \n",
    "          'max_depth':59, \n",
    "          'max_features': 72, \n",
    "          'min_samples_leaf': 2, \n",
    "          'min_samples_split': 6, \n",
    "          'n_estimators': 100}\n",
    "\n",
    "rf_model = RF_Model(reduce_train, ajusted_test, features, params, reduce_train, ajusted_test, \n",
    "                          categoricals=categoricals, verbose=False, is_classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data:  0.6898883009994121\n"
     ]
    }
   ],
   "source": [
    "rf_train_pred = rf_model.oof_pred\n",
    "print('Accuracy on training data: ', rf_model.score)\n",
    "# Accuracy on training data:  0.5400765760009788 - Classifier\n",
    "# Accuracy on training data:  0.586369916234434 - Regressor\n",
    "# Accuracy on training data:  0.7094658767129483 - With SMOTE\n",
    "# Accuracy on training data:  0.6651401764771585 - with SMOTE Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial score of fold 0 is: 0.489359227817188\n",
      "Partial score of fold 1 is: 0.5059178463680669\n",
      "Partial score of fold 2 is: 0.4906122448979592\n",
      "Partial score of fold 3 is: 0.4889199906694658\n",
      "Partial score of fold 4 is: 0.4694873881204231\n"
     ]
    }
   ],
   "source": [
    "# K Nearest Classifier\n",
    "\n",
    "weights = 0.3491139618762451\n",
    "if weights >= 0 and weights < 1.0:\n",
    "    weights = 'uniform'\n",
    "else:\n",
    "    weights = 'distance'\n",
    "\n",
    "algorithm = 0.04441288498288465\n",
    "if algorithm >= 0 and algorithm < 1.0:\n",
    "    algorithm = 'ball_tree'\n",
    "elif algorithm >= 1 and algorithm < 2.0:\n",
    "    algorithm = 'kd_tree'\n",
    "elif algorithm >= 2 and algorithm < 3.0:\n",
    "    algorithm = 'brute'\n",
    "else:\n",
    "    algorithm = 'auto'\n",
    "\n",
    "params = {\n",
    "         'n_neighbors': int(19.544302888065488),\n",
    "        'weights': weights,\n",
    "        'algorithm': algorithm,\n",
    "        'leaf_size': int(29.702070879545722),\n",
    "        'p': int(2.986361352754792),\n",
    "        'n_jobs': -1\n",
    "}\n",
    "knn_model = KNN_Model(reduce_train, ajusted_test, features, params, reduce_train, ajusted_test, \n",
    "                      categoricals=categoricals, verbose=False, is_classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data:  0.4888640647908957\n"
     ]
    }
   ],
   "source": [
    "knn_train_pred = knn_model.oof_pred\n",
    "print('Accuracy on training data: ', knn_model.score)\n",
    "# Accuracy on training data:  0.4820109463573031 - Classifier\n",
    "# Accuracy on training data:  0.535143726742256 - Regressor\n",
    "# Accuracy on training data:  0.5574548037485966 - With SMOTE\n",
    "# Accuracy on training data:  0.4888640647908957 - with SMOTE Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.31967\tval-mlogloss:1.32612\n",
      "Multiple eval metrics have been passed: 'val-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.464104\tval-mlogloss:0.777055\n",
      "[200]\ttrain-mlogloss:0.271738\tval-mlogloss:0.694881\n",
      "[300]\ttrain-mlogloss:0.176011\tval-mlogloss:0.657615\n",
      "[400]\ttrain-mlogloss:0.12543\tval-mlogloss:0.639673\n",
      "[500]\ttrain-mlogloss:0.102664\tval-mlogloss:0.631439\n",
      "[600]\ttrain-mlogloss:0.091826\tval-mlogloss:0.628635\n",
      "[666]\ttrain-mlogloss:0.086872\tval-mlogloss:0.627012\n",
      "Partial score of fold 0 is: 0.6968639184397163\n",
      "[0]\ttrain-mlogloss:1.32018\tval-mlogloss:1.32442\n",
      "Multiple eval metrics have been passed: 'val-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.470686\tval-mlogloss:0.782342\n",
      "[200]\ttrain-mlogloss:0.270777\tval-mlogloss:0.707626\n",
      "[300]\ttrain-mlogloss:0.173312\tval-mlogloss:0.6717\n",
      "[400]\ttrain-mlogloss:0.125251\tval-mlogloss:0.657752\n",
      "[500]\ttrain-mlogloss:0.100312\tval-mlogloss:0.650232\n",
      "[600]\ttrain-mlogloss:0.090636\tval-mlogloss:0.64768\n",
      "[666]\ttrain-mlogloss:0.087252\tval-mlogloss:0.646989\n",
      "Partial score of fold 1 is: 0.6913689815842023\n",
      "[0]\ttrain-mlogloss:1.31746\tval-mlogloss:1.32776\n",
      "Multiple eval metrics have been passed: 'val-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.461817\tval-mlogloss:0.783112\n",
      "[200]\ttrain-mlogloss:0.26632\tval-mlogloss:0.709021\n",
      "[300]\ttrain-mlogloss:0.171765\tval-mlogloss:0.674776\n",
      "[400]\ttrain-mlogloss:0.122995\tval-mlogloss:0.660392\n",
      "[500]\ttrain-mlogloss:0.100995\tval-mlogloss:0.651947\n",
      "[600]\ttrain-mlogloss:0.090562\tval-mlogloss:0.649577\n",
      "[666]\ttrain-mlogloss:0.08599\tval-mlogloss:0.648363\n",
      "Partial score of fold 2 is: 0.6937207748830996\n",
      "[0]\ttrain-mlogloss:1.31957\tval-mlogloss:1.32944\n",
      "Multiple eval metrics have been passed: 'val-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.469042\tval-mlogloss:0.789364\n",
      "[200]\ttrain-mlogloss:0.271717\tval-mlogloss:0.710962\n",
      "[300]\ttrain-mlogloss:0.175634\tval-mlogloss:0.675649\n",
      "[400]\ttrain-mlogloss:0.124568\tval-mlogloss:0.658944\n",
      "[500]\ttrain-mlogloss:0.102153\tval-mlogloss:0.652539\n",
      "[600]\ttrain-mlogloss:0.09066\tval-mlogloss:0.649371\n",
      "[666]\ttrain-mlogloss:0.086592\tval-mlogloss:0.647885\n",
      "Partial score of fold 3 is: 0.6827915233551536\n",
      "[0]\ttrain-mlogloss:1.32127\tval-mlogloss:1.32857\n",
      "Multiple eval metrics have been passed: 'val-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.469961\tval-mlogloss:0.791002\n",
      "[200]\ttrain-mlogloss:0.274715\tval-mlogloss:0.71783\n",
      "[300]\ttrain-mlogloss:0.178996\tval-mlogloss:0.685588\n",
      "[400]\ttrain-mlogloss:0.128615\tval-mlogloss:0.667588\n",
      "[500]\ttrain-mlogloss:0.10461\tval-mlogloss:0.661759\n",
      "[600]\ttrain-mlogloss:0.092517\tval-mlogloss:0.658897\n",
      "[666]\ttrain-mlogloss:0.089018\tval-mlogloss:0.658086\n",
      "Partial score of fold 4 is: 0.6703485790408525\n",
      "Our oof cohen kappa score is:  0.6870184942607847\n"
     ]
    }
   ],
   "source": [
    "# XG Boost\n",
    "params_class = {\n",
    "            'colsample_bytree': 0.2,                 \n",
    "            'eta': 0.3,\n",
    "            'objective':'multi:softmax',\n",
    "            'num_class': 4,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 1,\n",
    "            'min_child_weight': 3,\n",
    "            'gamma': 0.25,\n",
    "            'eval_metric': 'mlogloss'\n",
    "         }\n",
    "\n",
    "params_reg = {\n",
    "            'colsample_bytree': 0.2,                 \n",
    "            'learning_rate': 0.01,\n",
    "            'objective':'reg:squarederror',\n",
    "            'max_depth': 6,\n",
    "            'subsample': 1,\n",
    "            'min_child_weight': 3,\n",
    "            'gamma': 0.25,\n",
    "            'n_estimators': 1400\n",
    "         }\n",
    "\n",
    "xgb_model = Xgb_Model(reduce_train, ajusted_test, features, params_reg, reduce_train, ajusted_test, \n",
    "                            categoricals=categoricals, verbose=False, is_classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data:  0.6870184942607847\n"
     ]
    }
   ],
   "source": [
    "xgb_train_pred = xgb_model.oof_pred\n",
    "print('Accuracy on training data: ', xgb_model.score)\n",
    "# Accuracy on training data:  0.603943020944038 - with Regressor\n",
    "# Accuracy on training data:  0.6287394007914076 - With SMOTE Regresor\n",
    "# Accuracy on training data:  0.6870184942607847 - with SMOTE Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial score of fold 0 is: 0.644002600498429\n",
      "Partial score of fold 1 is: 0.6354516863680728\n",
      "Partial score of fold 2 is: 0.6415747350205494\n",
      "Partial score of fold 3 is: 0.61964517524881\n",
      "Partial score of fold 4 is: 0.6210764750296625\n"
     ]
    }
   ],
   "source": [
    "# Catboost\n",
    "params_class = {\n",
    "            'loss_function': 'MultiClass',\n",
    "            'classes_count': 4,\n",
    "            'task_type': \"CPU\",\n",
    "            'iterations': 1860,\n",
    "            'depth': 6,\n",
    "            'early_stopping_rounds': 300,\n",
    "            'l2_leaf_reg': 2,\n",
    "            'rsm': 1,\n",
    "            'bootstrap_type': 'Bayesian',\n",
    "            'bagging_temperature': 1,\n",
    "            'random_seed': 42,\n",
    "            'learning_rate': 0.04,\n",
    "            'eval_metric': 'MultiClass'\n",
    "        }\n",
    "\n",
    "params_reg = {\n",
    "            'loss_function': 'MultiRMSE',\n",
    "            'task_type': \"CPU\",\n",
    "            'iterations': 1860,\n",
    "            'depth': 6,\n",
    "            'early_stopping_rounds': 300,\n",
    "            'l2_leaf_reg': 2,\n",
    "            'rsm': 1,\n",
    "            'bootstrap_type': 'Bayesian',\n",
    "            'bagging_temperature': 1,\n",
    "            'random_seed': 42,\n",
    "            'learning_rate': 0.04\n",
    "        }\n",
    "\n",
    "cat_model = Catb_Class_Model(reduce_train, ajusted_test, features, params_reg, reduce_train, ajusted_test, \n",
    "                             categoricals=categoricals, verbose=False, is_classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data:  0.6323386206747311\n"
     ]
    }
   ],
   "source": [
    "cat_train_pred = cat_model.oof_pred\n",
    "print('Accuracy on training data: ', cat_model.score)\n",
    "# Accuracy on training data:  0.6028335173880344\n",
    "# Accuracy on training data:  0.6501526286037309 - With SMOTE Regressor\n",
    "# Accuracy on training data:  0.6323386206747311 - with SMOTE Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's multi_logloss: 1.0201\tvalid_1's multi_logloss: 1.06005\n",
      "[200]\ttraining's multi_logloss: 0.931783\tvalid_1's multi_logloss: 0.99381\n",
      "[300]\ttraining's multi_logloss: 0.852804\tvalid_1's multi_logloss: 0.940239\n",
      "[400]\ttraining's multi_logloss: 0.790431\tvalid_1's multi_logloss: 0.902415\n",
      "[500]\ttraining's multi_logloss: 0.732426\tvalid_1's multi_logloss: 0.870372\n",
      "[600]\ttraining's multi_logloss: 0.710666\tvalid_1's multi_logloss: 0.860039\n",
      "[700]\ttraining's multi_logloss: 0.672745\tvalid_1's multi_logloss: 0.840422\n",
      "[800]\ttraining's multi_logloss: 0.648423\tvalid_1's multi_logloss: 0.828825\n",
      "[900]\ttraining's multi_logloss: 0.615722\tvalid_1's multi_logloss: 0.812634\n",
      "[1000]\ttraining's multi_logloss: 0.593785\tvalid_1's multi_logloss: 0.802732\n",
      "Partial score of fold 0 is: 0.6501350621285791\n",
      "[100]\ttraining's multi_logloss: 1.02125\tvalid_1's multi_logloss: 1.05881\n",
      "[200]\ttraining's multi_logloss: 0.933296\tvalid_1's multi_logloss: 0.992198\n",
      "[300]\ttraining's multi_logloss: 0.853847\tvalid_1's multi_logloss: 0.93776\n",
      "[400]\ttraining's multi_logloss: 0.790896\tvalid_1's multi_logloss: 0.897764\n",
      "[500]\ttraining's multi_logloss: 0.732497\tvalid_1's multi_logloss: 0.866117\n",
      "[600]\ttraining's multi_logloss: 0.711592\tvalid_1's multi_logloss: 0.85556\n",
      "[700]\ttraining's multi_logloss: 0.67324\tvalid_1's multi_logloss: 0.834919\n",
      "[800]\ttraining's multi_logloss: 0.649447\tvalid_1's multi_logloss: 0.82373\n",
      "[900]\ttraining's multi_logloss: 0.616588\tvalid_1's multi_logloss: 0.807462\n",
      "[1000]\ttraining's multi_logloss: 0.594979\tvalid_1's multi_logloss: 0.797857\n",
      "Partial score of fold 1 is: 0.6441935483870967\n",
      "[100]\ttraining's multi_logloss: 1.01791\tvalid_1's multi_logloss: 1.06532\n",
      "[200]\ttraining's multi_logloss: 0.929251\tvalid_1's multi_logloss: 0.999851\n",
      "[300]\ttraining's multi_logloss: 0.850205\tvalid_1's multi_logloss: 0.947754\n",
      "[400]\ttraining's multi_logloss: 0.787719\tvalid_1's multi_logloss: 0.910149\n",
      "[500]\ttraining's multi_logloss: 0.729318\tvalid_1's multi_logloss: 0.878571\n",
      "[600]\ttraining's multi_logloss: 0.707576\tvalid_1's multi_logloss: 0.867325\n",
      "[700]\ttraining's multi_logloss: 0.668676\tvalid_1's multi_logloss: 0.847488\n",
      "[800]\ttraining's multi_logloss: 0.643149\tvalid_1's multi_logloss: 0.835211\n",
      "[900]\ttraining's multi_logloss: 0.610489\tvalid_1's multi_logloss: 0.821129\n",
      "[1000]\ttraining's multi_logloss: 0.588563\tvalid_1's multi_logloss: 0.811092\n",
      "Partial score of fold 2 is: 0.6414759909764745\n",
      "[100]\ttraining's multi_logloss: 1.01708\tvalid_1's multi_logloss: 1.06934\n",
      "[200]\ttraining's multi_logloss: 0.929641\tvalid_1's multi_logloss: 1.00509\n",
      "[300]\ttraining's multi_logloss: 0.848582\tvalid_1's multi_logloss: 0.950274\n",
      "[400]\ttraining's multi_logloss: 0.785358\tvalid_1's multi_logloss: 0.912792\n",
      "[500]\ttraining's multi_logloss: 0.728387\tvalid_1's multi_logloss: 0.881311\n",
      "[600]\ttraining's multi_logloss: 0.70622\tvalid_1's multi_logloss: 0.870221\n",
      "[700]\ttraining's multi_logloss: 0.668815\tvalid_1's multi_logloss: 0.85047\n",
      "[800]\ttraining's multi_logloss: 0.644719\tvalid_1's multi_logloss: 0.838207\n",
      "[900]\ttraining's multi_logloss: 0.611556\tvalid_1's multi_logloss: 0.821394\n",
      "[1000]\ttraining's multi_logloss: 0.589863\tvalid_1's multi_logloss: 0.810471\n",
      "Partial score of fold 3 is: 0.6343163538873995\n",
      "[100]\ttraining's multi_logloss: 1.01643\tvalid_1's multi_logloss: 1.06881\n",
      "[200]\ttraining's multi_logloss: 0.92815\tvalid_1's multi_logloss: 1.00533\n",
      "[300]\ttraining's multi_logloss: 0.848979\tvalid_1's multi_logloss: 0.952941\n",
      "[400]\ttraining's multi_logloss: 0.78532\tvalid_1's multi_logloss: 0.913704\n",
      "[500]\ttraining's multi_logloss: 0.727623\tvalid_1's multi_logloss: 0.88151\n",
      "[600]\ttraining's multi_logloss: 0.70625\tvalid_1's multi_logloss: 0.870495\n",
      "[700]\ttraining's multi_logloss: 0.668828\tvalid_1's multi_logloss: 0.850826\n",
      "[800]\ttraining's multi_logloss: 0.644642\tvalid_1's multi_logloss: 0.839683\n",
      "[900]\ttraining's multi_logloss: 0.612532\tvalid_1's multi_logloss: 0.824335\n",
      "[1000]\ttraining's multi_logloss: 0.59075\tvalid_1's multi_logloss: 0.813933\n",
      "Partial score of fold 4 is: 0.6228154819341696\n",
      "Our oof cohen kappa score is:  0.6385652248688622\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "params_class = {\n",
    "        'boosting_type': 'dart',\n",
    "        'feature_fraction': 0.7766778552692686,\n",
    "        'lambda_l1': 0.4958811953667753,\n",
    "        'lambda_l2': 0.08799041939480234,\n",
    "        'learning_rate': 0.06209127849529422,\n",
    "        'min_child_samples': 336,\n",
    "        'num_leaves': 39,\n",
    "        'subsample': 0.519326536607012,\n",
    "        'n_estimators': 1000,\n",
    "        'early_stopping_rounds': 50,\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 4,\n",
    "        'metric': 'multi_logloss'\n",
    "}\n",
    "\n",
    "params_reg = {\n",
    "        'boosting_type': 'dart',\n",
    "        'feature_fraction': 0.7766778552692686,\n",
    "        'lambda_l1': 0.4958811953667753,\n",
    "        'lambda_l2': 0.08799041939480234,\n",
    "        'learning_rate': 0.06209127849529422,\n",
    "        'min_child_samples': 336,\n",
    "        'num_leaves': 39,\n",
    "        'subsample': 0.519326536607012,\n",
    "        'n_estimators': 1000,\n",
    "        'early_stopping_rounds': 100\n",
    "}\n",
    "\n",
    "lgb_model = Lgb_Class_Model(reduce_train, ajusted_test, features, params_reg, reduce_train, ajusted_test, \n",
    "                      categoricals=categoricals, verbose=True, is_classifier=False, is_lgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data:  0.6385652248688622\n"
     ]
    }
   ],
   "source": [
    "lgb_train_pred = lgb_model.oof_pred\n",
    "print('Accuracy on training data: ', lgb_model.score)\n",
    "# Accuracy on training data:  0.6043009253169422\n",
    "# Accuracy on training data:  0.6612323346523459 - With SMOTE Regressor\n",
    "# Accuracy on training data:  0.6385652248688622 - with SMOTE Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_pred = rf_model.y_pred\n",
    "knn_test_pred = knn_model.y_pred\n",
    "xgb_test_pred = xgb_model.y_pred\n",
    "cat_test_pred = cat_model.y_pred\n",
    "lgb_test_pred = lgb_model.y_pred\n",
    "\n",
    "train_pred_df = pd.DataFrame()\n",
    "test_pred_df = pd.DataFrame()\n",
    "\n",
    "train_pred_df['rf_pred'] = rf_train_pred\n",
    "train_pred_df['knn_pred'] = knn_train_pred\n",
    "train_pred_df['xgb_pred'] = xgb_train_pred\n",
    "train_pred_df['cat_pred'] = cat_train_pred\n",
    "train_pred_df['lgb_pred'] = lgb_train_pred\n",
    "train_pred_df['installation_id'] = reduce_train['installation_id']\n",
    "train_pred_df['accuracy_group'] = reduce_train['accuracy_group']\n",
    "\n",
    "test_pred_df['rf_pred'] = rf_test_pred\n",
    "test_pred_df['knn_pred'] = knn_test_pred\n",
    "test_pred_df['xgb_pred'] = xgb_test_pred\n",
    "test_pred_df['cat_pred'] = cat_test_pred\n",
    "test_pred_df['lgb_pred'] = lgb_test_pred\n",
    "test_pred_df['installation_id'] = ajusted_test['installation_id']\n",
    "test_pred_df['accuracy_group'] = ajusted_test['accuracy_group']\n",
    "\n",
    "new_features = ['rf_pred', 'knn_pred', 'xgb_pred', 'cat_pred', 'lgb_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 100)               600       \n",
      "_________________________________________________________________\n",
      "layer_normalization_30 (Laye (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_31 (Laye (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_32 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 4)                 104       \n",
      "=================================================================\n",
      "Total params: 7,379\n",
      "Trainable params: 7,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 28304 samples, validate on 7076 samples\n",
      "Epoch 1/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 1.1488 - sparse_categorical_accuracy: 0.5071\n",
      "Epoch 00001: val_loss improved from inf to 0.85193, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 3s 90us/sample - loss: 1.1488 - sparse_categorical_accuracy: 0.5072 - val_loss: 0.8519 - val_sparse_categorical_accuracy: 0.7566\n",
      "Epoch 2/100\n",
      "27840/28304 [============================>.] - ETA: 0s - loss: 0.8488 - sparse_categorical_accuracy: 0.7244\n",
      "Epoch 00002: val_loss improved from 0.85193 to 0.73257, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 80us/sample - loss: 0.8485 - sparse_categorical_accuracy: 0.7247 - val_loss: 0.7326 - val_sparse_categorical_accuracy: 0.7677\n",
      "Epoch 3/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7880 - sparse_categorical_accuracy: 0.7490\n",
      "Epoch 00003: val_loss improved from 0.73257 to 0.72813, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7880 - sparse_categorical_accuracy: 0.7489 - val_loss: 0.7281 - val_sparse_categorical_accuracy: 0.7660\n",
      "Epoch 4/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7725 - sparse_categorical_accuracy: 0.7553\n",
      "Epoch 00004: val_loss improved from 0.72813 to 0.71821, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 81us/sample - loss: 0.7730 - sparse_categorical_accuracy: 0.7549 - val_loss: 0.7182 - val_sparse_categorical_accuracy: 0.7708\n",
      "Epoch 5/100\n",
      "28128/28304 [============================>.] - ETA: 0s - loss: 0.7640 - sparse_categorical_accuracy: 0.7576\n",
      "Epoch 00005: val_loss improved from 0.71821 to 0.71265, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7645 - sparse_categorical_accuracy: 0.7575 - val_loss: 0.7127 - val_sparse_categorical_accuracy: 0.7730\n",
      "Epoch 6/100\n",
      "27616/28304 [============================>.] - ETA: 0s - loss: 0.7620 - sparse_categorical_accuracy: 0.7571\n",
      "Epoch 00006: val_loss improved from 0.71265 to 0.70920, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7620 - sparse_categorical_accuracy: 0.7573 - val_loss: 0.7092 - val_sparse_categorical_accuracy: 0.7720\n",
      "Epoch 7/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7531 - sparse_categorical_accuracy: 0.7606\n",
      "Epoch 00007: val_loss did not improve from 0.70920\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7536 - sparse_categorical_accuracy: 0.7602 - val_loss: 0.7094 - val_sparse_categorical_accuracy: 0.7737\n",
      "Epoch 8/100\n",
      "27872/28304 [============================>.] - ETA: 0s - loss: 0.7579 - sparse_categorical_accuracy: 0.7572\n",
      "Epoch 00008: val_loss improved from 0.70920 to 0.70562, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 83us/sample - loss: 0.7568 - sparse_categorical_accuracy: 0.7576 - val_loss: 0.7056 - val_sparse_categorical_accuracy: 0.7752\n",
      "Epoch 9/100\n",
      "27872/28304 [============================>.] - ETA: 0s - loss: 0.7539 - sparse_categorical_accuracy: 0.7598\n",
      "Epoch 00009: val_loss did not improve from 0.70562\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7528 - sparse_categorical_accuracy: 0.7601 - val_loss: 0.7060 - val_sparse_categorical_accuracy: 0.7736\n",
      "Epoch 10/100\n",
      "28064/28304 [============================>.] - ETA: 0s - loss: 0.7511 - sparse_categorical_accuracy: 0.7591\n",
      "Epoch 00010: val_loss did not improve from 0.70562\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7503 - sparse_categorical_accuracy: 0.7594 - val_loss: 0.7087 - val_sparse_categorical_accuracy: 0.7692\n",
      "Epoch 11/100\n",
      "27840/28304 [============================>.] - ETA: 0s - loss: 0.7487 - sparse_categorical_accuracy: 0.7612\n",
      "Epoch 00011: val_loss improved from 0.70562 to 0.70233, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7487 - sparse_categorical_accuracy: 0.7608 - val_loss: 0.7023 - val_sparse_categorical_accuracy: 0.7733\n",
      "Epoch 12/100\n",
      "27552/28304 [============================>.] - ETA: 0s - loss: 0.7477 - sparse_categorical_accuracy: 0.7617\n",
      "Epoch 00012: val_loss did not improve from 0.70233\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7488 - sparse_categorical_accuracy: 0.7612 - val_loss: 0.7041 - val_sparse_categorical_accuracy: 0.7740\n",
      "Epoch 13/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7475 - sparse_categorical_accuracy: 0.7637\n",
      "Epoch 00013: val_loss did not improve from 0.70233\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7470 - sparse_categorical_accuracy: 0.7638 - val_loss: 0.7062 - val_sparse_categorical_accuracy: 0.7743\n",
      "Epoch 14/100\n",
      "27776/28304 [============================>.] - ETA: 0s - loss: 0.7435 - sparse_categorical_accuracy: 0.7630\n",
      "Epoch 00014: val_loss did not improve from 0.70233\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7440 - sparse_categorical_accuracy: 0.7626 - val_loss: 0.7062 - val_sparse_categorical_accuracy: 0.7737\n",
      "Epoch 15/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7443 - sparse_categorical_accuracy: 0.7624\n",
      "Epoch 00015: val_loss did not improve from 0.70233\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7441 - sparse_categorical_accuracy: 0.7625 - val_loss: 0.7029 - val_sparse_categorical_accuracy: 0.7759\n",
      "Epoch 16/100\n",
      "27872/28304 [============================>.] - ETA: 0s - loss: 0.7423 - sparse_categorical_accuracy: 0.7626\n",
      "Epoch 00016: val_loss did not improve from 0.70233\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7434 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.7041 - val_sparse_categorical_accuracy: 0.7720\n",
      "Epoch 17/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7444 - sparse_categorical_accuracy: 0.7635\n",
      "Epoch 00017: val_loss improved from 0.70233 to 0.70194, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7450 - sparse_categorical_accuracy: 0.7631 - val_loss: 0.7019 - val_sparse_categorical_accuracy: 0.7737\n",
      "Epoch 18/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7468 - sparse_categorical_accuracy: 0.7622\n",
      "Epoch 00018: val_loss did not improve from 0.70194\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7462 - sparse_categorical_accuracy: 0.7627 - val_loss: 0.7025 - val_sparse_categorical_accuracy: 0.7767\n",
      "Epoch 19/100\n",
      "27904/28304 [============================>.] - ETA: 0s - loss: 0.7431 - sparse_categorical_accuracy: 0.7646- ETA: 0s - loss: 0.7412 - sparse_categorical_accuracy: 0\n",
      "Epoch 00019: val_loss improved from 0.70194 to 0.70173, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7429 - sparse_categorical_accuracy: 0.7645 - val_loss: 0.7017 - val_sparse_categorical_accuracy: 0.7744\n",
      "Epoch 20/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7416 - sparse_categorical_accuracy: 0.7645\n",
      "Epoch 00020: val_loss improved from 0.70173 to 0.69861, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7414 - sparse_categorical_accuracy: 0.7646 - val_loss: 0.6986 - val_sparse_categorical_accuracy: 0.7735\n",
      "Epoch 21/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7418 - sparse_categorical_accuracy: 0.7645\n",
      "Epoch 00021: val_loss did not improve from 0.69861\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7420 - sparse_categorical_accuracy: 0.7643 - val_loss: 0.6998 - val_sparse_categorical_accuracy: 0.7759\n",
      "Epoch 22/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7439 - sparse_categorical_accuracy: 0.7632- ETA: 0s - loss: 0.7425 - sparse_categorical_accuracy: 0.7\n",
      "Epoch 00022: val_loss did not improve from 0.69861\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7440 - sparse_categorical_accuracy: 0.7632 - val_loss: 0.6986 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 23/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7412 - sparse_categorical_accuracy: 0.7642\n",
      "Epoch 00023: val_loss did not improve from 0.69861\n",
      "28304/28304 [==============================] - 2s 71us/sample - loss: 0.7415 - sparse_categorical_accuracy: 0.7641 - val_loss: 0.7006 - val_sparse_categorical_accuracy: 0.7773\n",
      "Epoch 24/100\n",
      "28032/28304 [============================>.] - ETA: 0s - loss: 0.7410 - sparse_categorical_accuracy: 0.7651\n",
      "Epoch 00024: val_loss did not improve from 0.69861\n",
      "28304/28304 [==============================] - 2s 71us/sample - loss: 0.7414 - sparse_categorical_accuracy: 0.7650 - val_loss: 0.7011 - val_sparse_categorical_accuracy: 0.7740\n",
      "Epoch 25/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7435 - sparse_categorical_accuracy: 0.7637\n",
      "Epoch 00025: val_loss improved from 0.69861 to 0.69828, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7434 - sparse_categorical_accuracy: 0.7639 - val_loss: 0.6983 - val_sparse_categorical_accuracy: 0.7769\n",
      "Epoch 26/100\n",
      "28032/28304 [============================>.] - ETA: 0s - loss: 0.7390 - sparse_categorical_accuracy: 0.7656- ETA: 0s - loss: 0.7349 - sparse_categorica\n",
      "Epoch 00026: val_loss did not improve from 0.69828\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7390 - sparse_categorical_accuracy: 0.7655 - val_loss: 0.6987 - val_sparse_categorical_accuracy: 0.7752\n",
      "Epoch 27/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7419 - sparse_categorical_accuracy: 0.7638\n",
      "Epoch 00027: val_loss did not improve from 0.69828\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7424 - sparse_categorical_accuracy: 0.7634 - val_loss: 0.6998 - val_sparse_categorical_accuracy: 0.7747\n",
      "Epoch 28/100\n",
      "28064/28304 [============================>.] - ETA: 0s - loss: 0.7395 - sparse_categorical_accuracy: 0.7658\n",
      "Epoch 00028: val_loss did not improve from 0.69828\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7395 - sparse_categorical_accuracy: 0.7657 - val_loss: 0.6986 - val_sparse_categorical_accuracy: 0.7754\n",
      "Epoch 29/100\n",
      "27520/28304 [============================>.] - ETA: 0s - loss: 0.7385 - sparse_categorical_accuracy: 0.7654\n",
      "Epoch 00029: val_loss improved from 0.69828 to 0.69666, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7388 - sparse_categorical_accuracy: 0.7649 - val_loss: 0.6967 - val_sparse_categorical_accuracy: 0.7760\n",
      "Epoch 30/100\n",
      "28032/28304 [============================>.] - ETA: 0s - loss: 0.7376 - sparse_categorical_accuracy: 0.7646\n",
      "Epoch 00030: val_loss did not improve from 0.69666\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7378 - sparse_categorical_accuracy: 0.7645 - val_loss: 0.6979 - val_sparse_categorical_accuracy: 0.7740\n",
      "Epoch 31/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7414 - sparse_categorical_accuracy: 0.7649\n",
      "Epoch 00031: val_loss did not improve from 0.69666\n",
      "28304/28304 [==============================] - 2s 71us/sample - loss: 0.7425 - sparse_categorical_accuracy: 0.7646 - val_loss: 0.6998 - val_sparse_categorical_accuracy: 0.7747\n",
      "Epoch 32/100\n",
      "27584/28304 [============================>.] - ETA: 0s - loss: 0.7408 - sparse_categorical_accuracy: 0.7638\n",
      "Epoch 00032: val_loss did not improve from 0.69666\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7388 - sparse_categorical_accuracy: 0.7644 - val_loss: 0.6991 - val_sparse_categorical_accuracy: 0.7767\n",
      "Epoch 33/100\n",
      "28000/28304 [============================>.] - ETA: 0s - loss: 0.7402 - sparse_categorical_accuracy: 0.7657\n",
      "Epoch 00033: val_loss did not improve from 0.69666\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7403 - sparse_categorical_accuracy: 0.7656 - val_loss: 0.6970 - val_sparse_categorical_accuracy: 0.7781\n",
      "Epoch 34/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7398 - sparse_categorical_accuracy: 0.7646\n",
      "Epoch 00034: val_loss did not improve from 0.69666\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7404 - sparse_categorical_accuracy: 0.7643 - val_loss: 0.6981 - val_sparse_categorical_accuracy: 0.7766\n",
      "Epoch 35/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7391 - sparse_categorical_accuracy: 0.7639\n",
      "Epoch 00035: val_loss improved from 0.69666 to 0.69582, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7384 - sparse_categorical_accuracy: 0.7643 - val_loss: 0.6958 - val_sparse_categorical_accuracy: 0.7754\n",
      "Epoch 36/100\n",
      "27552/28304 [============================>.] - ETA: 0s - loss: 0.7381 - sparse_categorical_accuracy: 0.7673\n",
      "Epoch 00036: val_loss improved from 0.69582 to 0.69493, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7382 - sparse_categorical_accuracy: 0.7673 - val_loss: 0.6949 - val_sparse_categorical_accuracy: 0.7754\n",
      "Epoch 37/100\n",
      "28000/28304 [============================>.] - ETA: 0s - loss: 0.7384 - sparse_categorical_accuracy: 0.7651- ETA: 0s - loss: 0.7381 - sparse_categorical_accuracy: \n",
      "Epoch 00037: val_loss did not improve from 0.69493\n",
      "28304/28304 [==============================] - 2s 71us/sample - loss: 0.7380 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.6969 - val_sparse_categorical_accuracy: 0.7760\n",
      "Epoch 38/100\n",
      "27744/28304 [============================>.] - ETA: 0s - loss: 0.7357 - sparse_categorical_accuracy: 0.7658\n",
      "Epoch 00038: val_loss did not improve from 0.69493\n",
      "28304/28304 [==============================] - 2s 71us/sample - loss: 0.7357 - sparse_categorical_accuracy: 0.7658 - val_loss: 0.6974 - val_sparse_categorical_accuracy: 0.7774\n",
      "Epoch 39/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.7375 - sparse_categorical_accuracy: 0.7644\n",
      "Epoch 00039: val_loss did not improve from 0.69493\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7382 - sparse_categorical_accuracy: 0.7641 - val_loss: 0.6970 - val_sparse_categorical_accuracy: 0.7730\n",
      "Epoch 40/100\n",
      "28256/28304 [============================>.] - ETA: 0s - loss: 0.7386 - sparse_categorical_accuracy: 0.7652\n",
      "Epoch 00040: val_loss did not improve from 0.69493\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7384 - sparse_categorical_accuracy: 0.7653 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.7760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100\n",
      "27744/28304 [============================>.] - ETA: 0s - loss: 0.7419 - sparse_categorical_accuracy: 0.7625\n",
      "Epoch 00041: val_loss did not improve from 0.69493\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7418 - sparse_categorical_accuracy: 0.7627 - val_loss: 0.6970 - val_sparse_categorical_accuracy: 0.7761\n",
      "Epoch 42/100\n",
      "28256/28304 [============================>.] - ETA: 0s - loss: 0.7383 - sparse_categorical_accuracy: 0.7643\n",
      "Epoch 00042: val_loss did not improve from 0.69493\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7384 - sparse_categorical_accuracy: 0.7643 - val_loss: 0.6959 - val_sparse_categorical_accuracy: 0.7760\n",
      "Epoch 43/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7369 - sparse_categorical_accuracy: 0.7652\n",
      "Epoch 00043: val_loss did not improve from 0.69493\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7371 - sparse_categorical_accuracy: 0.7654 - val_loss: 0.6958 - val_sparse_categorical_accuracy: 0.7770\n",
      "Epoch 44/100\n",
      "27680/28304 [============================>.] - ETA: 0s - loss: 0.7387 - sparse_categorical_accuracy: 0.7651\n",
      "Epoch 00044: val_loss did not improve from 0.69493\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7375 - sparse_categorical_accuracy: 0.7655 - val_loss: 0.6959 - val_sparse_categorical_accuracy: 0.7773\n",
      "Epoch 45/100\n",
      "27744/28304 [============================>.] - ETA: 0s - loss: 0.7351 - sparse_categorical_accuracy: 0.7668- ETA: 0s - loss: 0.7348 - sparse_categorical_accuracy: 0.767\n",
      "Epoch 00045: val_loss did not improve from 0.69493\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7362 - sparse_categorical_accuracy: 0.7660 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.7773\n",
      "Epoch 46/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7370 - sparse_categorical_accuracy: 0.7667\n",
      "Epoch 00046: val_loss did not improve from 0.69493\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7374 - sparse_categorical_accuracy: 0.7667 - val_loss: 0.6965 - val_sparse_categorical_accuracy: 0.7749\n",
      "Epoch 47/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7359 - sparse_categorical_accuracy: 0.7663\n",
      "Epoch 00047: val_loss did not improve from 0.69493\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7358 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.6977 - val_sparse_categorical_accuracy: 0.7767\n",
      "Epoch 48/100\n",
      "27584/28304 [============================>.] - ETA: 0s - loss: 0.7355 - sparse_categorical_accuracy: 0.7666\n",
      "Epoch 00048: val_loss improved from 0.69493 to 0.69447, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7356 - sparse_categorical_accuracy: 0.7663 - val_loss: 0.6945 - val_sparse_categorical_accuracy: 0.7754\n",
      "Epoch 49/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7357 - sparse_categorical_accuracy: 0.7648\n",
      "Epoch 00049: val_loss did not improve from 0.69447\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7356 - sparse_categorical_accuracy: 0.7648 - val_loss: 0.6965 - val_sparse_categorical_accuracy: 0.7760\n",
      "Epoch 50/100\n",
      "27520/28304 [============================>.] - ETA: 0s - loss: 0.7359 - sparse_categorical_accuracy: 0.7669\n",
      "Epoch 00050: val_loss did not improve from 0.69447\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7365 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.6948 - val_sparse_categorical_accuracy: 0.7776\n",
      "Epoch 51/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7386 - sparse_categorical_accuracy: 0.7659\n",
      "Epoch 00051: val_loss did not improve from 0.69447\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7381 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.6955 - val_sparse_categorical_accuracy: 0.7769\n",
      "Epoch 52/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.7365 - sparse_categorical_accuracy: 0.7666\n",
      "Epoch 00052: val_loss improved from 0.69447 to 0.69431, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7365 - sparse_categorical_accuracy: 0.7666 - val_loss: 0.6943 - val_sparse_categorical_accuracy: 0.7764\n",
      "Epoch 53/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7366 - sparse_categorical_accuracy: 0.7656\n",
      "Epoch 00053: val_loss did not improve from 0.69431\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7369 - sparse_categorical_accuracy: 0.7654 - val_loss: 0.6952 - val_sparse_categorical_accuracy: 0.7744\n",
      "Epoch 54/100\n",
      "27616/28304 [============================>.] - ETA: 0s - loss: 0.7373 - sparse_categorical_accuracy: 0.7638\n",
      "Epoch 00054: val_loss improved from 0.69431 to 0.69384, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7367 - sparse_categorical_accuracy: 0.7642 - val_loss: 0.6938 - val_sparse_categorical_accuracy: 0.7759\n",
      "Epoch 55/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7358 - sparse_categorical_accuracy: 0.7662\n",
      "Epoch 00055: val_loss did not improve from 0.69384\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7360 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.6941 - val_sparse_categorical_accuracy: 0.7766\n",
      "Epoch 56/100\n",
      "27744/28304 [============================>.] - ETA: 0s - loss: 0.7380 - sparse_categorical_accuracy: 0.7660\n",
      "Epoch 00056: val_loss did not improve from 0.69384\n",
      "28304/28304 [==============================] - 2s 71us/sample - loss: 0.7368 - sparse_categorical_accuracy: 0.7666 - val_loss: 0.6960 - val_sparse_categorical_accuracy: 0.7754\n",
      "Epoch 57/100\n",
      "28256/28304 [============================>.] - ETA: 0s - loss: 0.7379 - sparse_categorical_accuracy: 0.7652- ETA: 1s - loss: 0.7461 - sparse_c\n",
      "Epoch 00057: val_loss did not improve from 0.69384\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7376 - sparse_categorical_accuracy: 0.7653 - val_loss: 0.6967 - val_sparse_categorical_accuracy: 0.7773\n",
      "Epoch 58/100\n",
      "28000/28304 [============================>.] - ETA: 0s - loss: 0.7371 - sparse_categorical_accuracy: 0.7646\n",
      "Epoch 00058: val_loss did not improve from 0.69384\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7368 - sparse_categorical_accuracy: 0.7647 - val_loss: 0.6959 - val_sparse_categorical_accuracy: 0.7750\n",
      "Epoch 59/100\n",
      "28064/28304 [============================>.] - ETA: 0s - loss: 0.7374 - sparse_categorical_accuracy: 0.7656\n",
      "Epoch 00059: val_loss did not improve from 0.69384\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7377 - sparse_categorical_accuracy: 0.7653 - val_loss: 0.6956 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 60/100\n",
      "27584/28304 [============================>.] - ETA: 0s - loss: 0.7349 - sparse_categorical_accuracy: 0.7667\n",
      "Epoch 00060: val_loss did not improve from 0.69384\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7356 - sparse_categorical_accuracy: 0.7663 - val_loss: 0.6951 - val_sparse_categorical_accuracy: 0.7764\n",
      "Epoch 61/100\n",
      "27680/28304 [============================>.] - ETA: 0s - loss: 0.7354 - sparse_categorical_accuracy: 0.7661\n",
      "Epoch 00061: val_loss did not improve from 0.69384\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7340 - sparse_categorical_accuracy: 0.7667 - val_loss: 0.6951 - val_sparse_categorical_accuracy: 0.7773\n",
      "Epoch 62/100\n",
      "28128/28304 [============================>.] - ETA: 0s - loss: 0.7346 - sparse_categorical_accuracy: 0.7665\n",
      "Epoch 00062: val_loss did not improve from 0.69384\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7360 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.6949 - val_sparse_categorical_accuracy: 0.7780\n",
      "Epoch 63/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7347 - sparse_categorical_accuracy: 0.7695\n",
      "Epoch 00063: val_loss did not improve from 0.69384\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7338 - sparse_categorical_accuracy: 0.7698 - val_loss: 0.6954 - val_sparse_categorical_accuracy: 0.7733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7350 - sparse_categorical_accuracy: 0.7672\n",
      "Epoch 00064: val_loss did not improve from 0.69384\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7355 - sparse_categorical_accuracy: 0.7671 - val_loss: 0.6956 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 65/100\n",
      "27712/28304 [============================>.] - ETA: 0s - loss: 0.7353 - sparse_categorical_accuracy: 0.7671- ETA: 0s - loss: 0.7321 - sparse_categorica\n",
      "Epoch 00065: val_loss improved from 0.69384 to 0.69325, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7359 - sparse_categorical_accuracy: 0.7666 - val_loss: 0.6933 - val_sparse_categorical_accuracy: 0.7764\n",
      "Epoch 66/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7341 - sparse_categorical_accuracy: 0.7662\n",
      "Epoch 00066: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7344 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.6950 - val_sparse_categorical_accuracy: 0.7770\n",
      "Epoch 67/100\n",
      "27616/28304 [============================>.] - ETA: 0s - loss: 0.7372 - sparse_categorical_accuracy: 0.7666\n",
      "Epoch 00067: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7378 - sparse_categorical_accuracy: 0.7665 - val_loss: 0.6959 - val_sparse_categorical_accuracy: 0.7761\n",
      "Epoch 68/100\n",
      "27584/28304 [============================>.] - ETA: 0s - loss: 0.7342 - sparse_categorical_accuracy: 0.7650\n",
      "Epoch 00068: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7322 - sparse_categorical_accuracy: 0.7657 - val_loss: 0.6960 - val_sparse_categorical_accuracy: 0.7764\n",
      "Epoch 69/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.7352 - sparse_categorical_accuracy: 0.7643\n",
      "Epoch 00069: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7349 - sparse_categorical_accuracy: 0.7646 - val_loss: 0.6945 - val_sparse_categorical_accuracy: 0.7778\n",
      "Epoch 70/100\n",
      "28096/28304 [============================>.] - ETA: 0s - loss: 0.7334 - sparse_categorical_accuracy: 0.7649\n",
      "Epoch 00070: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7333 - sparse_categorical_accuracy: 0.7649 - val_loss: 0.6979 - val_sparse_categorical_accuracy: 0.7726\n",
      "Epoch 71/100\n",
      "28256/28304 [============================>.] - ETA: 0s - loss: 0.7376 - sparse_categorical_accuracy: 0.7655\n",
      "Epoch 00071: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7376 - sparse_categorical_accuracy: 0.7655 - val_loss: 0.6943 - val_sparse_categorical_accuracy: 0.7783\n",
      "Epoch 72/100\n",
      "28032/28304 [============================>.] - ETA: 0s - loss: 0.7331 - sparse_categorical_accuracy: 0.7666\n",
      "Epoch 00072: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7331 - sparse_categorical_accuracy: 0.7667 - val_loss: 0.6936 - val_sparse_categorical_accuracy: 0.7753\n",
      "Epoch 73/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7325 - sparse_categorical_accuracy: 0.7673\n",
      "Epoch 00073: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7325 - sparse_categorical_accuracy: 0.7673 - val_loss: 0.6951 - val_sparse_categorical_accuracy: 0.7773\n",
      "Epoch 74/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.7355 - sparse_categorical_accuracy: 0.7662\n",
      "Epoch 00074: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7356 - sparse_categorical_accuracy: 0.7663 - val_loss: 0.6949 - val_sparse_categorical_accuracy: 0.7760\n",
      "Epoch 75/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7344 - sparse_categorical_accuracy: 0.7667\n",
      "Epoch 00075: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 71us/sample - loss: 0.7350 - sparse_categorical_accuracy: 0.7665 - val_loss: 0.6942 - val_sparse_categorical_accuracy: 0.7742\n",
      "Epoch 76/100\n",
      "28256/28304 [============================>.] - ETA: 0s - loss: 0.7352 - sparse_categorical_accuracy: 0.7680- ETA: 0s - loss: 0.7337 - sparse_categorical_accuracy: - ETA: 0s - loss: 0.7307 - sparse_categorical_accurac\n",
      "Epoch 00076: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7352 - sparse_categorical_accuracy: 0.7681 - val_loss: 0.6965 - val_sparse_categorical_accuracy: 0.7753\n",
      "Epoch 77/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.7337 - sparse_categorical_accuracy: 0.7674\n",
      "Epoch 00077: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7335 - sparse_categorical_accuracy: 0.7675 - val_loss: 0.6935 - val_sparse_categorical_accuracy: 0.7791\n",
      "Epoch 78/100\n",
      "28256/28304 [============================>.] - ETA: 0s - loss: 0.7361 - sparse_categorical_accuracy: 0.7651\n",
      "Epoch 00078: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7358 - sparse_categorical_accuracy: 0.7651 - val_loss: 0.6950 - val_sparse_categorical_accuracy: 0.7777\n",
      "Epoch 79/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7352 - sparse_categorical_accuracy: 0.7667\n",
      "Epoch 00079: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7351 - sparse_categorical_accuracy: 0.7667 - val_loss: 0.6973 - val_sparse_categorical_accuracy: 0.7740\n",
      "Epoch 80/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7341 - sparse_categorical_accuracy: 0.7660\n",
      "Epoch 00080: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7342 - sparse_categorical_accuracy: 0.7660 - val_loss: 0.6951 - val_sparse_categorical_accuracy: 0.7764\n",
      "Epoch 81/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.7346 - sparse_categorical_accuracy: 0.7673- ETA: 0s - loss: 0.7402 - sparse_categorical_accurac\n",
      "Epoch 00081: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7343 - sparse_categorical_accuracy: 0.7674 - val_loss: 0.6965 - val_sparse_categorical_accuracy: 0.7759\n",
      "Epoch 82/100\n",
      "27904/28304 [============================>.] - ETA: 0s - loss: 0.7338 - sparse_categorical_accuracy: 0.7674\n",
      "Epoch 00082: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7342 - sparse_categorical_accuracy: 0.7674 - val_loss: 0.6939 - val_sparse_categorical_accuracy: 0.7749\n",
      "Epoch 83/100\n",
      "28256/28304 [============================>.] - ETA: 0s - loss: 0.7318 - sparse_categorical_accuracy: 0.7660\n",
      "Epoch 00083: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7319 - sparse_categorical_accuracy: 0.7659 - val_loss: 0.6945 - val_sparse_categorical_accuracy: 0.7750\n",
      "Epoch 84/100\n",
      "28064/28304 [============================>.] - ETA: 0s - loss: 0.7354 - sparse_categorical_accuracy: 0.7663- ETA: 1s - loss: 0.7413 - sparse_categor\n",
      "Epoch 00084: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7353 - sparse_categorical_accuracy: 0.7662 - val_loss: 0.6948 - val_sparse_categorical_accuracy: 0.7783\n",
      "Epoch 85/100\n",
      "27584/28304 [============================>.] - ETA: 0s - loss: 0.7319 - sparse_categorical_accuracy: 0.7678\n",
      "Epoch 00085: val_loss did not improve from 0.69325\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7324 - sparse_categorical_accuracy: 0.7677 - val_loss: 0.6949 - val_sparse_categorical_accuracy: 0.7761\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial score of fold 0 is: 0.6940699331360298\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_44 (Dense)             (None, 100)               600       \n",
      "_________________________________________________________________\n",
      "layer_normalization_33 (Laye (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_34 (Laye (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_35 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 4)                 104       \n",
      "=================================================================\n",
      "Total params: 7,379\n",
      "Trainable params: 7,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 28304 samples, validate on 7076 samples\n",
      "Epoch 1/100\n",
      "27776/28304 [============================>.] - ETA: 0s - loss: 1.1237 - sparse_categorical_accuracy: 0.5288\n",
      "Epoch 00001: val_loss improved from inf to 0.84147, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 3s 89us/sample - loss: 1.1205 - sparse_categorical_accuracy: 0.5316 - val_loss: 0.8415 - val_sparse_categorical_accuracy: 0.7499\n",
      "Epoch 2/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.8307 - sparse_categorical_accuracy: 0.7342\n",
      "Epoch 00002: val_loss improved from 0.84147 to 0.73814, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.8310 - sparse_categorical_accuracy: 0.7343 - val_loss: 0.7381 - val_sparse_categorical_accuracy: 0.7684\n",
      "Epoch 3/100\n",
      "27712/28304 [============================>.] - ETA: 0s - loss: 0.7861 - sparse_categorical_accuracy: 0.7520\n",
      "Epoch 00003: val_loss improved from 0.73814 to 0.73275, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7863 - sparse_categorical_accuracy: 0.7521 - val_loss: 0.7328 - val_sparse_categorical_accuracy: 0.7674\n",
      "Epoch 4/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7710 - sparse_categorical_accuracy: 0.7559\n",
      "Epoch 00004: val_loss improved from 0.73275 to 0.72476, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7708 - sparse_categorical_accuracy: 0.7559 - val_loss: 0.7248 - val_sparse_categorical_accuracy: 0.7689\n",
      "Epoch 5/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7661 - sparse_categorical_accuracy: 0.7575\n",
      "Epoch 00005: val_loss did not improve from 0.72476\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7658 - sparse_categorical_accuracy: 0.7576 - val_loss: 0.7267 - val_sparse_categorical_accuracy: 0.7685\n",
      "Epoch 6/100\n",
      "27520/28304 [============================>.] - ETA: 0s - loss: 0.7570 - sparse_categorical_accuracy: 0.7589\n",
      "Epoch 00006: val_loss improved from 0.72476 to 0.72464, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7574 - sparse_categorical_accuracy: 0.7586 - val_loss: 0.7246 - val_sparse_categorical_accuracy: 0.7688\n",
      "Epoch 7/100\n",
      "27584/28304 [============================>.] - ETA: 0s - loss: 0.7558 - sparse_categorical_accuracy: 0.7620\n",
      "Epoch 00007: val_loss improved from 0.72464 to 0.72337, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7554 - sparse_categorical_accuracy: 0.7621 - val_loss: 0.7234 - val_sparse_categorical_accuracy: 0.7688\n",
      "Epoch 8/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7508 - sparse_categorical_accuracy: 0.7632\n",
      "Epoch 00008: val_loss improved from 0.72337 to 0.72150, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7518 - sparse_categorical_accuracy: 0.7627 - val_loss: 0.7215 - val_sparse_categorical_accuracy: 0.7684\n",
      "Epoch 9/100\n",
      "27840/28304 [============================>.] - ETA: 0s - loss: 0.7483 - sparse_categorical_accuracy: 0.7625\n",
      "Epoch 00009: val_loss did not improve from 0.72150\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7488 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.7223 - val_sparse_categorical_accuracy: 0.7685\n",
      "Epoch 10/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7503 - sparse_categorical_accuracy: 0.7645\n",
      "Epoch 00010: val_loss did not improve from 0.72150\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7512 - sparse_categorical_accuracy: 0.7640 - val_loss: 0.7217 - val_sparse_categorical_accuracy: 0.7681\n",
      "Epoch 11/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7491 - sparse_categorical_accuracy: 0.7622\n",
      "Epoch 00011: val_loss improved from 0.72150 to 0.71846, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 70us/sample - loss: 0.7487 - sparse_categorical_accuracy: 0.7625 - val_loss: 0.7185 - val_sparse_categorical_accuracy: 0.7701\n",
      "Epoch 12/100\n",
      "28064/28304 [============================>.] - ETA: 0s - loss: 0.7477 - sparse_categorical_accuracy: 0.7628\n",
      "Epoch 00012: val_loss did not improve from 0.71846\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7481 - sparse_categorical_accuracy: 0.7629 - val_loss: 0.7198 - val_sparse_categorical_accuracy: 0.7702\n",
      "Epoch 13/100\n",
      "28128/28304 [============================>.] - ETA: 0s - loss: 0.7470 - sparse_categorical_accuracy: 0.7642\n",
      "Epoch 00013: val_loss did not improve from 0.71846\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7480 - sparse_categorical_accuracy: 0.7638 - val_loss: 0.7193 - val_sparse_categorical_accuracy: 0.7678\n",
      "Epoch 14/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7466 - sparse_categorical_accuracy: 0.7645\n",
      "Epoch 00014: val_loss improved from 0.71846 to 0.71790, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7464 - sparse_categorical_accuracy: 0.7646 - val_loss: 0.7179 - val_sparse_categorical_accuracy: 0.7702\n",
      "Epoch 15/100\n",
      "27840/28304 [============================>.] - ETA: 0s - loss: 0.7451 - sparse_categorical_accuracy: 0.7654\n",
      "Epoch 00015: val_loss did not improve from 0.71790\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7451 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.7192 - val_sparse_categorical_accuracy: 0.7692\n",
      "Epoch 16/100\n",
      "27840/28304 [============================>.] - ETA: 0s - loss: 0.7407 - sparse_categorical_accuracy: 0.7636\n",
      "Epoch 00016: val_loss improved from 0.71790 to 0.71762, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7414 - sparse_categorical_accuracy: 0.7632 - val_loss: 0.7176 - val_sparse_categorical_accuracy: 0.7712\n",
      "Epoch 17/100\n",
      "27904/28304 [============================>.] - ETA: 0s - loss: 0.7450 - sparse_categorical_accuracy: 0.7668\n",
      "Epoch 00017: val_loss did not improve from 0.71762\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7443 - sparse_categorical_accuracy: 0.7671 - val_loss: 0.7213 - val_sparse_categorical_accuracy: 0.7684\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27552/28304 [============================>.] - ETA: 0s - loss: 0.7454 - sparse_categorical_accuracy: 0.7632\n",
      "Epoch 00018: val_loss did not improve from 0.71762\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7450 - sparse_categorical_accuracy: 0.7635 - val_loss: 0.7180 - val_sparse_categorical_accuracy: 0.7685\n",
      "Epoch 19/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7429 - sparse_categorical_accuracy: 0.7656\n",
      "Epoch 00019: val_loss improved from 0.71762 to 0.71725, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7429 - sparse_categorical_accuracy: 0.7657 - val_loss: 0.7172 - val_sparse_categorical_accuracy: 0.7691\n",
      "Epoch 20/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7450 - sparse_categorical_accuracy: 0.7646\n",
      "Epoch 00020: val_loss improved from 0.71725 to 0.71688, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7435 - sparse_categorical_accuracy: 0.7651 - val_loss: 0.7169 - val_sparse_categorical_accuracy: 0.7688\n",
      "Epoch 21/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7440 - sparse_categorical_accuracy: 0.7654\n",
      "Epoch 00021: val_loss improved from 0.71688 to 0.71381, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7439 - sparse_categorical_accuracy: 0.7656 - val_loss: 0.7138 - val_sparse_categorical_accuracy: 0.7704\n",
      "Epoch 22/100\n",
      "27776/28304 [============================>.] - ETA: 0s - loss: 0.7413 - sparse_categorical_accuracy: 0.7667\n",
      "Epoch 00022: val_loss did not improve from 0.71381\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7414 - sparse_categorical_accuracy: 0.7665 - val_loss: 0.7148 - val_sparse_categorical_accuracy: 0.7695\n",
      "Epoch 23/100\n",
      "27680/28304 [============================>.] - ETA: 0s - loss: 0.7419 - sparse_categorical_accuracy: 0.7653\n",
      "Epoch 00023: val_loss did not improve from 0.71381\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7414 - sparse_categorical_accuracy: 0.7654 - val_loss: 0.7144 - val_sparse_categorical_accuracy: 0.7713\n",
      "Epoch 24/100\n",
      "28032/28304 [============================>.] - ETA: 0s - loss: 0.7425 - sparse_categorical_accuracy: 0.7658\n",
      "Epoch 00024: val_loss did not improve from 0.71381\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7427 - sparse_categorical_accuracy: 0.7657 - val_loss: 0.7146 - val_sparse_categorical_accuracy: 0.7692\n",
      "Epoch 25/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.7412 - sparse_categorical_accuracy: 0.7658\n",
      "Epoch 00025: val_loss improved from 0.71381 to 0.71360, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7409 - sparse_categorical_accuracy: 0.7659 - val_loss: 0.7136 - val_sparse_categorical_accuracy: 0.7713\n",
      "Epoch 26/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7441 - sparse_categorical_accuracy: 0.7648\n",
      "Epoch 00026: val_loss did not improve from 0.71360\n",
      "28304/28304 [==============================] - 2s 71us/sample - loss: 0.7440 - sparse_categorical_accuracy: 0.7649 - val_loss: 0.7152 - val_sparse_categorical_accuracy: 0.7711\n",
      "Epoch 27/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7386 - sparse_categorical_accuracy: 0.7668- ETA: 0s - loss: 0.7333 - sparse_categorical_ - ETA: 0s - loss: 0.7383 - sparse_categorical_accuracy: 0.766\n",
      "Epoch 00027: val_loss did not improve from 0.71360\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7385 - sparse_categorical_accuracy: 0.7669 - val_loss: 0.7154 - val_sparse_categorical_accuracy: 0.7695\n",
      "Epoch 28/100\n",
      "28096/28304 [============================>.] - ETA: 0s - loss: 0.7402 - sparse_categorical_accuracy: 0.7656\n",
      "Epoch 00028: val_loss did not improve from 0.71360\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7404 - sparse_categorical_accuracy: 0.7656 - val_loss: 0.7138 - val_sparse_categorical_accuracy: 0.7688\n",
      "Epoch 29/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7410 - sparse_categorical_accuracy: 0.7643\n",
      "Epoch 00029: val_loss did not improve from 0.71360\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7392 - sparse_categorical_accuracy: 0.7650 - val_loss: 0.7147 - val_sparse_categorical_accuracy: 0.7711\n",
      "Epoch 30/100\n",
      "27552/28304 [============================>.] - ETA: 0s - loss: 0.7384 - sparse_categorical_accuracy: 0.7666\n",
      "Epoch 00030: val_loss did not improve from 0.71360\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7393 - sparse_categorical_accuracy: 0.7663 - val_loss: 0.7173 - val_sparse_categorical_accuracy: 0.7695\n",
      "Epoch 31/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7365 - sparse_categorical_accuracy: 0.7666\n",
      "Epoch 00031: val_loss did not improve from 0.71360\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7357 - sparse_categorical_accuracy: 0.7671 - val_loss: 0.7161 - val_sparse_categorical_accuracy: 0.7694\n",
      "Epoch 32/100\n",
      "28032/28304 [============================>.] - ETA: 0s - loss: 0.7408 - sparse_categorical_accuracy: 0.7659- ETA: 0s - loss: 0.7418 - sparse_categorical_accurac\n",
      "Epoch 00032: val_loss improved from 0.71360 to 0.71355, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7407 - sparse_categorical_accuracy: 0.7660 - val_loss: 0.7136 - val_sparse_categorical_accuracy: 0.7698\n",
      "Epoch 33/100\n",
      "27616/28304 [============================>.] - ETA: 0s - loss: 0.7383 - sparse_categorical_accuracy: 0.7664\n",
      "Epoch 00033: val_loss improved from 0.71355 to 0.71349, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7397 - sparse_categorical_accuracy: 0.7659 - val_loss: 0.7135 - val_sparse_categorical_accuracy: 0.7709\n",
      "Epoch 34/100\n",
      "28064/28304 [============================>.] - ETA: 0s - loss: 0.7365 - sparse_categorical_accuracy: 0.7663\n",
      "Epoch 00034: val_loss improved from 0.71349 to 0.71275, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7363 - sparse_categorical_accuracy: 0.7665 - val_loss: 0.7128 - val_sparse_categorical_accuracy: 0.7681\n",
      "Epoch 35/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7405 - sparse_categorical_accuracy: 0.7661\n",
      "Epoch 00035: val_loss did not improve from 0.71275\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7401 - sparse_categorical_accuracy: 0.7663 - val_loss: 0.7152 - val_sparse_categorical_accuracy: 0.7706\n",
      "Epoch 36/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7385 - sparse_categorical_accuracy: 0.7652- ETA: 0s - loss: 0.7308 - sparse_categorical_acc\n",
      "Epoch 00036: val_loss did not improve from 0.71275\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7385 - sparse_categorical_accuracy: 0.7648 - val_loss: 0.7149 - val_sparse_categorical_accuracy: 0.7696\n",
      "Epoch 37/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7399 - sparse_categorical_accuracy: 0.7654\n",
      "Epoch 00037: val_loss did not improve from 0.71275\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7394 - sparse_categorical_accuracy: 0.7656 - val_loss: 0.7201 - val_sparse_categorical_accuracy: 0.7699\n",
      "Epoch 38/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7368 - sparse_categorical_accuracy: 0.7660\n",
      "Epoch 00038: val_loss did not improve from 0.71275\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7368 - sparse_categorical_accuracy: 0.7660 - val_loss: 0.7133 - val_sparse_categorical_accuracy: 0.7702\n",
      "Epoch 39/100\n",
      "27904/28304 [============================>.] - ETA: 0s - loss: 0.7376 - sparse_categorical_accuracy: 0.7684- ETA: 0s - loss: 0.7280 - sparse_categorical\n",
      "Epoch 00039: val_loss did not improve from 0.71275\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7383 - sparse_categorical_accuracy: 0.7683 - val_loss: 0.7130 - val_sparse_categorical_accuracy: 0.7709\n",
      "Epoch 40/100\n",
      "28096/28304 [============================>.] - ETA: 0s - loss: 0.7379 - sparse_categorical_accuracy: 0.7663\n",
      "Epoch 00040: val_loss did not improve from 0.71275\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7381 - sparse_categorical_accuracy: 0.7663 - val_loss: 0.7130 - val_sparse_categorical_accuracy: 0.7716\n",
      "Epoch 41/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7376 - sparse_categorical_accuracy: 0.7661\n",
      "Epoch 00041: val_loss did not improve from 0.71275\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7382 - sparse_categorical_accuracy: 0.7659 - val_loss: 0.7129 - val_sparse_categorical_accuracy: 0.7713\n",
      "Epoch 42/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7368 - sparse_categorical_accuracy: 0.7658\n",
      "Epoch 00042: val_loss did not improve from 0.71275\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7364 - sparse_categorical_accuracy: 0.7658 - val_loss: 0.7169 - val_sparse_categorical_accuracy: 0.7695\n",
      "Epoch 43/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7385 - sparse_categorical_accuracy: 0.7658\n",
      "Epoch 00043: val_loss did not improve from 0.71275\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7388 - sparse_categorical_accuracy: 0.7654 - val_loss: 0.7138 - val_sparse_categorical_accuracy: 0.7704\n",
      "Epoch 44/100\n",
      "28064/28304 [============================>.] - ETA: 0s - loss: 0.7343 - sparse_categorical_accuracy: 0.7668\n",
      "Epoch 00044: val_loss did not improve from 0.71275\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7340 - sparse_categorical_accuracy: 0.7671 - val_loss: 0.7158 - val_sparse_categorical_accuracy: 0.7687\n",
      "Epoch 45/100\n",
      "27680/28304 [============================>.] - ETA: 0s - loss: 0.7364 - sparse_categorical_accuracy: 0.7651\n",
      "Epoch 00045: val_loss did not improve from 0.71275\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7368 - sparse_categorical_accuracy: 0.7650 - val_loss: 0.7168 - val_sparse_categorical_accuracy: 0.7708\n",
      "Epoch 46/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7361 - sparse_categorical_accuracy: 0.7671\n",
      "Epoch 00046: val_loss improved from 0.71275 to 0.71248, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7353 - sparse_categorical_accuracy: 0.7675 - val_loss: 0.7125 - val_sparse_categorical_accuracy: 0.7708\n",
      "Epoch 47/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7351 - sparse_categorical_accuracy: 0.7673\n",
      "Epoch 00047: val_loss did not improve from 0.71248\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7350 - sparse_categorical_accuracy: 0.7674 - val_loss: 0.7139 - val_sparse_categorical_accuracy: 0.7694\n",
      "Epoch 48/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.7358 - sparse_categorical_accuracy: 0.7671\n",
      "Epoch 00048: val_loss improved from 0.71248 to 0.71148, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7352 - sparse_categorical_accuracy: 0.7673 - val_loss: 0.7115 - val_sparse_categorical_accuracy: 0.7699\n",
      "Epoch 49/100\n",
      "27520/28304 [============================>.] - ETA: 0s - loss: 0.7353 - sparse_categorical_accuracy: 0.7662\n",
      "Epoch 00049: val_loss did not improve from 0.71148\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7346 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.7120 - val_sparse_categorical_accuracy: 0.7689\n",
      "Epoch 50/100\n",
      "27584/28304 [============================>.] - ETA: 0s - loss: 0.7366 - sparse_categorical_accuracy: 0.7669\n",
      "Epoch 00050: val_loss improved from 0.71148 to 0.71123, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7360 - sparse_categorical_accuracy: 0.7673 - val_loss: 0.7112 - val_sparse_categorical_accuracy: 0.7720\n",
      "Epoch 51/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.7351 - sparse_categorical_accuracy: 0.7669\n",
      "Epoch 00051: val_loss did not improve from 0.71123\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7350 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.7114 - val_sparse_categorical_accuracy: 0.7705\n",
      "Epoch 52/100\n",
      "28256/28304 [============================>.] - ETA: 0s - loss: 0.7342 - sparse_categorical_accuracy: 0.7670\n",
      "Epoch 00052: val_loss did not improve from 0.71123\n",
      "28304/28304 [==============================] - 2s 72us/sample - loss: 0.7344 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.7115 - val_sparse_categorical_accuracy: 0.7706\n",
      "Epoch 53/100\n",
      "28128/28304 [============================>.] - ETA: 0s - loss: 0.7335 - sparse_categorical_accuracy: 0.7671\n",
      "Epoch 00053: val_loss did not improve from 0.71123\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7333 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.7114 - val_sparse_categorical_accuracy: 0.7708\n",
      "Epoch 54/100\n",
      "27904/28304 [============================>.] - ETA: 0s - loss: 0.7369 - sparse_categorical_accuracy: 0.7669\n",
      "Epoch 00054: val_loss did not improve from 0.71123\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7364 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.7135 - val_sparse_categorical_accuracy: 0.7694\n",
      "Epoch 55/100\n",
      "27872/28304 [============================>.] - ETA: 0s - loss: 0.7357 - sparse_categorical_accuracy: 0.7669\n",
      "Epoch 00055: val_loss did not improve from 0.71123\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7370 - sparse_categorical_accuracy: 0.7663 - val_loss: 0.7131 - val_sparse_categorical_accuracy: 0.7725\n",
      "Epoch 56/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7345 - sparse_categorical_accuracy: 0.7670\n",
      "Epoch 00056: val_loss improved from 0.71123 to 0.71109, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7339 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.7111 - val_sparse_categorical_accuracy: 0.7702\n",
      "Epoch 57/100\n",
      "28064/28304 [============================>.] - ETA: 0s - loss: 0.7352 - sparse_categorical_accuracy: 0.7681\n",
      "Epoch 00057: val_loss did not improve from 0.71109\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7353 - sparse_categorical_accuracy: 0.7681 - val_loss: 0.7134 - val_sparse_categorical_accuracy: 0.7699\n",
      "Epoch 58/100\n",
      "28000/28304 [============================>.] - ETA: 0s - loss: 0.7303 - sparse_categorical_accuracy: 0.7671\n",
      "Epoch 00058: val_loss did not improve from 0.71109\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7302 - sparse_categorical_accuracy: 0.7675 - val_loss: 0.7130 - val_sparse_categorical_accuracy: 0.7711\n",
      "Epoch 59/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7330 - sparse_categorical_accuracy: 0.7673\n",
      "Epoch 00059: val_loss did not improve from 0.71109\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7329 - sparse_categorical_accuracy: 0.7674 - val_loss: 0.7140 - val_sparse_categorical_accuracy: 0.7674\n",
      "Epoch 60/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7336 - sparse_categorical_accuracy: 0.7686\n",
      "Epoch 00060: val_loss did not improve from 0.71109\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7349 - sparse_categorical_accuracy: 0.7681 - val_loss: 0.7119 - val_sparse_categorical_accuracy: 0.7695\n",
      "Epoch 61/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7338 - sparse_categorical_accuracy: 0.7652\n",
      "Epoch 00061: val_loss improved from 0.71109 to 0.70966, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 79us/sample - loss: 0.7341 - sparse_categorical_accuracy: 0.7651 - val_loss: 0.7097 - val_sparse_categorical_accuracy: 0.7711\n",
      "Epoch 62/100\n",
      "27840/28304 [============================>.] - ETA: 0s - loss: 0.7333 - sparse_categorical_accuracy: 0.7672\n",
      "Epoch 00062: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7320 - sparse_categorical_accuracy: 0.7679 - val_loss: 0.7106 - val_sparse_categorical_accuracy: 0.7733\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27904/28304 [============================>.] - ETA: 0s - loss: 0.7338 - sparse_categorical_accuracy: 0.7661\n",
      "Epoch 00063: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7332 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.7122 - val_sparse_categorical_accuracy: 0.7692\n",
      "Epoch 64/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7341 - sparse_categorical_accuracy: 0.7682\n",
      "Epoch 00064: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 81us/sample - loss: 0.7344 - sparse_categorical_accuracy: 0.7681 - val_loss: 0.7113 - val_sparse_categorical_accuracy: 0.7702\n",
      "Epoch 65/100\n",
      "27616/28304 [============================>.] - ETA: 0s - loss: 0.7352 - sparse_categorical_accuracy: 0.7672\n",
      "Epoch 00065: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7357 - sparse_categorical_accuracy: 0.7669 - val_loss: 0.7114 - val_sparse_categorical_accuracy: 0.7715\n",
      "Epoch 66/100\n",
      "27776/28304 [============================>.] - ETA: 0s - loss: 0.7354 - sparse_categorical_accuracy: 0.7672\n",
      "Epoch 00066: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7343 - sparse_categorical_accuracy: 0.7678 - val_loss: 0.7121 - val_sparse_categorical_accuracy: 0.7732\n",
      "Epoch 67/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7353 - sparse_categorical_accuracy: 0.7674- ETA: 1s - loss: 0.7473 - s\n",
      "Epoch 00067: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7353 - sparse_categorical_accuracy: 0.7673 - val_loss: 0.7111 - val_sparse_categorical_accuracy: 0.7695\n",
      "Epoch 68/100\n",
      "27616/28304 [============================>.] - ETA: 0s - loss: 0.7325 - sparse_categorical_accuracy: 0.7685\n",
      "Epoch 00068: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7331 - sparse_categorical_accuracy: 0.7683 - val_loss: 0.7112 - val_sparse_categorical_accuracy: 0.7729\n",
      "Epoch 69/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7361 - sparse_categorical_accuracy: 0.7687\n",
      "Epoch 00069: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7357 - sparse_categorical_accuracy: 0.7689 - val_loss: 0.7106 - val_sparse_categorical_accuracy: 0.7736\n",
      "Epoch 70/100\n",
      "27712/28304 [============================>.] - ETA: 0s - loss: 0.7348 - sparse_categorical_accuracy: 0.7668\n",
      "Epoch 00070: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7353 - sparse_categorical_accuracy: 0.7671 - val_loss: 0.7143 - val_sparse_categorical_accuracy: 0.7730\n",
      "Epoch 71/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7340 - sparse_categorical_accuracy: 0.7677\n",
      "Epoch 00071: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7347 - sparse_categorical_accuracy: 0.7674 - val_loss: 0.7098 - val_sparse_categorical_accuracy: 0.7719\n",
      "Epoch 72/100\n",
      "28064/28304 [============================>.] - ETA: 0s - loss: 0.7324 - sparse_categorical_accuracy: 0.7676\n",
      "Epoch 00072: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7324 - sparse_categorical_accuracy: 0.7675 - val_loss: 0.7111 - val_sparse_categorical_accuracy: 0.7715\n",
      "Epoch 73/100\n",
      "27744/28304 [============================>.] - ETA: 0s - loss: 0.7354 - sparse_categorical_accuracy: 0.7659\n",
      "Epoch 00073: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7340 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.7115 - val_sparse_categorical_accuracy: 0.7701\n",
      "Epoch 74/100\n",
      "28000/28304 [============================>.] - ETA: 0s - loss: 0.7379 - sparse_categorical_accuracy: 0.7679\n",
      "Epoch 00074: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7383 - sparse_categorical_accuracy: 0.7676 - val_loss: 0.7111 - val_sparse_categorical_accuracy: 0.7719\n",
      "Epoch 75/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7360 - sparse_categorical_accuracy: 0.7672\n",
      "Epoch 00075: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7363 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.7118 - val_sparse_categorical_accuracy: 0.7719\n",
      "Epoch 76/100\n",
      "28096/28304 [============================>.] - ETA: 0s - loss: 0.7319 - sparse_categorical_accuracy: 0.7686\n",
      "Epoch 00076: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7327 - sparse_categorical_accuracy: 0.7684 - val_loss: 0.7116 - val_sparse_categorical_accuracy: 0.7699\n",
      "Epoch 77/100\n",
      "27584/28304 [============================>.] - ETA: 0s - loss: 0.7359 - sparse_categorical_accuracy: 0.7666\n",
      "Epoch 00077: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7347 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.7101 - val_sparse_categorical_accuracy: 0.7720\n",
      "Epoch 78/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.7333 - sparse_categorical_accuracy: 0.7671\n",
      "Epoch 00078: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7330 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.7119 - val_sparse_categorical_accuracy: 0.7711\n",
      "Epoch 79/100\n",
      "27616/28304 [============================>.] - ETA: 0s - loss: 0.7342 - sparse_categorical_accuracy: 0.7674\n",
      "Epoch 00079: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7353 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.7120 - val_sparse_categorical_accuracy: 0.7715\n",
      "Epoch 80/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7356 - sparse_categorical_accuracy: 0.7673\n",
      "Epoch 00080: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7355 - sparse_categorical_accuracy: 0.7671 - val_loss: 0.7147 - val_sparse_categorical_accuracy: 0.7684\n",
      "Epoch 81/100\n",
      "28128/28304 [============================>.] - ETA: 0s - loss: 0.7331 - sparse_categorical_accuracy: 0.7668\n",
      "Epoch 00081: val_loss did not improve from 0.70966\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7329 - sparse_categorical_accuracy: 0.7669 - val_loss: 0.7106 - val_sparse_categorical_accuracy: 0.7713\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Partial score of fold 1 is: 0.6919863691326811\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 100)               600       \n",
      "_________________________________________________________________\n",
      "layer_normalization_36 (Laye (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_37 (Laye (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_38 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 4)                 104       \n",
      "=================================================================\n",
      "Total params: 7,379\n",
      "Trainable params: 7,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28304 samples, validate on 7076 samples\n",
      "Epoch 1/100\n",
      "27904/28304 [============================>.] - ETA: 0s - loss: 1.0762 - sparse_categorical_accuracy: 0.5610\n",
      "Epoch 00001: val_loss improved from inf to 0.77633, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 3s 98us/sample - loss: 1.0735 - sparse_categorical_accuracy: 0.5628 - val_loss: 0.7763 - val_sparse_categorical_accuracy: 0.7572\n",
      "Epoch 2/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.8305 - sparse_categorical_accuracy: 0.7364\n",
      "Epoch 00002: val_loss improved from 0.77633 to 0.72565, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 87us/sample - loss: 0.8310 - sparse_categorical_accuracy: 0.7362 - val_loss: 0.7256 - val_sparse_categorical_accuracy: 0.7665\n",
      "Epoch 3/100\n",
      "27776/28304 [============================>.] - ETA: 0s - loss: 0.7901 - sparse_categorical_accuracy: 0.7491\n",
      "Epoch 00003: val_loss did not improve from 0.72565\n",
      "28304/28304 [==============================] - 2s 80us/sample - loss: 0.7899 - sparse_categorical_accuracy: 0.7490 - val_loss: 0.7271 - val_sparse_categorical_accuracy: 0.7638\n",
      "Epoch 4/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7738 - sparse_categorical_accuracy: 0.7532\n",
      "Epoch 00004: val_loss improved from 0.72565 to 0.71598, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7742 - sparse_categorical_accuracy: 0.7530 - val_loss: 0.7160 - val_sparse_categorical_accuracy: 0.7668\n",
      "Epoch 5/100\n",
      "27904/28304 [============================>.] - ETA: 0s - loss: 0.7677 - sparse_categorical_accuracy: 0.7557- ETA: 1s - loss: 0.7577 - spar\n",
      "Epoch 00005: val_loss improved from 0.71598 to 0.70938, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7673 - sparse_categorical_accuracy: 0.7557 - val_loss: 0.7094 - val_sparse_categorical_accuracy: 0.7728\n",
      "Epoch 6/100\n",
      "27552/28304 [============================>.] - ETA: 0s - loss: 0.7609 - sparse_categorical_accuracy: 0.7598- ETA: 1s - loss: 0.7597 - sp\n",
      "Epoch 00006: val_loss did not improve from 0.70938\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7605 - sparse_categorical_accuracy: 0.7600 - val_loss: 0.7191 - val_sparse_categorical_accuracy: 0.7660\n",
      "Epoch 7/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7579 - sparse_categorical_accuracy: 0.7606- ETA: 0s - loss: 0.7620 - sparse_categorical_accu\n",
      "Epoch 00007: val_loss did not improve from 0.70938\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7578 - sparse_categorical_accuracy: 0.7607 - val_loss: 0.7134 - val_sparse_categorical_accuracy: 0.7711\n",
      "Epoch 8/100\n",
      "28128/28304 [============================>.] - ETA: 0s - loss: 0.7554 - sparse_categorical_accuracy: 0.7605\n",
      "Epoch 00008: val_loss did not improve from 0.70938\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7552 - sparse_categorical_accuracy: 0.7604 - val_loss: 0.7108 - val_sparse_categorical_accuracy: 0.7701\n",
      "Epoch 9/100\n",
      "28064/28304 [============================>.] - ETA: 0s - loss: 0.7537 - sparse_categorical_accuracy: 0.7609\n",
      "Epoch 00009: val_loss improved from 0.70938 to 0.70751, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7542 - sparse_categorical_accuracy: 0.7608 - val_loss: 0.7075 - val_sparse_categorical_accuracy: 0.7701\n",
      "Epoch 10/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7511 - sparse_categorical_accuracy: 0.7610\n",
      "Epoch 00010: val_loss improved from 0.70751 to 0.70740, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7507 - sparse_categorical_accuracy: 0.7609 - val_loss: 0.7074 - val_sparse_categorical_accuracy: 0.7706\n",
      "Epoch 11/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7482 - sparse_categorical_accuracy: 0.7648\n",
      "Epoch 00011: val_loss did not improve from 0.70740\n",
      "28304/28304 [==============================] - 2s 80us/sample - loss: 0.7483 - sparse_categorical_accuracy: 0.7647 - val_loss: 0.7082 - val_sparse_categorical_accuracy: 0.7695\n",
      "Epoch 12/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7459 - sparse_categorical_accuracy: 0.7641\n",
      "Epoch 00012: val_loss did not improve from 0.70740\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7470 - sparse_categorical_accuracy: 0.7638 - val_loss: 0.7085 - val_sparse_categorical_accuracy: 0.7685\n",
      "Epoch 13/100\n",
      "28000/28304 [============================>.] - ETA: 0s - loss: 0.7496 - sparse_categorical_accuracy: 0.7622\n",
      "Epoch 00013: val_loss did not improve from 0.70740\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7496 - sparse_categorical_accuracy: 0.7620 - val_loss: 0.7075 - val_sparse_categorical_accuracy: 0.7684\n",
      "Epoch 14/100\n",
      "28128/28304 [============================>.] - ETA: 0s - loss: 0.7475 - sparse_categorical_accuracy: 0.7637\n",
      "Epoch 00014: val_loss improved from 0.70740 to 0.70580, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7474 - sparse_categorical_accuracy: 0.7637 - val_loss: 0.7058 - val_sparse_categorical_accuracy: 0.7689\n",
      "Epoch 15/100\n",
      "27680/28304 [============================>.] - ETA: 0s - loss: 0.7450 - sparse_categorical_accuracy: 0.7662\n",
      "Epoch 00015: val_loss did not improve from 0.70580\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7461 - sparse_categorical_accuracy: 0.7657 - val_loss: 0.7067 - val_sparse_categorical_accuracy: 0.7695\n",
      "Epoch 16/100\n",
      "28000/28304 [============================>.] - ETA: 0s - loss: 0.7466 - sparse_categorical_accuracy: 0.7660\n",
      "Epoch 00016: val_loss improved from 0.70580 to 0.70552, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7458 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.7055 - val_sparse_categorical_accuracy: 0.7711\n",
      "Epoch 17/100\n",
      "27872/28304 [============================>.] - ETA: 0s - loss: 0.7465 - sparse_categorical_accuracy: 0.7638\n",
      "Epoch 00017: val_loss did not improve from 0.70552\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7458 - sparse_categorical_accuracy: 0.7644 - val_loss: 0.7065 - val_sparse_categorical_accuracy: 0.7709\n",
      "Epoch 18/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7442 - sparse_categorical_accuracy: 0.7645\n",
      "Epoch 00018: val_loss did not improve from 0.70552\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7438 - sparse_categorical_accuracy: 0.7647 - val_loss: 0.7072 - val_sparse_categorical_accuracy: 0.7694\n",
      "Epoch 19/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7437 - sparse_categorical_accuracy: 0.7637\n",
      "Epoch 00019: val_loss improved from 0.70552 to 0.70486, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7436 - sparse_categorical_accuracy: 0.7636 - val_loss: 0.7049 - val_sparse_categorical_accuracy: 0.7679\n",
      "Epoch 20/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7429 - sparse_categorical_accuracy: 0.7642\n",
      "Epoch 00020: val_loss did not improve from 0.70486\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7427 - sparse_categorical_accuracy: 0.7642 - val_loss: 0.7067 - val_sparse_categorical_accuracy: 0.7711\n",
      "Epoch 21/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7431 - sparse_categorical_accuracy: 0.7648\n",
      "Epoch 00021: val_loss improved from 0.70486 to 0.70349, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7433 - sparse_categorical_accuracy: 0.7647 - val_loss: 0.7035 - val_sparse_categorical_accuracy: 0.7722\n",
      "Epoch 22/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7450 - sparse_categorical_accuracy: 0.7640\n",
      "Epoch 00022: val_loss did not improve from 0.70349\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7439 - sparse_categorical_accuracy: 0.7643 - val_loss: 0.7059 - val_sparse_categorical_accuracy: 0.7696\n",
      "Epoch 23/100\n",
      "28032/28304 [============================>.] - ETA: 0s - loss: 0.7411 - sparse_categorical_accuracy: 0.7653\n",
      "Epoch 00023: val_loss improved from 0.70349 to 0.70318, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7412 - sparse_categorical_accuracy: 0.7654 - val_loss: 0.7032 - val_sparse_categorical_accuracy: 0.7760\n",
      "Epoch 24/100\n",
      "28032/28304 [============================>.] - ETA: 0s - loss: 0.7432 - sparse_categorical_accuracy: 0.7654\n",
      "Epoch 00024: val_loss did not improve from 0.70318\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7434 - sparse_categorical_accuracy: 0.7653 - val_loss: 0.7066 - val_sparse_categorical_accuracy: 0.7651\n",
      "Epoch 25/100\n",
      "28096/28304 [============================>.] - ETA: 0s - loss: 0.7404 - sparse_categorical_accuracy: 0.7662\n",
      "Epoch 00025: val_loss did not improve from 0.70318\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7403 - sparse_categorical_accuracy: 0.7662 - val_loss: 0.7037 - val_sparse_categorical_accuracy: 0.7702\n",
      "Epoch 26/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7404 - sparse_categorical_accuracy: 0.7648\n",
      "Epoch 00026: val_loss improved from 0.70318 to 0.70196, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7405 - sparse_categorical_accuracy: 0.7648 - val_loss: 0.7020 - val_sparse_categorical_accuracy: 0.7695\n",
      "Epoch 27/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7404 - sparse_categorical_accuracy: 0.7661\n",
      "Epoch 00027: val_loss did not improve from 0.70196\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7411 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.7060 - val_sparse_categorical_accuracy: 0.7687\n",
      "Epoch 28/100\n",
      "27776/28304 [============================>.] - ETA: 0s - loss: 0.7407 - sparse_categorical_accuracy: 0.7665\n",
      "Epoch 00028: val_loss did not improve from 0.70196\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7416 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.7031 - val_sparse_categorical_accuracy: 0.7720\n",
      "Epoch 29/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7400 - sparse_categorical_accuracy: 0.765 - ETA: 0s - loss: 0.7397 - sparse_categorical_accuracy: 0.7660\n",
      "Epoch 00029: val_loss did not improve from 0.70196\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7397 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.7032 - val_sparse_categorical_accuracy: 0.7720\n",
      "Epoch 30/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7394 - sparse_categorical_accuracy: 0.7660\n",
      "Epoch 00030: val_loss improved from 0.70196 to 0.70160, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7380 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.7016 - val_sparse_categorical_accuracy: 0.7709\n",
      "Epoch 31/100\n",
      "28256/28304 [============================>.] - ETA: 0s - loss: 0.7396 - sparse_categorical_accuracy: 0.7649\n",
      "Epoch 00031: val_loss did not improve from 0.70160\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7398 - sparse_categorical_accuracy: 0.7648 - val_loss: 0.7025 - val_sparse_categorical_accuracy: 0.7688\n",
      "Epoch 32/100\n",
      "28256/28304 [============================>.] - ETA: 0s - loss: 0.7387 - sparse_categorical_accuracy: 0.7666- ETA: 0s - loss: 0.7364 - sparse_categorical_accuracy: 0.7\n",
      "Epoch 00032: val_loss did not improve from 0.70160\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7391 - sparse_categorical_accuracy: 0.7663 - val_loss: 0.7046 - val_sparse_categorical_accuracy: 0.7685\n",
      "Epoch 33/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7372 - sparse_categorical_accuracy: 0.7661- ETA: 1s - loss: 0.7432 - sparse_ca\n",
      "Epoch 00033: val_loss did not improve from 0.70160\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7383 - sparse_categorical_accuracy: 0.7657 - val_loss: 0.7016 - val_sparse_categorical_accuracy: 0.7709\n",
      "Epoch 34/100\n",
      "28128/28304 [============================>.] - ETA: 0s - loss: 0.7416 - sparse_categorical_accuracy: 0.7636\n",
      "Epoch 00034: val_loss did not improve from 0.70160\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7416 - sparse_categorical_accuracy: 0.7636 - val_loss: 0.7024 - val_sparse_categorical_accuracy: 0.7716\n",
      "Epoch 35/100\n",
      "27712/28304 [============================>.] - ETA: 0s - loss: 0.7381 - sparse_categorical_accuracy: 0.7656\n",
      "Epoch 00035: val_loss did not improve from 0.70160\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7370 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.7041 - val_sparse_categorical_accuracy: 0.7647\n",
      "Epoch 36/100\n",
      "27680/28304 [============================>.] - ETA: 0s - loss: 0.7334 - sparse_categorical_accuracy: 0.7676\n",
      "Epoch 00036: val_loss improved from 0.70160 to 0.69913, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7341 - sparse_categorical_accuracy: 0.7675 - val_loss: 0.6991 - val_sparse_categorical_accuracy: 0.7739\n",
      "Epoch 37/100\n",
      "27712/28304 [============================>.] - ETA: 0s - loss: 0.7394 - sparse_categorical_accuracy: 0.7665\n",
      "Epoch 00037: val_loss did not improve from 0.69913\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7383 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.7010 - val_sparse_categorical_accuracy: 0.7719\n",
      "Epoch 38/100\n",
      "28128/28304 [============================>.] - ETA: 0s - loss: 0.7379 - sparse_categorical_accuracy: 0.7663\n",
      "Epoch 00038: val_loss improved from 0.69913 to 0.69848, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7381 - sparse_categorical_accuracy: 0.7663 - val_loss: 0.6985 - val_sparse_categorical_accuracy: 0.7722\n",
      "Epoch 39/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7380 - sparse_categorical_accuracy: 0.7673\n",
      "Epoch 00039: val_loss did not improve from 0.69848\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7381 - sparse_categorical_accuracy: 0.7671 - val_loss: 0.6995 - val_sparse_categorical_accuracy: 0.7716\n",
      "Epoch 40/100\n",
      "28000/28304 [============================>.] - ETA: 0s - loss: 0.7386 - sparse_categorical_accuracy: 0.7654\n",
      "Epoch 00040: val_loss did not improve from 0.69848\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7389 - sparse_categorical_accuracy: 0.7653 - val_loss: 0.7045 - val_sparse_categorical_accuracy: 0.7671\n",
      "Epoch 41/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.7382 - sparse_categorical_accuracy: 0.7673\n",
      "Epoch 00041: val_loss did not improve from 0.69848\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7377 - sparse_categorical_accuracy: 0.7675 - val_loss: 0.7036 - val_sparse_categorical_accuracy: 0.7677\n",
      "Epoch 42/100\n",
      "28000/28304 [============================>.] - ETA: 0s - loss: 0.7403 - sparse_categorical_accuracy: 0.7654\n",
      "Epoch 00042: val_loss improved from 0.69848 to 0.69820, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7401 - sparse_categorical_accuracy: 0.7655 - val_loss: 0.6982 - val_sparse_categorical_accuracy: 0.7739\n",
      "Epoch 43/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7399 - sparse_categorical_accuracy: 0.7649\n",
      "Epoch 00043: val_loss did not improve from 0.69820\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7383 - sparse_categorical_accuracy: 0.7657 - val_loss: 0.6997 - val_sparse_categorical_accuracy: 0.7719\n",
      "Epoch 44/100\n",
      "27904/28304 [============================>.] - ETA: 0s - loss: 0.7378 - sparse_categorical_accuracy: 0.7668- ETA: 0s - loss: 0.7407 - sparse_categorical_accuracy: 0\n",
      "Epoch 00044: val_loss did not improve from 0.69820\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7382 - sparse_categorical_accuracy: 0.7667 - val_loss: 0.7022 - val_sparse_categorical_accuracy: 0.7696\n",
      "Epoch 45/100\n",
      "27904/28304 [============================>.] - ETA: 0s - loss: 0.7384 - sparse_categorical_accuracy: 0.7659\n",
      "Epoch 00045: val_loss did not improve from 0.69820\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7387 - sparse_categorical_accuracy: 0.7659 - val_loss: 0.7006 - val_sparse_categorical_accuracy: 0.7749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100\n",
      "28032/28304 [============================>.] - ETA: 0s - loss: 0.7336 - sparse_categorical_accuracy: 0.7675\n",
      "Epoch 00046: val_loss did not improve from 0.69820\n",
      "28304/28304 [==============================] - 2s 79us/sample - loss: 0.7339 - sparse_categorical_accuracy: 0.7676 - val_loss: 0.6995 - val_sparse_categorical_accuracy: 0.7725\n",
      "Epoch 47/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7369 - sparse_categorical_accuracy: 0.7647\n",
      "Epoch 00047: val_loss did not improve from 0.69820\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7364 - sparse_categorical_accuracy: 0.7650 - val_loss: 0.6992 - val_sparse_categorical_accuracy: 0.7732\n",
      "Epoch 48/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7341 - sparse_categorical_accuracy: 0.768 - ETA: 0s - loss: 0.7340 - sparse_categorical_accuracy: 0.7679\n",
      "Epoch 00048: val_loss did not improve from 0.69820\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7345 - sparse_categorical_accuracy: 0.7677 - val_loss: 0.7026 - val_sparse_categorical_accuracy: 0.7729\n",
      "Epoch 49/100\n",
      "27904/28304 [============================>.] - ETA: 0s - loss: 0.7357 - sparse_categorical_accuracy: 0.7664- ETA: 0s - loss: 0.7389 - sparse_categorical_accuracy:\n",
      "Epoch 00049: val_loss did not improve from 0.69820\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7367 - sparse_categorical_accuracy: 0.7663 - val_loss: 0.6986 - val_sparse_categorical_accuracy: 0.7756\n",
      "Epoch 50/100\n",
      "28096/28304 [============================>.] - ETA: 0s - loss: 0.7396 - sparse_categorical_accuracy: 0.7662\n",
      "Epoch 00050: val_loss improved from 0.69820 to 0.69785, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7403 - sparse_categorical_accuracy: 0.7659 - val_loss: 0.6979 - val_sparse_categorical_accuracy: 0.7733\n",
      "Epoch 51/100\n",
      "27744/28304 [============================>.] - ETA: 0s - loss: 0.7372 - sparse_categorical_accuracy: 0.7663\n",
      "Epoch 00051: val_loss improved from 0.69785 to 0.69717, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 79us/sample - loss: 0.7366 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.6972 - val_sparse_categorical_accuracy: 0.7722\n",
      "Epoch 52/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7389 - sparse_categorical_accuracy: 0.7667\n",
      "Epoch 00052: val_loss did not improve from 0.69717\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7386 - sparse_categorical_accuracy: 0.7667 - val_loss: 0.6991 - val_sparse_categorical_accuracy: 0.7761\n",
      "Epoch 53/100\n",
      "27776/28304 [============================>.] - ETA: 0s - loss: 0.7336 - sparse_categorical_accuracy: 0.7685- ETA: 1s - loss: 0.7291 - sparse_ca - ETA: 0s - loss: 0.7333 - sparse_categorical_accuracy: 0.768\n",
      "Epoch 00053: val_loss did not improve from 0.69717\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7343 - sparse_categorical_accuracy: 0.7684 - val_loss: 0.7012 - val_sparse_categorical_accuracy: 0.7720\n",
      "Epoch 54/100\n",
      "28000/28304 [============================>.] - ETA: 0s - loss: 0.7375 - sparse_categorical_accuracy: 0.7656\n",
      "Epoch 00054: val_loss did not improve from 0.69717\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7373 - sparse_categorical_accuracy: 0.7656 - val_loss: 0.6984 - val_sparse_categorical_accuracy: 0.7770\n",
      "Epoch 55/100\n",
      "27616/28304 [============================>.] - ETA: 0s - loss: 0.7354 - sparse_categorical_accuracy: 0.7674\n",
      "Epoch 00055: val_loss did not improve from 0.69717\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7353 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.6997 - val_sparse_categorical_accuracy: 0.7715\n",
      "Epoch 56/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7395 - sparse_categorical_accuracy: 0.7659\n",
      "Epoch 00056: val_loss did not improve from 0.69717\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7385 - sparse_categorical_accuracy: 0.7662 - val_loss: 0.7011 - val_sparse_categorical_accuracy: 0.7694\n",
      "Epoch 57/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7367 - sparse_categorical_accuracy: 0.7673\n",
      "Epoch 00057: val_loss did not improve from 0.69717\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7371 - sparse_categorical_accuracy: 0.7671 - val_loss: 0.7003 - val_sparse_categorical_accuracy: 0.7688\n",
      "Epoch 58/100\n",
      "28064/28304 [============================>.] - ETA: 0s - loss: 0.7363 - sparse_categorical_accuracy: 0.7664\n",
      "Epoch 00058: val_loss did not improve from 0.69717\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7368 - sparse_categorical_accuracy: 0.7662 - val_loss: 0.7042 - val_sparse_categorical_accuracy: 0.7674\n",
      "Epoch 59/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.7379 - sparse_categorical_accuracy: 0.7644\n",
      "Epoch 00059: val_loss did not improve from 0.69717\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7377 - sparse_categorical_accuracy: 0.7645 - val_loss: 0.6990 - val_sparse_categorical_accuracy: 0.7713\n",
      "Epoch 60/100\n",
      "27776/28304 [============================>.] - ETA: 0s - loss: 0.7371 - sparse_categorical_accuracy: 0.7665\n",
      "Epoch 00060: val_loss did not improve from 0.69717\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7370 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.6974 - val_sparse_categorical_accuracy: 0.7720\n",
      "Epoch 61/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.7362 - sparse_categorical_accuracy: 0.7670\n",
      "Epoch 00061: val_loss did not improve from 0.69717\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7360 - sparse_categorical_accuracy: 0.7671 - val_loss: 0.7014 - val_sparse_categorical_accuracy: 0.7687\n",
      "Epoch 62/100\n",
      "27712/28304 [============================>.] - ETA: 0s - loss: 0.7337 - sparse_categorical_accuracy: 0.7677\n",
      "Epoch 00062: val_loss did not improve from 0.69717\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7341 - sparse_categorical_accuracy: 0.7676 - val_loss: 0.6994 - val_sparse_categorical_accuracy: 0.7705\n",
      "Epoch 63/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7347 - sparse_categorical_accuracy: 0.7672\n",
      "Epoch 00063: val_loss did not improve from 0.69717\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7345 - sparse_categorical_accuracy: 0.7673 - val_loss: 0.6999 - val_sparse_categorical_accuracy: 0.7732\n",
      "Epoch 64/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7333 - sparse_categorical_accuracy: 0.7665\n",
      "Epoch 00064: val_loss did not improve from 0.69717\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7329 - sparse_categorical_accuracy: 0.7665 - val_loss: 0.6985 - val_sparse_categorical_accuracy: 0.7711\n",
      "Epoch 65/100\n",
      "27552/28304 [============================>.] - ETA: 0s - loss: 0.7373 - sparse_categorical_accuracy: 0.7677\n",
      "Epoch 00065: val_loss did not improve from 0.69717\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7366 - sparse_categorical_accuracy: 0.7677 - val_loss: 0.7002 - val_sparse_categorical_accuracy: 0.7715\n",
      "Epoch 66/100\n",
      "27776/28304 [============================>.] - ETA: 0s - loss: 0.7340 - sparse_categorical_accuracy: 0.7658\n",
      "Epoch 00066: val_loss improved from 0.69717 to 0.69631, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7342 - sparse_categorical_accuracy: 0.7660 - val_loss: 0.6963 - val_sparse_categorical_accuracy: 0.7709\n",
      "Epoch 67/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7361 - sparse_categorical_accuracy: 0.7673\n",
      "Epoch 00067: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7368 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.6984 - val_sparse_categorical_accuracy: 0.7712\n",
      "Epoch 68/100\n",
      "28096/28304 [============================>.] - ETA: 0s - loss: 0.7336 - sparse_categorical_accuracy: 0.7668\n",
      "Epoch 00068: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7341 - sparse_categorical_accuracy: 0.7667 - val_loss: 0.6994 - val_sparse_categorical_accuracy: 0.7728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100\n",
      "27552/28304 [============================>.] - ETA: 0s - loss: 0.7333 - sparse_categorical_accuracy: 0.7676\n",
      "Epoch 00069: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7334 - sparse_categorical_accuracy: 0.7675 - val_loss: 0.7025 - val_sparse_categorical_accuracy: 0.7713\n",
      "Epoch 70/100\n",
      "27872/28304 [============================>.] - ETA: 0s - loss: 0.7339 - sparse_categorical_accuracy: 0.7666\n",
      "Epoch 00070: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7341 - sparse_categorical_accuracy: 0.7667 - val_loss: 0.7002 - val_sparse_categorical_accuracy: 0.7684\n",
      "Epoch 71/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7361 - sparse_categorical_accuracy: 0.7653\n",
      "Epoch 00071: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7368 - sparse_categorical_accuracy: 0.7651 - val_loss: 0.6981 - val_sparse_categorical_accuracy: 0.7728\n",
      "Epoch 72/100\n",
      "28064/28304 [============================>.] - ETA: 0s - loss: 0.7324 - sparse_categorical_accuracy: 0.7657- ETA: 1s - loss: 0.7342 - sparse\n",
      "Epoch 00072: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7329 - sparse_categorical_accuracy: 0.7654 - val_loss: 0.6972 - val_sparse_categorical_accuracy: 0.7736\n",
      "Epoch 73/100\n",
      "28128/28304 [============================>.] - ETA: 0s - loss: 0.7369 - sparse_categorical_accuracy: 0.7673\n",
      "Epoch 00073: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7372 - sparse_categorical_accuracy: 0.7671 - val_loss: 0.6995 - val_sparse_categorical_accuracy: 0.7715\n",
      "Epoch 74/100\n",
      "27872/28304 [============================>.] - ETA: 0s - loss: 0.7344 - sparse_categorical_accuracy: 0.7669\n",
      "Epoch 00074: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7347 - sparse_categorical_accuracy: 0.7668 - val_loss: 0.6996 - val_sparse_categorical_accuracy: 0.7699\n",
      "Epoch 75/100\n",
      "28128/28304 [============================>.] - ETA: 0s - loss: 0.7360 - sparse_categorical_accuracy: 0.7666\n",
      "Epoch 00075: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7360 - sparse_categorical_accuracy: 0.7665 - val_loss: 0.6979 - val_sparse_categorical_accuracy: 0.7723\n",
      "Epoch 76/100\n",
      "27712/28304 [============================>.] - ETA: 0s - loss: 0.7372 - sparse_categorical_accuracy: 0.7649\n",
      "Epoch 00076: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7367 - sparse_categorical_accuracy: 0.7651 - val_loss: 0.6990 - val_sparse_categorical_accuracy: 0.7732\n",
      "Epoch 77/100\n",
      "27840/28304 [============================>.] - ETA: 0s - loss: 0.7345 - sparse_categorical_accuracy: 0.7663- ETA: 1s - loss: 0.7344 - sparse_categoric\n",
      "Epoch 00077: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7352 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.6991 - val_sparse_categorical_accuracy: 0.7723\n",
      "Epoch 78/100\n",
      "28128/28304 [============================>.] - ETA: 0s - loss: 0.7327 - sparse_categorical_accuracy: 0.7670\n",
      "Epoch 00078: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7332 - sparse_categorical_accuracy: 0.7665 - val_loss: 0.6981 - val_sparse_categorical_accuracy: 0.7733\n",
      "Epoch 79/100\n",
      "28032/28304 [============================>.] - ETA: 0s - loss: 0.7333 - sparse_categorical_accuracy: 0.7682- ETA: 1s - loss: 0.7316 - sparse_catego\n",
      "Epoch 00079: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7349 - sparse_categorical_accuracy: 0.7676 - val_loss: 0.7024 - val_sparse_categorical_accuracy: 0.7704\n",
      "Epoch 80/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7377 - sparse_categorical_accuracy: 0.7649\n",
      "Epoch 00080: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7379 - sparse_categorical_accuracy: 0.7648 - val_loss: 0.6977 - val_sparse_categorical_accuracy: 0.7735\n",
      "Epoch 81/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7362 - sparse_categorical_accuracy: 0.7659\n",
      "Epoch 00081: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7363 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.6981 - val_sparse_categorical_accuracy: 0.7711\n",
      "Epoch 82/100\n",
      "28128/28304 [============================>.] - ETA: 0s - loss: 0.7359 - sparse_categorical_accuracy: 0.7670\n",
      "Epoch 00082: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7348 - sparse_categorical_accuracy: 0.7674 - val_loss: 0.6971 - val_sparse_categorical_accuracy: 0.7769\n",
      "Epoch 83/100\n",
      "27904/28304 [============================>.] - ETA: 0s - loss: 0.7330 - sparse_categorical_accuracy: 0.7674\n",
      "Epoch 00083: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7332 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.6986 - val_sparse_categorical_accuracy: 0.7694\n",
      "Epoch 84/100\n",
      "27776/28304 [============================>.] - ETA: 0s - loss: 0.7346 - sparse_categorical_accuracy: 0.7670\n",
      "Epoch 00084: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7344 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.6994 - val_sparse_categorical_accuracy: 0.7719\n",
      "Epoch 85/100\n",
      "28064/28304 [============================>.] - ETA: 0s - loss: 0.7364 - sparse_categorical_accuracy: 0.7661\n",
      "Epoch 00085: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7366 - sparse_categorical_accuracy: 0.7659 - val_loss: 0.7002 - val_sparse_categorical_accuracy: 0.7718\n",
      "Epoch 86/100\n",
      "28128/28304 [============================>.] - ETA: 0s - loss: 0.7350 - sparse_categorical_accuracy: 0.7671\n",
      "Epoch 00086: val_loss did not improve from 0.69631\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7347 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.6977 - val_sparse_categorical_accuracy: 0.7753\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Partial score of fold 2 is: 0.6960890820206409\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_52 (Dense)             (None, 100)               600       \n",
      "_________________________________________________________________\n",
      "layer_normalization_39 (Laye (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_40 (Laye (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_41 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 4)                 104       \n",
      "=================================================================\n",
      "Total params: 7,379\n",
      "Trainable params: 7,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28304 samples, validate on 7076 samples\n",
      "Epoch 1/100\n",
      "27744/28304 [============================>.] - ETA: 0s - loss: 1.0973 - sparse_categorical_accuracy: 0.5365\n",
      "Epoch 00001: val_loss improved from inf to 0.79037, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 3s 89us/sample - loss: 1.0932 - sparse_categorical_accuracy: 0.5396 - val_loss: 0.7904 - val_sparse_categorical_accuracy: 0.7517\n",
      "Epoch 2/100\n",
      "27840/28304 [============================>.] - ETA: 0s - loss: 0.8343 - sparse_categorical_accuracy: 0.7330\n",
      "Epoch 00002: val_loss improved from 0.79037 to 0.72838, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.8331 - sparse_categorical_accuracy: 0.7336 - val_loss: 0.7284 - val_sparse_categorical_accuracy: 0.7651\n",
      "Epoch 3/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7823 - sparse_categorical_accuracy: 0.7519\n",
      "Epoch 00003: val_loss improved from 0.72838 to 0.72149, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7824 - sparse_categorical_accuracy: 0.7519 - val_loss: 0.7215 - val_sparse_categorical_accuracy: 0.7698\n",
      "Epoch 4/100\n",
      "27872/28304 [============================>.] - ETA: 0s - loss: 0.7710 - sparse_categorical_accuracy: 0.7575\n",
      "Epoch 00004: val_loss did not improve from 0.72149\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7707 - sparse_categorical_accuracy: 0.7576 - val_loss: 0.7248 - val_sparse_categorical_accuracy: 0.7614\n",
      "Epoch 5/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7643 - sparse_categorical_accuracy: 0.7588\n",
      "Epoch 00005: val_loss improved from 0.72149 to 0.71616, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7641 - sparse_categorical_accuracy: 0.7590 - val_loss: 0.7162 - val_sparse_categorical_accuracy: 0.7685\n",
      "Epoch 6/100\n",
      "27744/28304 [============================>.] - ETA: 0s - loss: 0.7607 - sparse_categorical_accuracy: 0.7608\n",
      "Epoch 00006: val_loss did not improve from 0.71616\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7603 - sparse_categorical_accuracy: 0.7608 - val_loss: 0.7188 - val_sparse_categorical_accuracy: 0.7684\n",
      "Epoch 7/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7559 - sparse_categorical_accuracy: 0.7618- ETA: 0s - loss: 0.7579 - sparse_categorical_accuracy:\n",
      "Epoch 00007: val_loss improved from 0.71616 to 0.71385, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7566 - sparse_categorical_accuracy: 0.7615 - val_loss: 0.7139 - val_sparse_categorical_accuracy: 0.7682\n",
      "Epoch 8/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7563 - sparse_categorical_accuracy: 0.7616\n",
      "Epoch 00008: val_loss did not improve from 0.71385\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7564 - sparse_categorical_accuracy: 0.7617 - val_loss: 0.7147 - val_sparse_categorical_accuracy: 0.7674\n",
      "Epoch 9/100\n",
      "28032/28304 [============================>.] - ETA: 0s - loss: 0.7527 - sparse_categorical_accuracy: 0.7612\n",
      "Epoch 00009: val_loss improved from 0.71385 to 0.71108, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7534 - sparse_categorical_accuracy: 0.7611 - val_loss: 0.7111 - val_sparse_categorical_accuracy: 0.7705\n",
      "Epoch 10/100\n",
      "27744/28304 [============================>.] - ETA: 0s - loss: 0.7514 - sparse_categorical_accuracy: 0.7634- ETA: 1s - loss: 0.7501 - sparse_\n",
      "Epoch 00010: val_loss did not improve from 0.71108\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7521 - sparse_categorical_accuracy: 0.7632 - val_loss: 0.7116 - val_sparse_categorical_accuracy: 0.7672\n",
      "Epoch 11/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7493 - sparse_categorical_accuracy: 0.7614\n",
      "Epoch 00011: val_loss did not improve from 0.71108\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7506 - sparse_categorical_accuracy: 0.7607 - val_loss: 0.7144 - val_sparse_categorical_accuracy: 0.7641\n",
      "Epoch 12/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7481 - sparse_categorical_accuracy: 0.7632\n",
      "Epoch 00012: val_loss did not improve from 0.71108\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7480 - sparse_categorical_accuracy: 0.7632 - val_loss: 0.7182 - val_sparse_categorical_accuracy: 0.7663\n",
      "Epoch 13/100\n",
      "28256/28304 [============================>.] - ETA: 0s - loss: 0.7493 - sparse_categorical_accuracy: 0.7631- ETA: 1s - loss: 0.7456 - sparse_categorical_accuracy - ETA: 0s - loss: 0.7553 - sparse_categorical_acc\n",
      "Epoch 00013: val_loss improved from 0.71108 to 0.71056, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7494 - sparse_categorical_accuracy: 0.7630 - val_loss: 0.7106 - val_sparse_categorical_accuracy: 0.7691\n",
      "Epoch 14/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7481 - sparse_categorical_accuracy: 0.7613\n",
      "Epoch 00014: val_loss improved from 0.71056 to 0.70832, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7477 - sparse_categorical_accuracy: 0.7613 - val_loss: 0.7083 - val_sparse_categorical_accuracy: 0.7713\n",
      "Epoch 15/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7490 - sparse_categorical_accuracy: 0.7622\n",
      "Epoch 00015: val_loss did not improve from 0.70832\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7489 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.7110 - val_sparse_categorical_accuracy: 0.7694\n",
      "Epoch 16/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7500 - sparse_categorical_accuracy: 0.7633\n",
      "Epoch 00016: val_loss did not improve from 0.70832\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7498 - sparse_categorical_accuracy: 0.7634 - val_loss: 0.7084 - val_sparse_categorical_accuracy: 0.7704\n",
      "Epoch 17/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7489 - sparse_categorical_accuracy: 0.7623- ETA: 0s - loss: 0.7553 - sparse_categorical_ac\n",
      "Epoch 00017: val_loss improved from 0.70832 to 0.70651, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7474 - sparse_categorical_accuracy: 0.7626 - val_loss: 0.7065 - val_sparse_categorical_accuracy: 0.7713\n",
      "Epoch 18/100\n",
      "27584/28304 [============================>.] - ETA: 0s - loss: 0.7415 - sparse_categorical_accuracy: 0.7639\n",
      "Epoch 00018: val_loss improved from 0.70651 to 0.70557, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7444 - sparse_categorical_accuracy: 0.7625 - val_loss: 0.7056 - val_sparse_categorical_accuracy: 0.7692\n",
      "Epoch 19/100\n",
      "27776/28304 [============================>.] - ETA: 0s - loss: 0.7461 - sparse_categorical_accuracy: 0.7645\n",
      "Epoch 00019: val_loss improved from 0.70557 to 0.70494, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7455 - sparse_categorical_accuracy: 0.7648 - val_loss: 0.7049 - val_sparse_categorical_accuracy: 0.7701\n",
      "Epoch 20/100\n",
      "27584/28304 [============================>.] - ETA: 0s - loss: 0.7423 - sparse_categorical_accuracy: 0.7646- ETA: 0s - loss: 0.7428 - sparse_categorical_accuracy: \n",
      "Epoch 00020: val_loss did not improve from 0.70494\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7424 - sparse_categorical_accuracy: 0.7643 - val_loss: 0.7101 - val_sparse_categorical_accuracy: 0.7716\n",
      "Epoch 21/100\n",
      "28000/28304 [============================>.] - ETA: 0s - loss: 0.7442 - sparse_categorical_accuracy: 0.7644\n",
      "Epoch 00021: val_loss improved from 0.70494 to 0.70430, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7447 - sparse_categorical_accuracy: 0.7640 - val_loss: 0.7043 - val_sparse_categorical_accuracy: 0.7689\n",
      "Epoch 22/100\n",
      "28096/28304 [============================>.] - ETA: 0s - loss: 0.7433 - sparse_categorical_accuracy: 0.7656\n",
      "Epoch 00022: val_loss did not improve from 0.70430\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7438 - sparse_categorical_accuracy: 0.7653 - val_loss: 0.7047 - val_sparse_categorical_accuracy: 0.7688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7423 - sparse_categorical_accuracy: 0.7654\n",
      "Epoch 00023: val_loss improved from 0.70430 to 0.70360, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7433 - sparse_categorical_accuracy: 0.7649 - val_loss: 0.7036 - val_sparse_categorical_accuracy: 0.7701\n",
      "Epoch 24/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7424 - sparse_categorical_accuracy: 0.7638\n",
      "Epoch 00024: val_loss did not improve from 0.70360\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7404 - sparse_categorical_accuracy: 0.7647 - val_loss: 0.7056 - val_sparse_categorical_accuracy: 0.7694\n",
      "Epoch 25/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7416 - sparse_categorical_accuracy: 0.7657\n",
      "Epoch 00025: val_loss improved from 0.70360 to 0.70286, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7428 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.7029 - val_sparse_categorical_accuracy: 0.7708\n",
      "Epoch 26/100\n",
      "27872/28304 [============================>.] - ETA: 0s - loss: 0.7418 - sparse_categorical_accuracy: 0.7637\n",
      "Epoch 00026: val_loss did not improve from 0.70286\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7421 - sparse_categorical_accuracy: 0.7637 - val_loss: 0.7062 - val_sparse_categorical_accuracy: 0.7698\n",
      "Epoch 27/100\n",
      "27552/28304 [============================>.] - ETA: 0s - loss: 0.7412 - sparse_categorical_accuracy: 0.7650\n",
      "Epoch 00027: val_loss did not improve from 0.70286\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7417 - sparse_categorical_accuracy: 0.7650 - val_loss: 0.7031 - val_sparse_categorical_accuracy: 0.7688\n",
      "Epoch 28/100\n",
      "27712/28304 [============================>.] - ETA: 0s - loss: 0.7406 - sparse_categorical_accuracy: 0.7657\n",
      "Epoch 00028: val_loss did not improve from 0.70286\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7404 - sparse_categorical_accuracy: 0.7658 - val_loss: 0.7074 - val_sparse_categorical_accuracy: 0.7711\n",
      "Epoch 29/100\n",
      "27616/28304 [============================>.] - ETA: 0s - loss: 0.7393 - sparse_categorical_accuracy: 0.7684\n",
      "Epoch 00029: val_loss did not improve from 0.70286\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7412 - sparse_categorical_accuracy: 0.7680 - val_loss: 0.7043 - val_sparse_categorical_accuracy: 0.7718\n",
      "Epoch 30/100\n",
      "27616/28304 [============================>.] - ETA: 0s - loss: 0.7377 - sparse_categorical_accuracy: 0.7659\n",
      "Epoch 00030: val_loss did not improve from 0.70286\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7393 - sparse_categorical_accuracy: 0.7654 - val_loss: 0.7070 - val_sparse_categorical_accuracy: 0.7702\n",
      "Epoch 31/100\n",
      "27616/28304 [============================>.] - ETA: 0s - loss: 0.7411 - sparse_categorical_accuracy: 0.7667\n",
      "Epoch 00031: val_loss did not improve from 0.70286\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7398 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.7049 - val_sparse_categorical_accuracy: 0.7689\n",
      "Epoch 32/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7389 - sparse_categorical_accuracy: 0.7661- ETA: 1s - loss: 0.7411 - sparse_c\n",
      "Epoch 00032: val_loss improved from 0.70286 to 0.70185, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7387 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.7018 - val_sparse_categorical_accuracy: 0.7712\n",
      "Epoch 33/100\n",
      "27872/28304 [============================>.] - ETA: 0s - loss: 0.7386 - sparse_categorical_accuracy: 0.7660\n",
      "Epoch 00033: val_loss did not improve from 0.70185\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7389 - sparse_categorical_accuracy: 0.7657 - val_loss: 0.7024 - val_sparse_categorical_accuracy: 0.7718\n",
      "Epoch 34/100\n",
      "27680/28304 [============================>.] - ETA: 0s - loss: 0.7434 - sparse_categorical_accuracy: 0.7653\n",
      "Epoch 00034: val_loss did not improve from 0.70185\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7417 - sparse_categorical_accuracy: 0.7658 - val_loss: 0.7022 - val_sparse_categorical_accuracy: 0.7723\n",
      "Epoch 35/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7416 - sparse_categorical_accuracy: 0.7651\n",
      "Epoch 00035: val_loss did not improve from 0.70185\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7416 - sparse_categorical_accuracy: 0.7651 - val_loss: 0.7037 - val_sparse_categorical_accuracy: 0.7701\n",
      "Epoch 36/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7349 - sparse_categorical_accuracy: 0.7656\n",
      "Epoch 00036: val_loss did not improve from 0.70185\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7354 - sparse_categorical_accuracy: 0.7654 - val_loss: 0.7057 - val_sparse_categorical_accuracy: 0.7706\n",
      "Epoch 37/100\n",
      "27712/28304 [============================>.] - ETA: 0s - loss: 0.7368 - sparse_categorical_accuracy: 0.7651\n",
      "Epoch 00037: val_loss did not improve from 0.70185\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7382 - sparse_categorical_accuracy: 0.7646 - val_loss: 0.7224 - val_sparse_categorical_accuracy: 0.7685\n",
      "Epoch 38/100\n",
      "28000/28304 [============================>.] - ETA: 0s - loss: 0.7379 - sparse_categorical_accuracy: 0.7672\n",
      "Epoch 00038: val_loss did not improve from 0.70185\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7387 - sparse_categorical_accuracy: 0.7667 - val_loss: 0.7031 - val_sparse_categorical_accuracy: 0.7692\n",
      "Epoch 39/100\n",
      "28064/28304 [============================>.] - ETA: 0s - loss: 0.7399 - sparse_categorical_accuracy: 0.7663- ETA: 1s - loss: 0.7317 - sparse_c\n",
      "Epoch 00039: val_loss improved from 0.70185 to 0.70152, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7398 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.7015 - val_sparse_categorical_accuracy: 0.7705\n",
      "Epoch 40/100\n",
      "28064/28304 [============================>.] - ETA: 0s - loss: 0.7373 - sparse_categorical_accuracy: 0.7667\n",
      "Epoch 00040: val_loss did not improve from 0.70152\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7373 - sparse_categorical_accuracy: 0.7666 - val_loss: 0.7016 - val_sparse_categorical_accuracy: 0.7723\n",
      "Epoch 41/100\n",
      "27904/28304 [============================>.] - ETA: 0s - loss: 0.7408 - sparse_categorical_accuracy: 0.7654\n",
      "Epoch 00041: val_loss improved from 0.70152 to 0.70137, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7394 - sparse_categorical_accuracy: 0.7660 - val_loss: 0.7014 - val_sparse_categorical_accuracy: 0.7704\n",
      "Epoch 42/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7382 - sparse_categorical_accuracy: 0.7654\n",
      "Epoch 00042: val_loss improved from 0.70137 to 0.70134, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7384 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.7013 - val_sparse_categorical_accuracy: 0.7692\n",
      "Epoch 43/100\n",
      "27680/28304 [============================>.] - ETA: 0s - loss: 0.7374 - sparse_categorical_accuracy: 0.7662\n",
      "Epoch 00043: val_loss improved from 0.70134 to 0.70064, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7375 - sparse_categorical_accuracy: 0.7663 - val_loss: 0.7006 - val_sparse_categorical_accuracy: 0.7701\n",
      "Epoch 44/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7386 - sparse_categorical_accuracy: 0.7667\n",
      "Epoch 00044: val_loss did not improve from 0.70064\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7387 - sparse_categorical_accuracy: 0.7667 - val_loss: 0.7027 - val_sparse_categorical_accuracy: 0.7695\n",
      "Epoch 45/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7395 - sparse_categorical_accuracy: 0.7660\n",
      "Epoch 00045: val_loss did not improve from 0.70064\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7390 - sparse_categorical_accuracy: 0.7663 - val_loss: 0.7036 - val_sparse_categorical_accuracy: 0.7698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100\n",
      "28032/28304 [============================>.] - ETA: 0s - loss: 0.7378 - sparse_categorical_accuracy: 0.7656\n",
      "Epoch 00046: val_loss did not improve from 0.70064\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7376 - sparse_categorical_accuracy: 0.7656 - val_loss: 0.7023 - val_sparse_categorical_accuracy: 0.7702\n",
      "Epoch 47/100\n",
      "28128/28304 [============================>.] - ETA: 0s - loss: 0.7378 - sparse_categorical_accuracy: 0.7678\n",
      "Epoch 00047: val_loss improved from 0.70064 to 0.70018, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 80us/sample - loss: 0.7378 - sparse_categorical_accuracy: 0.7677 - val_loss: 0.7002 - val_sparse_categorical_accuracy: 0.7675\n",
      "Epoch 48/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7379 - sparse_categorical_accuracy: 0.7666\n",
      "Epoch 00048: val_loss did not improve from 0.70018\n",
      "28304/28304 [==============================] - 2s 79us/sample - loss: 0.7377 - sparse_categorical_accuracy: 0.7668 - val_loss: 0.7025 - val_sparse_categorical_accuracy: 0.7681\n",
      "Epoch 49/100\n",
      "27552/28304 [============================>.] - ETA: 0s - loss: 0.7394 - sparse_categorical_accuracy: 0.7651\n",
      "Epoch 00049: val_loss did not improve from 0.70018\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7385 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.7011 - val_sparse_categorical_accuracy: 0.7696\n",
      "Epoch 50/100\n",
      "27616/28304 [============================>.] - ETA: 0s - loss: 0.7355 - sparse_categorical_accuracy: 0.7667\n",
      "Epoch 00050: val_loss did not improve from 0.70018\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7370 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.7013 - val_sparse_categorical_accuracy: 0.7705\n",
      "Epoch 51/100\n",
      "27552/28304 [============================>.] - ETA: 0s - loss: 0.7421 - sparse_categorical_accuracy: 0.7666- ETA: 0s - loss: 0.7531 - sparse_categorical\n",
      "Epoch 00051: val_loss did not improve from 0.70018\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7413 - sparse_categorical_accuracy: 0.7671 - val_loss: 0.7026 - val_sparse_categorical_accuracy: 0.7702\n",
      "Epoch 52/100\n",
      "27744/28304 [============================>.] - ETA: 0s - loss: 0.7370 - sparse_categorical_accuracy: 0.7654\n",
      "Epoch 00052: val_loss did not improve from 0.70018\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7382 - sparse_categorical_accuracy: 0.7649 - val_loss: 0.7045 - val_sparse_categorical_accuracy: 0.7668\n",
      "Epoch 53/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7398 - sparse_categorical_accuracy: 0.7659\n",
      "Epoch 00053: val_loss did not improve from 0.70018\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7396 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.7007 - val_sparse_categorical_accuracy: 0.7695\n",
      "Epoch 54/100\n",
      "27552/28304 [============================>.] - ETA: 0s - loss: 0.7384 - sparse_categorical_accuracy: 0.7644\n",
      "Epoch 00054: val_loss did not improve from 0.70018\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7371 - sparse_categorical_accuracy: 0.7650 - val_loss: 0.7025 - val_sparse_categorical_accuracy: 0.7681\n",
      "Epoch 55/100\n",
      "28096/28304 [============================>.] - ETA: 0s - loss: 0.7346 - sparse_categorical_accuracy: 0.7668\n",
      "Epoch 00055: val_loss did not improve from 0.70018\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7350 - sparse_categorical_accuracy: 0.7667 - val_loss: 0.7006 - val_sparse_categorical_accuracy: 0.7678\n",
      "Epoch 56/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7366 - sparse_categorical_accuracy: 0.7666\n",
      "Epoch 00056: val_loss did not improve from 0.70018\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7373 - sparse_categorical_accuracy: 0.7666 - val_loss: 0.7027 - val_sparse_categorical_accuracy: 0.7696\n",
      "Epoch 57/100\n",
      "27840/28304 [============================>.] - ETA: 0s - loss: 0.7369 - sparse_categorical_accuracy: 0.7660\n",
      "Epoch 00057: val_loss did not improve from 0.70018\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7369 - sparse_categorical_accuracy: 0.7659 - val_loss: 0.7029 - val_sparse_categorical_accuracy: 0.7684\n",
      "Epoch 58/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7344 - sparse_categorical_accuracy: 0.7670\n",
      "Epoch 00058: val_loss improved from 0.70018 to 0.70005, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7350 - sparse_categorical_accuracy: 0.7669 - val_loss: 0.7001 - val_sparse_categorical_accuracy: 0.7719\n",
      "Epoch 59/100\n",
      "28032/28304 [============================>.] - ETA: 0s - loss: 0.7389 - sparse_categorical_accuracy: 0.7667\n",
      "Epoch 00059: val_loss did not improve from 0.70005\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7382 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.7022 - val_sparse_categorical_accuracy: 0.7708\n",
      "Epoch 60/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7364 - sparse_categorical_accuracy: 0.7666\n",
      "Epoch 00060: val_loss did not improve from 0.70005\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7362 - sparse_categorical_accuracy: 0.7666 - val_loss: 0.7019 - val_sparse_categorical_accuracy: 0.7698\n",
      "Epoch 61/100\n",
      "27616/28304 [============================>.] - ETA: 0s - loss: 0.7387 - sparse_categorical_accuracy: 0.7660- ETA: 0s - loss: 0.7380 - sparse_categorical_accuracy: 0.76\n",
      "Epoch 00061: val_loss did not improve from 0.70005\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7387 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.7006 - val_sparse_categorical_accuracy: 0.7709\n",
      "Epoch 62/100\n",
      "28256/28304 [============================>.] - ETA: 0s - loss: 0.7368 - sparse_categorical_accuracy: 0.7665\n",
      "Epoch 00062: val_loss did not improve from 0.70005\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7365 - sparse_categorical_accuracy: 0.7666 - val_loss: 0.7012 - val_sparse_categorical_accuracy: 0.7706\n",
      "Epoch 63/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7351 - sparse_categorical_accuracy: 0.7664\n",
      "Epoch 00063: val_loss did not improve from 0.70005\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7349 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.7002 - val_sparse_categorical_accuracy: 0.7689\n",
      "Epoch 64/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7366 - sparse_categorical_accuracy: 0.7665\n",
      "Epoch 00064: val_loss did not improve from 0.70005\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7370 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.7027 - val_sparse_categorical_accuracy: 0.7698\n",
      "Epoch 65/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7356 - sparse_categorical_accuracy: 0.7682\n",
      "Epoch 00065: val_loss did not improve from 0.70005\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7360 - sparse_categorical_accuracy: 0.7681 - val_loss: 0.7015 - val_sparse_categorical_accuracy: 0.7695\n",
      "Epoch 66/100\n",
      "28032/28304 [============================>.] - ETA: 0s - loss: 0.7376 - sparse_categorical_accuracy: 0.7670\n",
      "Epoch 00066: val_loss improved from 0.70005 to 0.69951, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 82us/sample - loss: 0.7370 - sparse_categorical_accuracy: 0.7673 - val_loss: 0.6995 - val_sparse_categorical_accuracy: 0.7715\n",
      "Epoch 67/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7368 - sparse_categorical_accuracy: 0.7673\n",
      "Epoch 00067: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7367 - sparse_categorical_accuracy: 0.7671 - val_loss: 0.7011 - val_sparse_categorical_accuracy: 0.7698\n",
      "Epoch 68/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7387 - sparse_categorical_accuracy: 0.7661\n",
      "Epoch 00068: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 79us/sample - loss: 0.7386 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.6996 - val_sparse_categorical_accuracy: 0.7713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7363 - sparse_categorical_accuracy: 0.7663- ETA: 0s - loss: 0.7390 - sparse_categorical_accuracy: 0.\n",
      "Epoch 00069: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7363 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.7014 - val_sparse_categorical_accuracy: 0.7696\n",
      "Epoch 70/100\n",
      "27840/28304 [============================>.] - ETA: 0s - loss: 0.7358 - sparse_categorical_accuracy: 0.7660\n",
      "Epoch 00070: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7355 - sparse_categorical_accuracy: 0.7663 - val_loss: 0.7021 - val_sparse_categorical_accuracy: 0.7689\n",
      "Epoch 71/100\n",
      "27680/28304 [============================>.] - ETA: 0s - loss: 0.7352 - sparse_categorical_accuracy: 0.767 - ETA: 0s - loss: 0.7343 - sparse_categorical_accuracy: 0.7674\n",
      "Epoch 00071: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7350 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.7002 - val_sparse_categorical_accuracy: 0.7701\n",
      "Epoch 72/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7380 - sparse_categorical_accuracy: 0.7671\n",
      "Epoch 00072: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7376 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.7011 - val_sparse_categorical_accuracy: 0.7687\n",
      "Epoch 73/100\n",
      "28000/28304 [============================>.] - ETA: 0s - loss: 0.7374 - sparse_categorical_accuracy: 0.7666\n",
      "Epoch 00073: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7373 - sparse_categorical_accuracy: 0.7667 - val_loss: 0.6998 - val_sparse_categorical_accuracy: 0.7699\n",
      "Epoch 74/100\n",
      "27552/28304 [============================>.] - ETA: 0s - loss: 0.7330 - sparse_categorical_accuracy: 0.7673\n",
      "Epoch 00074: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7348 - sparse_categorical_accuracy: 0.7669 - val_loss: 0.7010 - val_sparse_categorical_accuracy: 0.7713\n",
      "Epoch 75/100\n",
      "27584/28304 [============================>.] - ETA: 0s - loss: 0.7299 - sparse_categorical_accuracy: 0.7672\n",
      "Epoch 00075: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7312 - sparse_categorical_accuracy: 0.7668 - val_loss: 0.7045 - val_sparse_categorical_accuracy: 0.7643\n",
      "Epoch 76/100\n",
      "27904/28304 [============================>.] - ETA: 0s - loss: 0.7384 - sparse_categorical_accuracy: 0.7664\n",
      "Epoch 00076: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7388 - sparse_categorical_accuracy: 0.7663 - val_loss: 0.7024 - val_sparse_categorical_accuracy: 0.7688\n",
      "Epoch 77/100\n",
      "28096/28304 [============================>.] - ETA: 0s - loss: 0.7358 - sparse_categorical_accuracy: 0.7672\n",
      "Epoch 00077: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7362 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.7001 - val_sparse_categorical_accuracy: 0.7702\n",
      "Epoch 78/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7343 - sparse_categorical_accuracy: 0.7678\n",
      "Epoch 00078: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7348 - sparse_categorical_accuracy: 0.7675 - val_loss: 0.7007 - val_sparse_categorical_accuracy: 0.7711\n",
      "Epoch 79/100\n",
      "27616/28304 [============================>.] - ETA: 0s - loss: 0.7377 - sparse_categorical_accuracy: 0.7661\n",
      "Epoch 00079: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7385 - sparse_categorical_accuracy: 0.7657 - val_loss: 0.7007 - val_sparse_categorical_accuracy: 0.7704\n",
      "Epoch 80/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.7351 - sparse_categorical_accuracy: 0.7665\n",
      "Epoch 00080: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7348 - sparse_categorical_accuracy: 0.7667 - val_loss: 0.7001 - val_sparse_categorical_accuracy: 0.7691\n",
      "Epoch 81/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7347 - sparse_categorical_accuracy: 0.7679- ETA: 1s - loss: 0.7316 - sparse\n",
      "Epoch 00081: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7338 - sparse_categorical_accuracy: 0.7682 - val_loss: 0.7024 - val_sparse_categorical_accuracy: 0.7709\n",
      "Epoch 82/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7362 - sparse_categorical_accuracy: 0.7673\n",
      "Epoch 00082: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7360 - sparse_categorical_accuracy: 0.7675 - val_loss: 0.7042 - val_sparse_categorical_accuracy: 0.7672\n",
      "Epoch 83/100\n",
      "28096/28304 [============================>.] - ETA: 0s - loss: 0.7316 - sparse_categorical_accuracy: 0.7659\n",
      "Epoch 00083: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7312 - sparse_categorical_accuracy: 0.7660 - val_loss: 0.7030 - val_sparse_categorical_accuracy: 0.7682\n",
      "Epoch 84/100\n",
      "27552/28304 [============================>.] - ETA: 0s - loss: 0.7344 - sparse_categorical_accuracy: 0.7690\n",
      "Epoch 00084: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7351 - sparse_categorical_accuracy: 0.7684 - val_loss: 0.7019 - val_sparse_categorical_accuracy: 0.7698\n",
      "Epoch 85/100\n",
      "27552/28304 [============================>.] - ETA: 0s - loss: 0.7377 - sparse_categorical_accuracy: 0.7662- ETA: 1s - loss: 0.7353 - sparse_categoric\n",
      "Epoch 00085: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7374 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.7010 - val_sparse_categorical_accuracy: 0.7699\n",
      "Epoch 86/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7352 - sparse_categorical_accuracy: 0.7680\n",
      "Epoch 00086: val_loss did not improve from 0.69951\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7352 - sparse_categorical_accuracy: 0.7679 - val_loss: 0.7012 - val_sparse_categorical_accuracy: 0.7704\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Partial score of fold 3 is: 0.6842878919860627\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_56 (Dense)             (None, 100)               600       \n",
      "_________________________________________________________________\n",
      "layer_normalization_42 (Laye (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_43 (Laye (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_44 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 4)                 104       \n",
      "=================================================================\n",
      "Total params: 7,379\n",
      "Trainable params: 7,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28304 samples, validate on 7076 samples\n",
      "Epoch 1/100\n",
      "27616/28304 [============================>.] - ETA: 0s - loss: 1.1213 - sparse_categorical_accuracy: 0.5197\n",
      "Epoch 00001: val_loss improved from inf to 0.89337, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 3s 92us/sample - loss: 1.1162 - sparse_categorical_accuracy: 0.5236 - val_loss: 0.8934 - val_sparse_categorical_accuracy: 0.7321\n",
      "Epoch 2/100\n",
      "27744/28304 [============================>.] - ETA: 0s - loss: 0.8467 - sparse_categorical_accuracy: 0.7243- ETA: 0s - loss: 0.8694 - sparse_categorical_ac\n",
      "Epoch 00002: val_loss improved from 0.89337 to 0.76631, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.8455 - sparse_categorical_accuracy: 0.7247 - val_loss: 0.7663 - val_sparse_categorical_accuracy: 0.7523\n",
      "Epoch 3/100\n",
      "28000/28304 [============================>.] - ETA: 0s - loss: 0.7818 - sparse_categorical_accuracy: 0.7518- ETA: 1s - loss: 0.8009 - sparse_c\n",
      "Epoch 00003: val_loss improved from 0.76631 to 0.75640, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 3s 89us/sample - loss: 0.7819 - sparse_categorical_accuracy: 0.7519 - val_loss: 0.7564 - val_sparse_categorical_accuracy: 0.7568\n",
      "Epoch 4/100\n",
      "27680/28304 [============================>.] - ETA: 0s - loss: 0.7686 - sparse_categorical_accuracy: 0.7556\n",
      "Epoch 00004: val_loss improved from 0.75640 to 0.75164, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 86us/sample - loss: 0.7688 - sparse_categorical_accuracy: 0.7556 - val_loss: 0.7516 - val_sparse_categorical_accuracy: 0.7583\n",
      "Epoch 5/100\n",
      "27840/28304 [============================>.] - ETA: 0s - loss: 0.7609 - sparse_categorical_accuracy: 0.7572\n",
      "Epoch 00005: val_loss improved from 0.75164 to 0.74954, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 88us/sample - loss: 0.7605 - sparse_categorical_accuracy: 0.7575 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.7599\n",
      "Epoch 6/100\n",
      "28000/28304 [============================>.] - ETA: 0s - loss: 0.7559 - sparse_categorical_accuracy: 0.7605\n",
      "Epoch 00006: val_loss improved from 0.74954 to 0.74778, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 83us/sample - loss: 0.7554 - sparse_categorical_accuracy: 0.7606 - val_loss: 0.7478 - val_sparse_categorical_accuracy: 0.7607\n",
      "Epoch 7/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.7527 - sparse_categorical_accuracy: 0.7623\n",
      "Epoch 00007: val_loss improved from 0.74778 to 0.74464, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7527 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.7446 - val_sparse_categorical_accuracy: 0.7606\n",
      "Epoch 8/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7475 - sparse_categorical_accuracy: 0.7633\n",
      "Epoch 00008: val_loss did not improve from 0.74464\n",
      "28304/28304 [==============================] - 2s 79us/sample - loss: 0.7486 - sparse_categorical_accuracy: 0.7627 - val_loss: 0.7452 - val_sparse_categorical_accuracy: 0.7593\n",
      "Epoch 9/100\n",
      "27840/28304 [============================>.] - ETA: 0s - loss: 0.7470 - sparse_categorical_accuracy: 0.7634\n",
      "Epoch 00009: val_loss improved from 0.74464 to 0.74382, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 83us/sample - loss: 0.7459 - sparse_categorical_accuracy: 0.7637 - val_loss: 0.7438 - val_sparse_categorical_accuracy: 0.7602\n",
      "Epoch 10/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7437 - sparse_categorical_accuracy: 0.7650\n",
      "Epoch 00010: val_loss did not improve from 0.74382\n",
      "28304/28304 [==============================] - 2s 84us/sample - loss: 0.7430 - sparse_categorical_accuracy: 0.7653 - val_loss: 0.7456 - val_sparse_categorical_accuracy: 0.7598\n",
      "Epoch 11/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7466 - sparse_categorical_accuracy: 0.7645- ETA: 0s - loss: 0.7482 - sparse_categorical_accuracy: \n",
      "Epoch 00011: val_loss improved from 0.74382 to 0.74073, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 85us/sample - loss: 0.7463 - sparse_categorical_accuracy: 0.7646 - val_loss: 0.7407 - val_sparse_categorical_accuracy: 0.7575\n",
      "Epoch 12/100\n",
      "27872/28304 [============================>.] - ETA: 0s - loss: 0.7441 - sparse_categorical_accuracy: 0.7648- ETA: 1s - loss: 0.7319 - sparse_cate\n",
      "Epoch 00012: val_loss improved from 0.74073 to 0.73850, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 85us/sample - loss: 0.7445 - sparse_categorical_accuracy: 0.7647 - val_loss: 0.7385 - val_sparse_categorical_accuracy: 0.7593\n",
      "Epoch 13/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7449 - sparse_categorical_accuracy: 0.7634\n",
      "Epoch 00013: val_loss did not improve from 0.73850\n",
      "28304/28304 [==============================] - 2s 83us/sample - loss: 0.7450 - sparse_categorical_accuracy: 0.7634 - val_loss: 0.7410 - val_sparse_categorical_accuracy: 0.7595\n",
      "Epoch 14/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7424 - sparse_categorical_accuracy: 0.7653\n",
      "Epoch 00014: val_loss did not improve from 0.73850\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7423 - sparse_categorical_accuracy: 0.7649 - val_loss: 0.7426 - val_sparse_categorical_accuracy: 0.7593\n",
      "Epoch 15/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7404 - sparse_categorical_accuracy: 0.7651\n",
      "Epoch 00015: val_loss did not improve from 0.73850\n",
      "28304/28304 [==============================] - 2s 83us/sample - loss: 0.7404 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.7432 - val_sparse_categorical_accuracy: 0.7585\n",
      "Epoch 16/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7421 - sparse_categorical_accuracy: 0.7657\n",
      "Epoch 00016: val_loss did not improve from 0.73850\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7402 - sparse_categorical_accuracy: 0.7666 - val_loss: 0.7410 - val_sparse_categorical_accuracy: 0.7573\n",
      "Epoch 17/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7404 - sparse_categorical_accuracy: 0.7651- ETA: 0s - loss: 0.7403 - sparse_categorical_a\n",
      "Epoch 00017: val_loss did not improve from 0.73850\n",
      "28304/28304 [==============================] - 2s 82us/sample - loss: 0.7403 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.7390 - val_sparse_categorical_accuracy: 0.7590\n",
      "Epoch 18/100\n",
      "27680/28304 [============================>.] - ETA: 0s - loss: 0.7373 - sparse_categorical_accuracy: 0.7665\n",
      "Epoch 00018: val_loss did not improve from 0.73850\n",
      "28304/28304 [==============================] - 2s 79us/sample - loss: 0.7392 - sparse_categorical_accuracy: 0.7655 - val_loss: 0.7389 - val_sparse_categorical_accuracy: 0.7586\n",
      "Epoch 19/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7390 - sparse_categorical_accuracy: 0.7667- ETA: 1s - loss: 0.7261 - sp\n",
      "Epoch 00019: val_loss improved from 0.73850 to 0.73809, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 86us/sample - loss: 0.7388 - sparse_categorical_accuracy: 0.7667 - val_loss: 0.7381 - val_sparse_categorical_accuracy: 0.7586\n",
      "Epoch 20/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7388 - sparse_categorical_accuracy: 0.7649\n",
      "Epoch 00020: val_loss did not improve from 0.73809\n",
      "28304/28304 [==============================] - 2s 83us/sample - loss: 0.7388 - sparse_categorical_accuracy: 0.7648 - val_loss: 0.7415 - val_sparse_categorical_accuracy: 0.7590\n",
      "Epoch 21/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7382 - sparse_categorical_accuracy: 0.7657\n",
      "Epoch 00021: val_loss did not improve from 0.73809\n",
      "28304/28304 [==============================] - 2s 84us/sample - loss: 0.7384 - sparse_categorical_accuracy: 0.7657 - val_loss: 0.7402 - val_sparse_categorical_accuracy: 0.7562\n",
      "Epoch 22/100\n",
      "27680/28304 [============================>.] - ETA: 0s - loss: 0.7405 - sparse_categorical_accuracy: 0.7674\n",
      "Epoch 00022: val_loss improved from 0.73809 to 0.73600, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 85us/sample - loss: 0.7414 - sparse_categorical_accuracy: 0.7671 - val_loss: 0.7360 - val_sparse_categorical_accuracy: 0.7593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "28000/28304 [============================>.] - ETA: 0s - loss: 0.7343 - sparse_categorical_accuracy: 0.7681\n",
      "Epoch 00023: val_loss did not improve from 0.73600\n",
      "28304/28304 [==============================] - 2s 79us/sample - loss: 0.7339 - sparse_categorical_accuracy: 0.7682 - val_loss: 0.7365 - val_sparse_categorical_accuracy: 0.7605\n",
      "Epoch 24/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7341 - sparse_categorical_accuracy: 0.7665\n",
      "Epoch 00024: val_loss did not improve from 0.73600\n",
      "28304/28304 [==============================] - 2s 82us/sample - loss: 0.7344 - sparse_categorical_accuracy: 0.7663 - val_loss: 0.7366 - val_sparse_categorical_accuracy: 0.7592\n",
      "Epoch 25/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7373 - sparse_categorical_accuracy: 0.7662\n",
      "Epoch 00025: val_loss did not improve from 0.73600\n",
      "28304/28304 [==============================] - 2s 82us/sample - loss: 0.7372 - sparse_categorical_accuracy: 0.7663 - val_loss: 0.7385 - val_sparse_categorical_accuracy: 0.7590\n",
      "Epoch 26/100\n",
      "28096/28304 [============================>.] - ETA: 0s - loss: 0.7371 - sparse_categorical_accuracy: 0.7669\n",
      "Epoch 00026: val_loss did not improve from 0.73600\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7362 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.7369 - val_sparse_categorical_accuracy: 0.7586\n",
      "Epoch 27/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7358 - sparse_categorical_accuracy: 0.7679\n",
      "Epoch 00027: val_loss did not improve from 0.73600\n",
      "28304/28304 [==============================] - 2s 85us/sample - loss: 0.7357 - sparse_categorical_accuracy: 0.7679 - val_loss: 0.7371 - val_sparse_categorical_accuracy: 0.7582\n",
      "Epoch 28/100\n",
      "28064/28304 [============================>.] - ETA: 0s - loss: 0.7348 - sparse_categorical_accuracy: 0.7675\n",
      "Epoch 00028: val_loss improved from 0.73600 to 0.73525, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 84us/sample - loss: 0.7349 - sparse_categorical_accuracy: 0.7674 - val_loss: 0.7352 - val_sparse_categorical_accuracy: 0.7585\n",
      "Epoch 29/100\n",
      "27872/28304 [============================>.] - ETA: 0s - loss: 0.7347 - sparse_categorical_accuracy: 0.7674\n",
      "Epoch 00029: val_loss did not improve from 0.73525\n",
      "28304/28304 [==============================] - 2s 80us/sample - loss: 0.7351 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.7354 - val_sparse_categorical_accuracy: 0.7578\n",
      "Epoch 30/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7344 - sparse_categorical_accuracy: 0.7655\n",
      "Epoch 00030: val_loss improved from 0.73525 to 0.73518, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 83us/sample - loss: 0.7340 - sparse_categorical_accuracy: 0.7659 - val_loss: 0.7352 - val_sparse_categorical_accuracy: 0.7593\n",
      "Epoch 31/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7321 - sparse_categorical_accuracy: 0.7665\n",
      "Epoch 00031: val_loss did not improve from 0.73518\n",
      "28304/28304 [==============================] - 2s 79us/sample - loss: 0.7318 - sparse_categorical_accuracy: 0.7668 - val_loss: 0.7401 - val_sparse_categorical_accuracy: 0.7583\n",
      "Epoch 32/100\n",
      "27840/28304 [============================>.] - ETA: 0s - loss: 0.7349 - sparse_categorical_accuracy: 0.7690- ETA: 0s - loss: 0.7368 - sparse_categorical_accuracy: 0.7\n",
      "Epoch 00032: val_loss did not improve from 0.73518\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7338 - sparse_categorical_accuracy: 0.7694 - val_loss: 0.7408 - val_sparse_categorical_accuracy: 0.7593\n",
      "Epoch 33/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7345 - sparse_categorical_accuracy: 0.7658\n",
      "Epoch 00033: val_loss did not improve from 0.73518\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7347 - sparse_categorical_accuracy: 0.7658 - val_loss: 0.7385 - val_sparse_categorical_accuracy: 0.7603\n",
      "Epoch 34/100\n",
      "28128/28304 [============================>.] - ETA: 0s - loss: 0.7318 - sparse_categorical_accuracy: 0.7672\n",
      "Epoch 00034: val_loss improved from 0.73518 to 0.73516, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 78us/sample - loss: 0.7318 - sparse_categorical_accuracy: 0.7671 - val_loss: 0.7352 - val_sparse_categorical_accuracy: 0.7596\n",
      "Epoch 35/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7323 - sparse_categorical_accuracy: 0.7674\n",
      "Epoch 00035: val_loss improved from 0.73516 to 0.73311, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 3s 89us/sample - loss: 0.7324 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.7331 - val_sparse_categorical_accuracy: 0.7599\n",
      "Epoch 36/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7341 - sparse_categorical_accuracy: 0.7668\n",
      "Epoch 00036: val_loss did not improve from 0.73311\n",
      "28304/28304 [==============================] - 2s 88us/sample - loss: 0.7341 - sparse_categorical_accuracy: 0.7668 - val_loss: 0.7343 - val_sparse_categorical_accuracy: 0.7596\n",
      "Epoch 37/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7340 - sparse_categorical_accuracy: 0.7665\n",
      "Epoch 00037: val_loss did not improve from 0.73311\n",
      "28304/28304 [==============================] - 3s 89us/sample - loss: 0.7334 - sparse_categorical_accuracy: 0.7667 - val_loss: 0.7346 - val_sparse_categorical_accuracy: 0.7585\n",
      "Epoch 38/100\n",
      "27776/28304 [============================>.] - ETA: 0s - loss: 0.7352 - sparse_categorical_accuracy: 0.7669\n",
      "Epoch 00038: val_loss did not improve from 0.73311\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7345 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.7365 - val_sparse_categorical_accuracy: 0.7620\n",
      "Epoch 39/100\n",
      "27712/28304 [============================>.] - ETA: 0s - loss: 0.7324 - sparse_categorical_accuracy: 0.7689\n",
      "Epoch 00039: val_loss did not improve from 0.73311\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7324 - sparse_categorical_accuracy: 0.7688 - val_loss: 0.7354 - val_sparse_categorical_accuracy: 0.7622\n",
      "Epoch 40/100\n",
      "27776/28304 [============================>.] - ETA: 0s - loss: 0.7343 - sparse_categorical_accuracy: 0.7672\n",
      "Epoch 00040: val_loss did not improve from 0.73311\n",
      "28304/28304 [==============================] - 2s 83us/sample - loss: 0.7331 - sparse_categorical_accuracy: 0.7676 - val_loss: 0.7351 - val_sparse_categorical_accuracy: 0.7590\n",
      "Epoch 41/100\n",
      "27872/28304 [============================>.] - ETA: 0s - loss: 0.7347 - sparse_categorical_accuracy: 0.7683\n",
      "Epoch 00041: val_loss did not improve from 0.73311\n",
      "28304/28304 [==============================] - 3s 88us/sample - loss: 0.7351 - sparse_categorical_accuracy: 0.7681 - val_loss: 0.7362 - val_sparse_categorical_accuracy: 0.7609\n",
      "Epoch 42/100\n",
      "28032/28304 [============================>.] - ETA: 0s - loss: 0.7328 - sparse_categorical_accuracy: 0.7663- ETA: 1s - loss: 0.7448 - sparse_ca\n",
      "Epoch 00042: val_loss did not improve from 0.73311\n",
      "28304/28304 [==============================] - 3s 89us/sample - loss: 0.7317 - sparse_categorical_accuracy: 0.7667 - val_loss: 0.7356 - val_sparse_categorical_accuracy: 0.7593\n",
      "Epoch 43/100\n",
      "27904/28304 [============================>.] - ETA: 0s - loss: 0.7322 - sparse_categorical_accuracy: 0.7675\n",
      "Epoch 00043: val_loss did not improve from 0.73311\n",
      "28304/28304 [==============================] - 2s 86us/sample - loss: 0.7320 - sparse_categorical_accuracy: 0.7676 - val_loss: 0.7380 - val_sparse_categorical_accuracy: 0.7610\n",
      "Epoch 44/100\n",
      "27904/28304 [============================>.] - ETA: 0s - loss: 0.7334 - sparse_categorical_accuracy: 0.7661\n",
      "Epoch 00044: val_loss did not improve from 0.73311\n",
      "28304/28304 [==============================] - 3s 92us/sample - loss: 0.7325 - sparse_categorical_accuracy: 0.7667 - val_loss: 0.7353 - val_sparse_categorical_accuracy: 0.7593\n",
      "Epoch 45/100\n",
      "28032/28304 [============================>.] - ETA: 0s - loss: 0.7321 - sparse_categorical_accuracy: 0.7688\n",
      "Epoch 00045: val_loss did not improve from 0.73311\n",
      "28304/28304 [==============================] - 3s 90us/sample - loss: 0.7320 - sparse_categorical_accuracy: 0.7688 - val_loss: 0.7333 - val_sparse_categorical_accuracy: 0.7610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7320 - sparse_categorical_accuracy: 0.7676\n",
      "Epoch 00046: val_loss did not improve from 0.73311\n",
      "28304/28304 [==============================] - 3s 89us/sample - loss: 0.7321 - sparse_categorical_accuracy: 0.7676 - val_loss: 0.7345 - val_sparse_categorical_accuracy: 0.7610\n",
      "Epoch 47/100\n",
      "27680/28304 [============================>.] - ETA: 0s - loss: 0.7328 - sparse_categorical_accuracy: 0.7677- ETA: 0s - loss: 0.7265 - sparse_categoric\n",
      "Epoch 00047: val_loss improved from 0.73311 to 0.73217, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 84us/sample - loss: 0.7334 - sparse_categorical_accuracy: 0.7675 - val_loss: 0.7322 - val_sparse_categorical_accuracy: 0.7595\n",
      "Epoch 48/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.7314 - sparse_categorical_accuracy: 0.7696\n",
      "Epoch 00048: val_loss did not improve from 0.73217\n",
      "28304/28304 [==============================] - 2s 86us/sample - loss: 0.7316 - sparse_categorical_accuracy: 0.7695 - val_loss: 0.7355 - val_sparse_categorical_accuracy: 0.7602\n",
      "Epoch 49/100\n",
      "28096/28304 [============================>.] - ETA: 0s - loss: 0.7328 - sparse_categorical_accuracy: 0.7681- ETA: 0s - loss: 0.7318 - sparse_categorical_accuracy: 0.\n",
      "Epoch 00049: val_loss did not improve from 0.73217\n",
      "28304/28304 [==============================] - 3s 91us/sample - loss: 0.7328 - sparse_categorical_accuracy: 0.7679 - val_loss: 0.7329 - val_sparse_categorical_accuracy: 0.7598\n",
      "Epoch 50/100\n",
      "27904/28304 [============================>.] - ETA: 0s - loss: 0.7340 - sparse_categorical_accuracy: 0.7682\n",
      "Epoch 00050: val_loss did not improve from 0.73217\n",
      "28304/28304 [==============================] - 3s 92us/sample - loss: 0.7340 - sparse_categorical_accuracy: 0.7684 - val_loss: 0.7378 - val_sparse_categorical_accuracy: 0.7595\n",
      "Epoch 51/100\n",
      "28256/28304 [============================>.] - ETA: 0s - loss: 0.7328 - sparse_categorical_accuracy: 0.7665\n",
      "Epoch 00051: val_loss did not improve from 0.73217\n",
      "28304/28304 [==============================] - 2s 82us/sample - loss: 0.7331 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.7336 - val_sparse_categorical_accuracy: 0.7592\n",
      "Epoch 52/100\n",
      "27744/28304 [============================>.] - ETA: 0s - loss: 0.7304 - sparse_categorical_accuracy: 0.7683\n",
      "Epoch 00052: val_loss did not improve from 0.73217\n",
      "28304/28304 [==============================] - 2s 81us/sample - loss: 0.7308 - sparse_categorical_accuracy: 0.7683 - val_loss: 0.7329 - val_sparse_categorical_accuracy: 0.7590\n",
      "Epoch 53/100\n",
      "27648/28304 [============================>.] - ETA: 0s - loss: 0.7331 - sparse_categorical_accuracy: 0.7674\n",
      "Epoch 00053: val_loss did not improve from 0.73217\n",
      "28304/28304 [==============================] - 3s 89us/sample - loss: 0.7335 - sparse_categorical_accuracy: 0.7675 - val_loss: 0.7332 - val_sparse_categorical_accuracy: 0.7582\n",
      "Epoch 54/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7330 - sparse_categorical_accuracy: 0.7676\n",
      "Epoch 00054: val_loss did not improve from 0.73217\n",
      "28304/28304 [==============================] - 2s 87us/sample - loss: 0.7326 - sparse_categorical_accuracy: 0.7676 - val_loss: 0.7322 - val_sparse_categorical_accuracy: 0.7589\n",
      "Epoch 55/100\n",
      "27584/28304 [============================>.] - ETA: 0s - loss: 0.7321 - sparse_categorical_accuracy: 0.7691\n",
      "Epoch 00055: val_loss did not improve from 0.73217\n",
      "28304/28304 [==============================] - 2s 80us/sample - loss: 0.7317 - sparse_categorical_accuracy: 0.7690 - val_loss: 0.7342 - val_sparse_categorical_accuracy: 0.7605\n",
      "Epoch 56/100\n",
      "27744/28304 [============================>.] - ETA: 0s - loss: 0.7330 - sparse_categorical_accuracy: 0.7673\n",
      "Epoch 00056: val_loss did not improve from 0.73217\n",
      "28304/28304 [==============================] - 2s 83us/sample - loss: 0.7338 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.7352 - val_sparse_categorical_accuracy: 0.7589\n",
      "Epoch 57/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7303 - sparse_categorical_accuracy: 0.7685\n",
      "Epoch 00057: val_loss did not improve from 0.73217\n",
      "28304/28304 [==============================] - 2s 88us/sample - loss: 0.7309 - sparse_categorical_accuracy: 0.7685 - val_loss: 0.7325 - val_sparse_categorical_accuracy: 0.7592\n",
      "Epoch 58/100\n",
      "27680/28304 [============================>.] - ETA: 0s - loss: 0.7309 - sparse_categorical_accuracy: 0.7679\n",
      "Epoch 00058: val_loss did not improve from 0.73217\n",
      "28304/28304 [==============================] - 2s 79us/sample - loss: 0.7314 - sparse_categorical_accuracy: 0.7680 - val_loss: 0.7324 - val_sparse_categorical_accuracy: 0.7603\n",
      "Epoch 59/100\n",
      "27616/28304 [============================>.] - ETA: 0s - loss: 0.7313 - sparse_categorical_accuracy: 0.7684\n",
      "Epoch 00059: val_loss improved from 0.73217 to 0.73073, saving model to nn_model.w8\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7321 - sparse_categorical_accuracy: 0.7683 - val_loss: 0.7307 - val_sparse_categorical_accuracy: 0.7596\n",
      "Epoch 60/100\n",
      "28128/28304 [============================>.] - ETA: 0s - loss: 0.7319 - sparse_categorical_accuracy: 0.7687\n",
      "Epoch 00060: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7321 - sparse_categorical_accuracy: 0.7686 - val_loss: 0.7321 - val_sparse_categorical_accuracy: 0.7598\n",
      "Epoch 61/100\n",
      "27872/28304 [============================>.] - ETA: 0s - loss: 0.7316 - sparse_categorical_accuracy: 0.7690\n",
      "Epoch 00061: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7318 - sparse_categorical_accuracy: 0.7690 - val_loss: 0.7337 - val_sparse_categorical_accuracy: 0.7607\n",
      "Epoch 62/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7307 - sparse_categorical_accuracy: 0.7692\n",
      "Epoch 00062: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7301 - sparse_categorical_accuracy: 0.7693 - val_loss: 0.7339 - val_sparse_categorical_accuracy: 0.7617\n",
      "Epoch 63/100\n",
      "27808/28304 [============================>.] - ETA: 0s - loss: 0.7324 - sparse_categorical_accuracy: 0.7683\n",
      "Epoch 00063: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7322 - sparse_categorical_accuracy: 0.7684 - val_loss: 0.7323 - val_sparse_categorical_accuracy: 0.7605\n",
      "Epoch 64/100\n",
      "28096/28304 [============================>.] - ETA: 0s - loss: 0.7310 - sparse_categorical_accuracy: 0.7674\n",
      "Epoch 00064: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7301 - sparse_categorical_accuracy: 0.7676 - val_loss: 0.7342 - val_sparse_categorical_accuracy: 0.7612\n",
      "Epoch 65/100\n",
      "27872/28304 [============================>.] - ETA: 0s - loss: 0.7273 - sparse_categorical_accuracy: 0.7698\n",
      "Epoch 00065: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7278 - sparse_categorical_accuracy: 0.7696 - val_loss: 0.7333 - val_sparse_categorical_accuracy: 0.7596\n",
      "Epoch 66/100\n",
      "28224/28304 [============================>.] - ETA: 0s - loss: 0.7297 - sparse_categorical_accuracy: 0.7684\n",
      "Epoch 00066: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7294 - sparse_categorical_accuracy: 0.7685 - val_loss: 0.7361 - val_sparse_categorical_accuracy: 0.7602\n",
      "Epoch 67/100\n",
      "27776/28304 [============================>.] - ETA: 0s - loss: 0.7295 - sparse_categorical_accuracy: 0.7687\n",
      "Epoch 00067: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7287 - sparse_categorical_accuracy: 0.7690 - val_loss: 0.7324 - val_sparse_categorical_accuracy: 0.7607\n",
      "Epoch 68/100\n",
      "27776/28304 [============================>.] - ETA: 0s - loss: 0.7334 - sparse_categorical_accuracy: 0.7670\n",
      "Epoch 00068: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7325 - sparse_categorical_accuracy: 0.7676 - val_loss: 0.7333 - val_sparse_categorical_accuracy: 0.7610\n",
      "Epoch 69/100\n",
      "27840/28304 [============================>.] - ETA: 0s - loss: 0.7277 - sparse_categorical_accuracy: 0.7702\n",
      "Epoch 00069: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 77us/sample - loss: 0.7275 - sparse_categorical_accuracy: 0.7698 - val_loss: 0.7337 - val_sparse_categorical_accuracy: 0.7595\n",
      "Epoch 70/100\n",
      "27776/28304 [============================>.] - ETA: 0s - loss: 0.7295 - sparse_categorical_accuracy: 0.7687\n",
      "Epoch 00070: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7313 - sparse_categorical_accuracy: 0.7680 - val_loss: 0.7320 - val_sparse_categorical_accuracy: 0.7599\n",
      "Epoch 71/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7289 - sparse_categorical_accuracy: 0.7698\n",
      "Epoch 00071: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7289 - sparse_categorical_accuracy: 0.7698 - val_loss: 0.7350 - val_sparse_categorical_accuracy: 0.7609\n",
      "Epoch 72/100\n",
      "27744/28304 [============================>.] - ETA: 0s - loss: 0.7300 - sparse_categorical_accuracy: 0.7698\n",
      "Epoch 00072: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7288 - sparse_categorical_accuracy: 0.7701 - val_loss: 0.7358 - val_sparse_categorical_accuracy: 0.7599\n",
      "Epoch 73/100\n",
      "27584/28304 [============================>.] - ETA: 0s - loss: 0.7323 - sparse_categorical_accuracy: 0.7685\n",
      "Epoch 00073: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 74us/sample - loss: 0.7310 - sparse_categorical_accuracy: 0.7689 - val_loss: 0.7309 - val_sparse_categorical_accuracy: 0.7605\n",
      "Epoch 74/100\n",
      "27968/28304 [============================>.] - ETA: 0s - loss: 0.7308 - sparse_categorical_accuracy: 0.7687- ETA: 1s - loss: 0.7324 - sparse_cate\n",
      "Epoch 00074: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 73us/sample - loss: 0.7306 - sparse_categorical_accuracy: 0.7688 - val_loss: 0.7338 - val_sparse_categorical_accuracy: 0.7610\n",
      "Epoch 75/100\n",
      "28192/28304 [============================>.] - ETA: 0s - loss: 0.7288 - sparse_categorical_accuracy: 0.7694- ETA: 0s - loss: 0.7333 - sparse_categorical_accuracy: 0\n",
      "Epoch 00075: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7281 - sparse_categorical_accuracy: 0.7698 - val_loss: 0.7353 - val_sparse_categorical_accuracy: 0.7617\n",
      "Epoch 76/100\n",
      "28288/28304 [============================>.] - ETA: 0s - loss: 0.7286 - sparse_categorical_accuracy: 0.7687\n",
      "Epoch 00076: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7289 - sparse_categorical_accuracy: 0.7686 - val_loss: 0.7321 - val_sparse_categorical_accuracy: 0.7612\n",
      "Epoch 77/100\n",
      "27936/28304 [============================>.] - ETA: 0s - loss: 0.7271 - sparse_categorical_accuracy: 0.7697\n",
      "Epoch 00077: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 76us/sample - loss: 0.7286 - sparse_categorical_accuracy: 0.7691 - val_loss: 0.7317 - val_sparse_categorical_accuracy: 0.7616\n",
      "Epoch 78/100\n",
      "27744/28304 [============================>.] - ETA: 0s - loss: 0.7291 - sparse_categorical_accuracy: 0.7692\n",
      "Epoch 00078: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7302 - sparse_categorical_accuracy: 0.7689 - val_loss: 0.7309 - val_sparse_categorical_accuracy: 0.7596\n",
      "Epoch 79/100\n",
      "28160/28304 [============================>.] - ETA: 0s - loss: 0.7300 - sparse_categorical_accuracy: 0.7690\n",
      "Epoch 00079: val_loss did not improve from 0.73073\n",
      "28304/28304 [==============================] - 2s 75us/sample - loss: 0.7299 - sparse_categorical_accuracy: 0.7690 - val_loss: 0.7359 - val_sparse_categorical_accuracy: 0.7617\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Partial score of fold 4 is: 0.6646466832593876\n",
      "Our oof cohen kappa score is:  0.6861723009814613\n"
     ]
    }
   ],
   "source": [
    "nn_model = Nn_Class_Model(train_pred_df, test_pred_df, new_features, {}, reduce_train, ajusted_test, \n",
    "                          categoricals=categoricals, verbose=True, is_classifier=True, is_lgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data:  0.6861723009814613\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy on training data: ', nn_model.score)\n",
    "# Accuracy on training data:   0.6088463108528276\n",
    "# Accuracy on training data:  0.7190729225551159 With SMOTE\n",
    "# Accuracy on training data:  0.6884159873437191 - with SMOTE Classifier all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.77      0.78      8845\n",
      "           1       0.81      0.75      0.78      8845\n",
      "           2       0.83      0.74      0.78      8845\n",
      "           3       0.68      0.83      0.75      8845\n",
      "\n",
      "    accuracy                           0.77     35380\n",
      "   macro avg       0.78      0.77      0.77     35380\n",
      "weighted avg       0.78      0.77      0.77     35380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(reduce_train['accuracy_group'], np.around(nn_model.oof_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    0.592\n",
       "0.0    0.225\n",
       "1.0    0.107\n",
       "2.0    0.076\n",
       "Name: accuracy_group, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['installation_id'] = ajusted_test['installation_id']\n",
    "submission['accuracy_group'] =  np.around(nn_model.y_pred)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission['accuracy_group'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T12:57:13.236097Z",
     "start_time": "2020-01-17T12:57:12.112045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch cjs\n",
      "Your branch is up to date with 'new_origin/cjs'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
      "\n",
      "\tmodified:   Big Three.ipynb\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\n",
      "\t.idea/\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
