{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/data-science-bowl-2019/train.csv\n",
      "/kaggle/input/data-science-bowl-2019/specs.csv\n",
      "/kaggle/input/data-science-bowl-2019/sample_submission.csv\n",
      "/kaggle/input/data-science-bowl-2019/train_labels.csv\n",
      "/kaggle/input/data-science-bowl-2019/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from catboost import CatBoostRegressor\n",
    "from matplotlib import pyplot\n",
    "import shap\n",
    "import random\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from pprint import pprint\n",
    "from bayes_opt import BayesianOptimization\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "import gc\n",
    "import json\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_qwk_lgb_regr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for lgb.\n",
    "    \"\"\"\n",
    "    dist = Counter(reduce_train['accuracy_group'])\n",
    "    for k in dist:\n",
    "        dist[k] /= len(reduce_train)\n",
    "    reduce_train['accuracy_group'].hist()\n",
    "    \n",
    "    acum = 0\n",
    "    bound = {}\n",
    "    for i in range(3):\n",
    "        acum += dist[i]\n",
    "        bound[i] = np.percentile(y_pred, acum * 100)\n",
    "\n",
    "    def classify(x):\n",
    "        if x <= bound[0]:\n",
    "            return 0\n",
    "        elif x <= bound[1]:\n",
    "            return 1\n",
    "        elif x <= bound[2]:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    y_pred = np.array(list(map(classify, y_pred))).reshape(y_true.shape)\n",
    "\n",
    "    return 'cappa', cohen_kappa_score(y_true, y_pred, weights='quadratic'), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwk3(a1, a2, max_rat=3):\n",
    "    assert(len(a1) == len(a2))\n",
    "    a1 = np.asarray(a1, dtype=int)\n",
    "    a2 = np.asarray(a2, dtype=int)\n",
    "\n",
    "    hist1 = np.zeros((max_rat + 1, ))\n",
    "    hist2 = np.zeros((max_rat + 1, ))\n",
    "\n",
    "    o = 0\n",
    "    for k in range(a1.shape[0]):\n",
    "        i, j = a1[k], a2[k]\n",
    "        hist1[i] += 1\n",
    "        hist2[j] += 1\n",
    "        o +=  (i - j) * (i - j)\n",
    "\n",
    "    e = 0\n",
    "    for i in range(max_rat + 1):\n",
    "        for j in range(max_rat + 1):\n",
    "            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n",
    "\n",
    "    e = e / a1.shape[0]\n",
    "\n",
    "    return 1 - o / e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    print('Reading train.csv file....')\n",
    "    train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n",
    "    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n",
    "\n",
    "    print('Reading test.csv file....')\n",
    "    test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n",
    "    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n",
    "\n",
    "    print('Reading train_labels.csv file....')\n",
    "    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n",
    "    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n",
    "\n",
    "    return train, test, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_title(train, test, train_labels):\n",
    "    # encode title\n",
    "    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n",
    "    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n",
    "    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n",
    "    # make a list with all the unique 'titles' from the train and test set\n",
    "    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n",
    "    # make a list with all the unique 'event_code' from the train and test set\n",
    "    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n",
    "    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n",
    "    # make a list with all the unique worlds from the train and test set\n",
    "    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n",
    "    # create a dictionary numerating the titles\n",
    "    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
    "    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
    "    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n",
    "    # replace the text titles with the number titles from the dict\n",
    "    train['title'] = train['title'].map(activities_map)\n",
    "    test['title'] = test['title'].map(activities_map)\n",
    "    train['world'] = train['world'].map(activities_world)\n",
    "    test['world'] = test['world'].map(activities_world)\n",
    "    train_labels['title'] = train_labels['title'].map(activities_map)\n",
    "    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n",
    "    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n",
    "    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n",
    "    # convert text into datetime\n",
    "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "    \n",
    "    \n",
    "    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test(train, test):\n",
    "    \n",
    "    compiled_train = []\n",
    "    compiled_test = []\n",
    "    assessment_sessions_by_instid = {}\n",
    "    \n",
    "    # Loop through each train installation id\n",
    "    for ins_id, user_sample in tqdm(train.groupby('installation_id', sort = False), \n",
    "                                   total = train['installation_id'].nunique()):\n",
    "        compiled_train += get_data(user_sample, test_set = False)  \n",
    "        \n",
    "    reduce_train = pd.DataFrame(compiled_train)\n",
    "    del compiled_train\n",
    "    \n",
    "    # Loop through each test installation id\n",
    "    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), \n",
    "                                   total = test['installation_id'].nunique()):\n",
    "        test_data = get_data(user_sample, test_set = True)\n",
    "        compiled_test.append(test_data)   \n",
    "        \n",
    "    reduce_test = pd.DataFrame(compiled_test)\n",
    "    del compiled_test\n",
    "    \n",
    "    categoricals = ['session_title']\n",
    "    \n",
    "    return reduce_train, reduce_test, categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(user_sample, test_set=False):\n",
    "    '''\n",
    "    The user_sample is a DataFrame from train or test where the only one \n",
    "    installation_id is filtered\n",
    "    And the test_set parameter is related with the labels processing, that is only requered\n",
    "    if test_set=False\n",
    "    '''\n",
    "    # Constants and parameters declaration\n",
    "    last_activity = 0\n",
    "    \n",
    "    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "    \n",
    "    # new features: time spent in each activity\n",
    "    last_session_time_sec = 0\n",
    "    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n",
    "    all_assessments = []\n",
    "    accumulated_accuracy_group = 0\n",
    "    accumulated_accuracy = 0\n",
    "    accumulated_correct_attempts = 0 \n",
    "    accumulated_uncorrect_attempts = 0\n",
    "    accumulated_actions = 0\n",
    "    counter = 0\n",
    "    time_first_activity = float(user_sample['timestamp'].values[0])\n",
    "    durations = []\n",
    "    durations_game = []\n",
    "    durations_activity = []\n",
    "    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n",
    "    last_game_time_title = {'lgt_' + title: 0 for title in assess_titles}\n",
    "    ac_game_time_title = {'agt_' + title: 0 for title in assess_titles}\n",
    "    ac_true_attempts_title = {'ata_' + title: 0 for title in assess_titles}\n",
    "    ac_false_attempts_title = {'afa_' + title: 0 for title in assess_titles}\n",
    "    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n",
    "    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n",
    "    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n",
    "    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n",
    "    session_count = 0\n",
    "    \n",
    "    # itarates through each session of one instalation_id\n",
    "    for i, session in user_sample.groupby('game_session', sort=False):\n",
    "        # i = game_session_id\n",
    "        # session is a DataFrame that contain only one game_session\n",
    "        \n",
    "        # get some sessions information\n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_title_text = activities_labels[session_title]\n",
    "        game_session = session['game_session'].iloc[0]          \n",
    "            \n",
    "        # for each assessment, and only this kind off session, the features below are processed\n",
    "        # and a register are generated\n",
    "        if (session_type == 'Assessment') & (test_set or len(session)>1):\n",
    "            # search for event_code 4100, that represents the assessments trial\n",
    "            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n",
    "            # then, check the numbers of wins and the number of losses\n",
    "            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n",
    "            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n",
    "            # copy a dict to use as feature template, it's initialized with some itens: \n",
    "            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "            features = user_activities_count.copy()\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            features.update(event_code_count.copy())\n",
    "            features.update(title_count.copy())\n",
    "            features.update(event_id_count.copy())\n",
    "            features.update(title_event_code_count.copy())\n",
    "            features.update(last_game_time_title.copy())\n",
    "            features.update(ac_game_time_title.copy())\n",
    "            features.update(ac_true_attempts_title.copy())\n",
    "            features.update(ac_false_attempts_title.copy())\n",
    "            features['installation_session_count'] = session_count\n",
    "            \n",
    "            variety_features = [('var_event_code', event_code_count), \n",
    "                                ('var_event_id', event_id_count), \n",
    "                                ('var_title', title_count), \n",
    "                                ('var_title_event_code', title_event_code_count)]\n",
    "            \n",
    "            for name, dict_counts in variety_features:\n",
    "                arr = np.array(list(dict_counts.values()))\n",
    "                features[name] = np.count_nonzero(arr)\n",
    "                \n",
    "            # get installation_id for aggregated features\n",
    "            features['installation_id'] = session['installation_id'].iloc[-1]\n",
    "            features['game_session'] = game_session\n",
    "            # add title as feature, remembering that title represents the name of the game\n",
    "            features['session_title'] = session['title'].iloc[0]\n",
    "            # the 4 lines below add the feature of the history of the trials of this player\n",
    "            # this is based on the all time attempts so far, at the moment of this assessment\n",
    "            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
    "            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n",
    "            accumulated_correct_attempts += true_attempts \n",
    "            accumulated_uncorrect_attempts += false_attempts\n",
    "            \n",
    "            # ----------------------------------------------\n",
    "            ac_true_attempts_title['ata_' + session_title_text] += true_attempts\n",
    "            ac_false_attempts_title['afa_' + session_title_text] += false_attempts\n",
    "            \n",
    "            \n",
    "            last_game_time_title['lgt_' + session_title_text] = session['game_time'].iloc[-1]\n",
    "            ac_game_time_title['agt_' + session_title_text] += session['game_time'].iloc[-1]\n",
    "            # ----------------------------------------------\n",
    "            \n",
    "            # the time spent in the app so far\n",
    "            if durations == []:\n",
    "                features['duration_mean'] = 0\n",
    "                features['duration_std'] = 0\n",
    "                features['last_duration'] = 0\n",
    "                features['duration_max'] = 0\n",
    "            else:\n",
    "                features['duration_mean'] = np.mean(durations)\n",
    "                features['duration_std'] = np.std(durations)\n",
    "                features['last_duration'] = durations[-1]\n",
    "                features['duration_max'] = np.max(durations)\n",
    "            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            \n",
    "            if durations_game == []:\n",
    "                features['duration_game_mean'] = 0\n",
    "                features['duration_game_std'] = 0\n",
    "                features['game_last_duration'] = 0\n",
    "                features['game_max_duration'] = 0\n",
    "            else:\n",
    "                features['duration_game_mean'] = np.mean(durations_game)\n",
    "                features['duration_game_std'] = np.std(durations_game)\n",
    "                features['game_last_duration'] = durations_game[-1]\n",
    "                features['game_max_duration'] = np.max(durations_game)\n",
    "                \n",
    "            if durations_activity == []:\n",
    "                features['duration_activity_mean'] = 0\n",
    "                features['duration_activity_std'] = 0\n",
    "                features['game_activity_duration'] = 0\n",
    "                features['game_activity_max'] = 0\n",
    "            else:\n",
    "                features['duration_activity_mean'] = np.mean(durations_activity)\n",
    "                features['duration_activity_std'] = np.std(durations_activity)\n",
    "                features['game_activity_duration'] = durations_activity[-1]\n",
    "                features['game_activity_max'] = np.max(durations_activity)\n",
    "            \n",
    "            # the accuracy is the all time wins divided by the all time attempts\n",
    "            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n",
    "            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n",
    "            accumulated_accuracy += accuracy\n",
    "            last_accuracy_title['acc_' + session_title_text] = accuracy\n",
    "            # a feature of the current accuracy categorized\n",
    "            # it is a counter of how many times this player was in each accuracy group\n",
    "            if accuracy == 0:\n",
    "                features['accuracy_group'] = 0\n",
    "            elif accuracy == 1:\n",
    "                features['accuracy_group'] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features['accuracy_group'] = 2\n",
    "            else:\n",
    "                features['accuracy_group'] = 1\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[features['accuracy_group']] += 1\n",
    "            # mean of the all accuracy groups of this player\n",
    "            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n",
    "            accumulated_accuracy_group += features['accuracy_group']\n",
    "            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n",
    "            features['accumulated_actions'] = accumulated_actions\n",
    "            \n",
    "            # there are some conditions to allow this features to be inserted in the datasets\n",
    "            # if it's a test set, all sessions belong to the final dataset\n",
    "            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n",
    "            # that means, must exist an event_code 4100 or 4110\n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "            elif true_attempts+false_attempts > 0:\n",
    "                all_assessments.append(features)\n",
    "                \n",
    "            counter += 1\n",
    "            \n",
    "        if session_type == 'Game':\n",
    "            durations_game.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            \n",
    "        if session_type == 'Activity':\n",
    "            durations_activity.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "                \n",
    "        \n",
    "        session_count += 1\n",
    "        # this piece counts how many actions was made in each event_code so far\n",
    "        def update_counters(counter: dict, col: str):\n",
    "                num_of_session_count = Counter(session[col])\n",
    "                for k in num_of_session_count.keys():\n",
    "                    x = k\n",
    "                    if col == 'title':\n",
    "                        x = activities_labels[k]\n",
    "                    counter[x] += num_of_session_count[k]\n",
    "                return counter\n",
    "            \n",
    "        event_code_count = update_counters(event_code_count, \"event_code\")\n",
    "        event_id_count = update_counters(event_id_count, \"event_id\")\n",
    "        title_count = update_counters(title_count, 'title')\n",
    "        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n",
    "\n",
    "        # counts how many actions the player has done so far, used in the feature of the same name\n",
    "        accumulated_actions += len(session)\n",
    "        if last_activity != session_type:\n",
    "            user_activities_count[session_type] += 1\n",
    "            last_activitiy = session_type \n",
    "                        \n",
    "    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n",
    "    if test_set:\n",
    "        return all_assessments[-1]\n",
    "    # in the train_set, all assessments goes to the dataset\n",
    "    return all_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dead_weight(df, train_labels, test_set=False):\n",
    "    #df = df[df['world'] != 'NONE']\n",
    "\n",
    "    # filtering by ids that took assessments\n",
    "    ids_w_assessments = df[df['type'] == 'Assessment']['installation_id'].drop_duplicates()\n",
    "    df = df[df['installation_id'].isin(ids_w_assessments)]\n",
    "    \n",
    "    #If training set then make sure the installation ids are in the labels and remove assements not in the labels\n",
    "    if test_set == False:\n",
    "        # drop data whose installation does not contain any scored assessments in train_labels\n",
    "        df = df[df['installation_id'].isin(train_labels['installation_id'].unique())]\n",
    "\n",
    "        assessments = df[df.type == 'Assessment']\n",
    "        assessments = assessments[~assessments.game_session.isin(train_labels.game_session)]\n",
    "        df = df[~df.game_session.isin(assessments.game_session)]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stract_hists(feature, train, test, adjust=False, plot=False):\n",
    "    n_bins = 10\n",
    "    train_data = train[feature]\n",
    "    test_data = test[feature]\n",
    "    if adjust:\n",
    "        test_data *= train_data.mean() / test_data.mean()\n",
    "    perc_90 = np.percentile(train_data, 95)\n",
    "    train_data = np.clip(train_data, 0, perc_90)\n",
    "    test_data = np.clip(test_data, 0, perc_90)\n",
    "    train_hist = np.histogram(train_data, bins=n_bins)[0] / len(train_data)\n",
    "    test_hist = np.histogram(test_data, bins=n_bins)[0] / len(test_data)\n",
    "    msre = mean_squared_error(train_hist, test_hist)\n",
    "    if plot:\n",
    "        print(msre)\n",
    "        plt.bar(range(n_bins), train_hist, color='blue', alpha=0.5)\n",
    "        plt.bar(range(n_bins), test_hist, color='red', alpha=0.5)\n",
    "        plt.show()\n",
    "    return msre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction\n",
    "def get_class_pred(pred, train_t):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for lgb.\n",
    "    \"\"\"\n",
    "    dist = Counter(train_t['accuracy_group'])\n",
    "    for k in dist:\n",
    "        dist[k] /= len(train_t)\n",
    "    \n",
    "    acum = 0\n",
    "    bound = {}\n",
    "    for i in range(3):\n",
    "        acum += dist[i]\n",
    "        bound[i] = np.percentile(pred, acum * 100)\n",
    "\n",
    "    def classify(x):\n",
    "        if x <= bound[0]:\n",
    "            return 0\n",
    "        elif x <= bound[1]:\n",
    "            return 1\n",
    "        elif x <= bound[2]:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    y_pred = np.array(list(map(classify, pred)))\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(reduce_train, reduce_test):\n",
    "    for df in [reduce_train, reduce_test]:\n",
    "        df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count')\n",
    "        df['installation_duration_mean'] = df.groupby(['installation_id'])['duration_mean'].transform('mean')\n",
    "        #df['installation_duration_std'] = df.groupby(['installation_id'])['duration_mean'].transform('std')\n",
    "        df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n",
    "        \n",
    "        df['sum_event_code_count'] = df[[2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, \n",
    "                                        4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080, 2035, \n",
    "                                        2040, 4090, 4220, 4095]].sum(axis = 1)\n",
    "        \n",
    "        df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n",
    "        #df['installation_event_code_count_std'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('std')\n",
    "        \n",
    "    features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n",
    "    features = [x for x in features if x not in cols_to_drop]\n",
    "    \n",
    "    return reduce_train, reduce_test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Model(object):\n",
    "    \n",
    "    def __init__(self, train_df, test_df, features, params, categoricals=[], n_splits=5, verbose=True):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.features = features\n",
    "        self.n_splits = n_splits\n",
    "        self.categoricals = categoricals\n",
    "        self.target = 'accuracy_group'\n",
    "        self.cv = self.get_cv()\n",
    "        self.verbose = verbose\n",
    "        self.params = params\n",
    "        self.y_pred, self.score, self.model, self.oof_pred = self.fit()\n",
    "        \n",
    "    def train_model(self, train_set, val_set):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_cv(self):\n",
    "        cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
    "        return cv.split(self.train_df, self.train_df[self.target])\n",
    "    \n",
    "    def get_params(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def convert_x(self, x):\n",
    "        return x\n",
    "        \n",
    "    def fit(self):\n",
    "        oof_pred = np.zeros((len(reduce_train), ))\n",
    "        y_pred = np.zeros((len(reduce_test), ))\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv):\n",
    "            try:\n",
    "                x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n",
    "                y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n",
    "                train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n",
    "                model = self.train_model(train_set, val_set)\n",
    "                conv_x_val = self.convert_x(x_val)\n",
    "                tmp_pred = model.predict(conv_x_val)\n",
    "                oof_pred[val_idx] = tmp_pred.reshape(oof_pred[val_idx].shape)\n",
    "                x_test = self.convert_x(self.test_df[self.features])\n",
    "                y_pred += model.predict(x_test).reshape(y_pred.shape) / self.n_splits\n",
    "                print('Partial score of fold {} is: {}'.format(fold, qwk3(y_val, tmp_pred)))\n",
    "                loss_score = qwk3(self.train_df[self.target], oof_pred)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('Error training: val_idx = ', val_idx)\n",
    "        if self.verbose:\n",
    "            print('Our oof cohen kappa score is: ', loss_score)\n",
    "        del self.train_df, self.test_df, self.cv\n",
    "        gc.collect()\n",
    "        return y_pred, loss_score, model, oof_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xgb_Model(Base_Model):\n",
    "          \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        return xgb.train(self.params, train_set, \n",
    "                         num_boost_round=667, evals=[(train_set, 'train'), (val_set, 'val')], \n",
    "                         verbose_eval=verbosity, early_stopping_rounds=100)\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = xgb.DMatrix(x_train, y_train)\n",
    "        val_set = xgb.DMatrix(x_val, y_val)\n",
    "        return train_set, val_set\n",
    "    \n",
    "    def convert_x(self, x):\n",
    "        return xgb.DMatrix(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catb_Model(Base_Model):\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        clf = CatBoostRegressor(**self.params)\n",
    "        clf.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                eval_set=(val_set['X'], val_set['y']),\n",
    "                verbose=verbosity,)\n",
    "        return clf\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RF_Model(Base_Model):\n",
    "        \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        rf = RandomForestClassifier(**self.params)\n",
    "        rf.fit(train_set['X'], train_set['y'])\n",
    "        return rf\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Model(Base_Model):\n",
    "        \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        knn = KNeighborsClassifier(**self.params)\n",
    "        knn.fit(train_set['X'], train_set['y'])\n",
    "        return knn\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lgb_Model(Base_Model):\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        return lgb.train(self.params, train_set, valid_sets=[train_set, val_set], verbose_eval=verbosity)\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = lgb.Dataset(x_train, y_train)\n",
    "        val_set = lgb.Dataset(x_val, y_val)\n",
    "        return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "class Cnn_Model(Base_Model):\n",
    "    \n",
    "    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n",
    "        self.create_feat_2d(features)\n",
    "        super().__init__(train_df, test_df, features, {}, categoricals, n_splits, verbose)\n",
    "        \n",
    "    def create_feat_2d(self, features, n_feats_repeat=50):\n",
    "        self.n_feats = len(features)\n",
    "        self.n_feats_repeat = n_feats_repeat\n",
    "        self.mask = np.zeros((self.n_feats_repeat, self.n_feats), dtype=np.int32)\n",
    "        for i in range(self.n_feats_repeat):\n",
    "            l = list(range(self.n_feats))\n",
    "            for j in range(self.n_feats):\n",
    "                c = l.pop(choice(range(len(l))))\n",
    "                self.mask[i, j] = c\n",
    "        self.mask = tf.convert_to_tensor(self.mask)\n",
    "        print(self.mask.shape)\n",
    "       \n",
    "        \n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "\n",
    "        inp = tf.keras.layers.Input(shape=(self.n_feats))\n",
    "        x = tf.keras.layers.Lambda(lambda x: tf.gather(x, self.mask, axis=1))(inp)\n",
    "        x = tf.keras.layers.Reshape((self.n_feats_repeat, self.n_feats, 1))(x)\n",
    "        x = tf.keras.layers.Conv2D(18, (50, 50), strides=50, activation='relu')(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        #x = tf.keras.layers.Dense(200, activation='relu')(x)\n",
    "        #x = tf.keras.layers.LayerNormalization()(x)\n",
    "        #x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "        x = tf.keras.layers.LayerNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(50, activation='relu')(x)\n",
    "        x = tf.keras.layers.LayerNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        out = tf.keras.layers.Dense(1)(x)\n",
    "        \n",
    "        model = tf.keras.Model(inp, out)\n",
    "    \n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mse')\n",
    "        print(model.summary())\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "        model.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                validation_data=(val_set['X'], val_set['y']),\n",
    "                epochs=100,\n",
    "                 callbacks=[save_best, early_stop])\n",
    "        model.load_weights('nn_model.w8')\n",
    "        return model\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nn_Model(Base_Model):\n",
    "\n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(train_set['X'].shape[1],)),\n",
    "            tf.keras.layers.Dense(200, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(100, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(50, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(25, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1, activation='relu')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-4), loss='mse')\n",
    "        print(model.summary())\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "        model.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                validation_data=(val_set['X'], val_set['y']),\n",
    "                epochs=100,\n",
    "                 callbacks=[save_best, early_stop])\n",
    "        model.load_weights('nn_model.w8')\n",
    "        return model\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train.csv file....\n",
      "Training.csv file have 11341042 rows and 11 columns\n",
      "Reading test.csv file....\n",
      "Test.csv file have 1156414 rows and 11 columns\n",
      "Reading train_labels.csv file....\n",
      "Train_labels.csv file have 17690 rows and 7 columns\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "train, test, train_labels = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unwanted data\n",
    "train = remove_dead_weight(train, train_labels, test_set=False)\n",
    "test = remove_dead_weight(test, train_labels, test_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get usefull dict with maping encode\n",
    "train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(train, test, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd2398a2d5c49c89919c9a8b5fe0312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3614), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4267f19b9e4af58a5ed93534a67921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tranform function to get the train and test set\n",
    "reduce_train, reduce_test, categoricals = get_train_and_test(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2264"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete train and test to free up resources\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the accuracy group vals\n",
    "reduce_train = reduce_train.set_index('game_session')\n",
    "train_labels = train_labels.set_index('game_session')\n",
    "reduce_train.update(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "reduce_train = reduce_train.reset_index()\n",
    "train_labels = train_labels.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cols to drop for training\n",
    "cols_to_drop = ['accuracy_group', 'game_session', 'installation_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_train, reduce_test, features = preprocess(reduce_train, reduce_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Catagoricals\n",
    "def create_cats(train, test, categoricals):\n",
    "    tmp_train = train.copy()\n",
    "    tmp_test = test.copy()\n",
    "    tmp_train['session_title'] = tmp_train['session_title'].astype(CategoricalDtype(categories=activities_labels))\n",
    "    tmp_test['session_title'] = tmp_test['session_title'].astype(CategoricalDtype(categories=activities_labels))\n",
    "    #tmp_train['world'] = tmp_train['world'].astype(CategoricalDtype(categories=[0, 1, 2]))\n",
    "    #tmp_test['world'] = tmp_test['world'].astype(CategoricalDtype(categories=[0, 1, 2]))\n",
    "    train_cats = pd.get_dummies(tmp_train[categoricals], prefix=categoricals)\n",
    "    test_cats = pd.get_dummies(tmp_test[categoricals], prefix=categoricals)\n",
    "    \n",
    "    tmp_train = tmp_train.drop(categoricals, axis=1)\n",
    "    tmp_test = tmp_test.drop(categoricals, axis=1)\n",
    "    \n",
    "    tmp_train = pd.concat([tmp_train, train_cats], axis=1, sort=False)\n",
    "    tmp_test = pd.concat([tmp_test, test_cats], axis=1, sort=False)\n",
    "    \n",
    "    return tmp_train, tmp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Cats\n",
    "reduce_train, reduce_test = create_cats(reduce_train, reduce_test, categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min Max\n",
    "def create_min_max(train, test, categoricals):\n",
    "    tmp_train = train.copy()\n",
    "    tmp_test = test.copy()\n",
    "    scalars = [x for x in tmp_train.columns if x not in (categoricals + cols_to_drop)]\n",
    "    tmp_train[scalars] = tmp_train[scalars].apply(lambda x: (x - x.min()) / (x.max() - x.min())).fillna(0)\n",
    "    tmp_test[scalars] = tmp_test[scalars].apply(lambda x: (x - x.min()) / (x.max() - x.min())).fillna(0)\n",
    "    \n",
    "    return tmp_train, tmp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_train, reduce_test = create_min_max(reduce_train, reduce_test, categoricals)\n",
    "reduce_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_train.columns]\n",
    "reduce_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n",
    "features = [x for x in features if x not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: FEAT_A: Clip FEAT_B: 27253bdc - Correlation: 1.0\n",
      "2: FEAT_A: 2050 FEAT_B: 2040 - Correlation: 0.9965259434878112\n",
      "3: FEAT_A: 2050 FEAT_B: 2b9272f4 - Correlation: 0.9999839030068793\n",
      "4: FEAT_A: 2050 FEAT_B: 73757a5e - Correlation: 0.999805014671399\n",
      "5: FEAT_A: 2050 FEAT_B: 26fd2d99 - Correlation: 0.9965084543995784\n",
      "6: FEAT_A: 2050 FEAT_B: 08fd73f3 - Correlation: 0.9966123918733626\n",
      "7: FEAT_A: 2050 FEAT_B: dcaede90 - Correlation: 0.9965259434878112\n",
      "8: FEAT_A: 2050 FEAT_B: 37c53127 - Correlation: 1.0\n",
      "9: FEAT_A: 2050 FEAT_B: Scrub_A_Dub_2050 - Correlation: 1.0\n",
      "10: FEAT_A: 2050 FEAT_B: Scrub_A_Dub_3021 - Correlation: 0.999805014671399\n",
      "11: FEAT_A: 2050 FEAT_B: Scrub_A_Dub_2040 - Correlation: 0.9965259434878112\n",
      "12: FEAT_A: 2050 FEAT_B: Scrub_A_Dub_2030 - Correlation: 0.9966123918733626\n",
      "13: FEAT_A: 2050 FEAT_B: Scrub_A_Dub_3121 - Correlation: 0.9999839030068793\n",
      "14: FEAT_A: 2050 FEAT_B: Scrub_A_Dub_2020 - Correlation: 0.9965084543995784\n",
      "15: FEAT_A: 4230 FEAT_B: 4235 - Correlation: 0.9999995197498749\n",
      "16: FEAT_A: 4230 FEAT_B: 85de926c - Correlation: 0.9999995197498749\n",
      "17: FEAT_A: 4230 FEAT_B: ad148f58 - Correlation: 0.9999999999999999\n",
      "18: FEAT_A: 4230 FEAT_B: Bubble_Bath_4235 - Correlation: 0.9999995197498749\n",
      "19: FEAT_A: 4230 FEAT_B: Bubble_Bath_4230 - Correlation: 0.9999999999999999\n",
      "20: FEAT_A: 5000 FEAT_B: 5010 - Correlation: 0.9991849213605304\n",
      "21: FEAT_A: 5000 FEAT_B: a6d66e51 - Correlation: 0.9999999999999999\n",
      "22: FEAT_A: 5000 FEAT_B: 71e712d8 - Correlation: 0.9991849213605304\n",
      "23: FEAT_A: 5000 FEAT_B: Watering_Hole__Activity__5010 - Correlation: 0.9991849213605304\n",
      "24: FEAT_A: 5000 FEAT_B: Watering_Hole__Activity__5000 - Correlation: 0.9999999999999999\n",
      "25: FEAT_A: 3110 FEAT_B: 3010 - Correlation: 0.9999437304284393\n",
      "26: FEAT_A: 3120 FEAT_B: 3020 - Correlation: 0.9998761417908988\n",
      "27: FEAT_A: 3121 FEAT_B: 3021 - Correlation: 0.999909181283087\n",
      "28: FEAT_A: 4031 FEAT_B: 1996c610 - Correlation: 0.9999999999999999\n",
      "29: FEAT_A: 4031 FEAT_B: Dino_Drink_4031 - Correlation: 0.9999999999999999\n",
      "30: FEAT_A: 4050 FEAT_B: a1192f43 - Correlation: 1.0\n",
      "31: FEAT_A: 4050 FEAT_B: Crystals_Rule_4050 - Correlation: 1.0\n",
      "32: FEAT_A: 2020 FEAT_B: 2030 - Correlation: 0.9968874854317976\n",
      "33: FEAT_A: 4220 FEAT_B: 1340b8d7 - Correlation: 1.0\n",
      "34: FEAT_A: 4220 FEAT_B: Bubble_Bath_4220 - Correlation: 1.0\n",
      "35: FEAT_A: Heavy__Heavier__Heaviest FEAT_B: Heavy__Heavier__Heaviest_2000 - Correlation: 1.0\n",
      "36: FEAT_A: Tree_Top_City___Level_1 FEAT_B: Tree_Top_City___Level_1_2000 - Correlation: 1.0\n",
      "37: FEAT_A: Crystal_Caves___Level_2 FEAT_B: Crystal_Caves___Level_2_2000 - Correlation: 1.0\n",
      "38: FEAT_A: Pirate_s_Tale FEAT_B: Pirate_s_Tale_2000 - Correlation: 1.0\n",
      "39: FEAT_A: Bottle_Filler__Activity_ FEAT_B: bb3e370b - Correlation: 0.9950043311420291\n",
      "40: FEAT_A: Bottle_Filler__Activity_ FEAT_B: Bottle_Filler__Activity__4030 - Correlation: 0.9950043311420291\n",
      "41: FEAT_A: Welcome_to_Lost_Lagoon_ FEAT_B: Welcome_to_Lost_Lagoon__2000 - Correlation: 0.9999999999999999\n",
      "42: FEAT_A: Tree_Top_City___Level_2 FEAT_B: Tree_Top_City___Level_2_2000 - Correlation: 1.0\n",
      "43: FEAT_A: Lifting_Heavy_Things FEAT_B: Lifting_Heavy_Things_2000 - Correlation: 1.0\n",
      "44: FEAT_A: Treasure_Map FEAT_B: Treasure_Map_2000 - Correlation: 1.0\n",
      "45: FEAT_A: Magma_Peak___Level_1 FEAT_B: Magma_Peak___Level_1_2000 - Correlation: 1.0\n",
      "46: FEAT_A: Magma_Peak___Level_2 FEAT_B: Magma_Peak___Level_2_2000 - Correlation: 1.0\n",
      "47: FEAT_A: Tree_Top_City___Level_3 FEAT_B: Tree_Top_City___Level_3_2000 - Correlation: 1.0\n",
      "48: FEAT_A: Honey_Cake FEAT_B: Honey_Cake_2000 - Correlation: 0.9999999999999998\n",
      "49: FEAT_A: Balancing_Act FEAT_B: Balancing_Act_2000 - Correlation: 1.0\n",
      "50: FEAT_A: Ordering_Spheres FEAT_B: Ordering_Spheres_2000 - Correlation: 1.0\n",
      "51: FEAT_A: 12_Monkeys FEAT_B: 12_Monkeys_2000 - Correlation: 1.0\n",
      "52: FEAT_A: Slop_Problem FEAT_B: Slop_Problem_2000 - Correlation: 1.0\n",
      "53: FEAT_A: Crystal_Caves___Level_3 FEAT_B: Crystal_Caves___Level_3_2000 - Correlation: 0.9999999999999999\n",
      "54: FEAT_A: Rulers FEAT_B: Rulers_2000 - Correlation: 1.0\n",
      "55: FEAT_A: Costume_Box FEAT_B: Costume_Box_2000 - Correlation: 1.0\n",
      "56: FEAT_A: Crystal_Caves___Level_1 FEAT_B: Crystal_Caves___Level_1_2000 - Correlation: 1.0\n",
      "57: FEAT_A: e7561dd2 FEAT_B: Pan_Balance_4025 - Correlation: 0.9999999999999999\n",
      "58: FEAT_A: 51102b85 FEAT_B: Bird_Measurer__Assessment__4030 - Correlation: 1.0\n",
      "59: FEAT_A: a7640a16 FEAT_B: Happy_Camel_4070 - Correlation: 0.9999999999999998\n",
      "60: FEAT_A: 5f5b2617 FEAT_B: Bottle_Filler__Activity__4080 - Correlation: 1.0\n",
      "61: FEAT_A: 74e5f8a7 FEAT_B: Dino_Drink_4020 - Correlation: 1.0\n",
      "62: FEAT_A: b5053438 FEAT_B: 28520915 - Correlation: 0.9990905166101867\n",
      "63: FEAT_A: b5053438 FEAT_B: d3268efa - Correlation: 0.9995763267046287\n",
      "64: FEAT_A: b5053438 FEAT_B: Cauldron_Filler__Assessment__3121 - Correlation: 1.0\n",
      "65: FEAT_A: b5053438 FEAT_B: Cauldron_Filler__Assessment__3021 - Correlation: 0.9995763267046287\n",
      "66: FEAT_A: b5053438 FEAT_B: Cauldron_Filler__Assessment__2030 - Correlation: 0.9990905166101867\n",
      "67: FEAT_A: d88ca108 FEAT_B: Air_Show_2070 - Correlation: 0.9999999999999999\n",
      "68: FEAT_A: a44b10dc FEAT_B: Flower_Waterer__Activity__4070 - Correlation: 1.0\n",
      "69: FEAT_A: 92687c59 FEAT_B: Scrub_A_Dub_4090 - Correlation: 1.0\n",
      "70: FEAT_A: cb6010f8 FEAT_B: 56817e2b - Correlation: 0.9990475739429779\n",
      "71: FEAT_A: cb6010f8 FEAT_B: 47026d5f - Correlation: 0.9996901555713461\n",
      "72: FEAT_A: cb6010f8 FEAT_B: Chow_Time_2030 - Correlation: 0.9990475739429779\n",
      "73: FEAT_A: cb6010f8 FEAT_B: Chow_Time_3021 - Correlation: 0.9996901555713461\n",
      "74: FEAT_A: cb6010f8 FEAT_B: Chow_Time_3121 - Correlation: 0.9999999999999998\n",
      "75: FEAT_A: 119b5b02 FEAT_B: Dino_Dive_4080 - Correlation: 1.0\n",
      "76: FEAT_A: 3bb91ced FEAT_B: Happy_Camel_2081 - Correlation: 1.0\n",
      "77: FEAT_A: fd20ea40 FEAT_B: Leaf_Leader_4010 - Correlation: 1.0\n",
      "78: FEAT_A: 363d3849 FEAT_B: 9e4c8c7b - Correlation: 0.9992130941883662\n",
      "79: FEAT_A: 363d3849 FEAT_B: All_Star_Sorting_3110 - Correlation: 0.9992130941883662\n",
      "80: FEAT_A: 363d3849 FEAT_B: All_Star_Sorting_3010 - Correlation: 1.0\n",
      "81: FEAT_A: 5e109ec3 FEAT_B: Cart_Balancer__Assessment__4030 - Correlation: 0.9999999999999999\n",
      "82: FEAT_A: d88e8f25 FEAT_B: ac92046e - Correlation: 0.9999763070332084\n",
      "83: FEAT_A: d88e8f25 FEAT_B: Scrub_A_Dub_3020 - Correlation: 0.9999999999999998\n",
      "84: FEAT_A: d88e8f25 FEAT_B: Scrub_A_Dub_3120 - Correlation: 0.9999763070332084\n",
      "85: FEAT_A: 9b4001e4 FEAT_B: f5b8c21a - Correlation: 0.9976837802056752\n",
      "86: FEAT_A: 9b4001e4 FEAT_B: 58a0de5c - Correlation: 0.9999159362216206\n",
      "87: FEAT_A: 9b4001e4 FEAT_B: Air_Show_2030 - Correlation: 0.9976837802056752\n",
      "88: FEAT_A: 9b4001e4 FEAT_B: Air_Show_3121 - Correlation: 0.9999159362216206\n",
      "89: FEAT_A: 9b4001e4 FEAT_B: Air_Show_3021 - Correlation: 1.0\n",
      "90: FEAT_A: 3dfd4aa4 FEAT_B: 9d29771f - Correlation: 0.9968407858128351\n",
      "91: FEAT_A: 3dfd4aa4 FEAT_B: 3bfd1a65 - Correlation: 0.9989227582059058\n",
      "92: FEAT_A: 3dfd4aa4 FEAT_B: c74f40cd - Correlation: 0.996856591248274\n",
      "93: FEAT_A: 3dfd4aa4 FEAT_B: 83c6c409 - Correlation: 0.9999964240952471\n",
      "94: FEAT_A: 3dfd4aa4 FEAT_B: 28ed704e - Correlation: 0.9999097134882681\n",
      "95: FEAT_A: 3dfd4aa4 FEAT_B: db02c830 - Correlation: 0.9989193628092897\n",
      "96: FEAT_A: 3dfd4aa4 FEAT_B: Mushroom_Sorter__Assessment__2025 - Correlation: 0.9989193628092897\n",
      "97: FEAT_A: 3dfd4aa4 FEAT_B: Mushroom_Sorter__Assessment__2020 - Correlation: 1.0\n",
      "98: FEAT_A: 3dfd4aa4 FEAT_B: Mushroom_Sorter__Assessment__3121 - Correlation: 0.996856591248274\n",
      "99: FEAT_A: 3dfd4aa4 FEAT_B: Mushroom_Sorter__Assessment__4025 - Correlation: 0.9999097134882681\n",
      "100: FEAT_A: 3dfd4aa4 FEAT_B: Mushroom_Sorter__Assessment__2035 - Correlation: 0.9999964240952471\n",
      "101: FEAT_A: 3dfd4aa4 FEAT_B: Mushroom_Sorter__Assessment__2000 - Correlation: 0.9989227582059058\n",
      "102: FEAT_A: 3dfd4aa4 FEAT_B: Mushroom_Sorter__Assessment__3021 - Correlation: 0.9968407858128351\n",
      "103: FEAT_A: 392e14df FEAT_B: Cauldron_Filler__Assessment__4100 - Correlation: 1.0\n",
      "104: FEAT_A: 1325467d FEAT_B: Sandcastle_Builder__Activity__4070 - Correlation: 0.9999999999999999\n",
      "105: FEAT_A: 262136f4 FEAT_B: Leaf_Leader_4020 - Correlation: 0.9999999999999998\n",
      "106: FEAT_A: 7cf1bc53 FEAT_B: 5154fc30 - Correlation: 0.9961960780519666\n",
      "107: FEAT_A: 7cf1bc53 FEAT_B: 3babcb9b - Correlation: 0.9961136759226805\n",
      "108: FEAT_A: 7cf1bc53 FEAT_B: 3ddc79c3 - Correlation: 0.9982332387790258\n",
      "109: FEAT_A: 7cf1bc53 FEAT_B: e720d930 - Correlation: 0.9979345979643146\n",
      "110: FEAT_A: 7cf1bc53 FEAT_B: 3323d7e9 - Correlation: 0.9987451849598988\n",
      "111: FEAT_A: 7cf1bc53 FEAT_B: Crystals_Rule_3121 - Correlation: 0.9979345979643146\n",
      "112: FEAT_A: 7cf1bc53 FEAT_B: Crystals_Rule_3010 - Correlation: 0.9961960780519666\n",
      "113: FEAT_A: 7cf1bc53 FEAT_B: Crystals_Rule_2020 - Correlation: 0.9999999999999998\n",
      "114: FEAT_A: 7cf1bc53 FEAT_B: Crystals_Rule_3110 - Correlation: 0.9961136759226805\n",
      "115: FEAT_A: 7cf1bc53 FEAT_B: Crystals_Rule_2030 - Correlation: 0.9987451849598988\n",
      "116: FEAT_A: 7cf1bc53 FEAT_B: Crystals_Rule_3021 - Correlation: 0.9982332387790258\n",
      "117: FEAT_A: f3cd5473 FEAT_B: Pan_Balance_4070 - Correlation: 1.0\n",
      "118: FEAT_A: 6f8106d9 FEAT_B: Dino_Drink_4090 - Correlation: 0.9999999999999999\n",
      "119: FEAT_A: 0a08139c FEAT_B: 71fe8f75 - Correlation: 0.9999850342981553\n",
      "120: FEAT_A: 0a08139c FEAT_B: Bug_Measurer__Activity__3010 - Correlation: 1.0\n",
      "121: FEAT_A: 0a08139c FEAT_B: Bug_Measurer__Activity__3110 - Correlation: 0.9999850342981553\n",
      "122: FEAT_A: 49ed92e9 FEAT_B: bd701df8 - Correlation: 0.9993109138888528\n",
      "123: FEAT_A: 49ed92e9 FEAT_B: Watering_Hole__Activity__3110 - Correlation: 0.9993109138888528\n",
      "124: FEAT_A: 49ed92e9 FEAT_B: Watering_Hole__Activity__3010 - Correlation: 0.9999999999999998\n",
      "125: FEAT_A: c277e121 FEAT_B: d45ed6a1 - Correlation: 0.9979692879543314\n",
      "126: FEAT_A: c277e121 FEAT_B: b120f2ac - Correlation: 0.999998383574455\n",
      "127: FEAT_A: c277e121 FEAT_B: All_Star_Sorting_2025 - Correlation: 0.999998383574455\n",
      "128: FEAT_A: c277e121 FEAT_B: All_Star_Sorting_3120 - Correlation: 0.9979692879543314\n",
      "129: FEAT_A: c277e121 FEAT_B: All_Star_Sorting_3020 - Correlation: 1.0\n",
      "130: FEAT_A: 0d18d96c FEAT_B: Mushroom_Sorter__Assessment__4035 - Correlation: 1.0\n",
      "131: FEAT_A: 37ee8496 FEAT_B: 30614231 - Correlation: 0.9968428313010148\n",
      "132: FEAT_A: 37ee8496 FEAT_B: Cauldron_Filler__Assessment__4030 - Correlation: 1.0\n",
      "133: FEAT_A: 37ee8496 FEAT_B: Cauldron_Filler__Assessment__4020 - Correlation: 0.9968428313010148\n",
      "134: FEAT_A: 5de79a6a FEAT_B: 31973d56 - Correlation: 0.9973945339840764\n",
      "135: FEAT_A: 5de79a6a FEAT_B: Cart_Balancer__Assessment__3020 - Correlation: 0.9999999999999999\n",
      "136: FEAT_A: 5de79a6a FEAT_B: Cart_Balancer__Assessment__3120 - Correlation: 0.9973945339840764\n",
      "137: FEAT_A: 29f54413 FEAT_B: Leaf_Leader_2060 - Correlation: 1.0\n",
      "138: FEAT_A: 736f9581 FEAT_B: 9b23e8ee - Correlation: 1.0\n",
      "139: FEAT_A: 736f9581 FEAT_B: Egg_Dropper__Activity__2000 - Correlation: 1.0\n",
      "140: FEAT_A: 736f9581 FEAT_B: Egg_Dropper__Activity__2020 - Correlation: 1.0\n",
      "141: FEAT_A: 28f975ea FEAT_B: Air_Show_4020 - Correlation: 0.9999999999999998\n",
      "142: FEAT_A: 19967db1 FEAT_B: Chow_Time_4090 - Correlation: 0.9999999999999999\n",
      "143: FEAT_A: 16dffff1 FEAT_B: 77ead60d - Correlation: 0.9984674845132654\n",
      "144: FEAT_A: 16dffff1 FEAT_B: 4d911100 - Correlation: 0.9986046680098587\n",
      "145: FEAT_A: 16dffff1 FEAT_B: Dino_Drink_3021 - Correlation: 0.9984674845132654\n",
      "146: FEAT_A: 16dffff1 FEAT_B: Dino_Drink_2030 - Correlation: 1.0\n",
      "147: FEAT_A: 16dffff1 FEAT_B: Dino_Drink_3121 - Correlation: 0.9986046680098587\n",
      "148: FEAT_A: e080a381 FEAT_B: Pan_Balance_4090 - Correlation: 1.0\n",
      "149: FEAT_A: 6077cc36 FEAT_B: Bird_Measurer__Assessment__4080 - Correlation: 0.9999999999999998\n",
      "150: FEAT_A: 709b1251 FEAT_B: e3ff61fb - Correlation: 0.9995444786291252\n",
      "151: FEAT_A: 709b1251 FEAT_B: 7961e599 - Correlation: 0.995000488193363\n",
      "152: FEAT_A: 709b1251 FEAT_B: Dino_Dive_2020 - Correlation: 0.995000488193363\n",
      "153: FEAT_A: 709b1251 FEAT_B: Dino_Dive_3121 - Correlation: 1.0\n",
      "154: FEAT_A: 709b1251 FEAT_B: Dino_Dive_3021 - Correlation: 0.9995444786291252\n",
      "155: FEAT_A: abc5811c FEAT_B: Happy_Camel_4010 - Correlation: 1.0\n",
      "156: FEAT_A: 3ee399c3 FEAT_B: Cauldron_Filler__Assessment__4070 - Correlation: 0.9999999999999999\n",
      "157: FEAT_A: 070a5291 FEAT_B: Bird_Measurer__Assessment__4100 - Correlation: 0.9999999999999998\n",
      "158: FEAT_A: 53c6e11a FEAT_B: Leaf_Leader_2075 - Correlation: 1.0\n",
      "159: FEAT_A: d2278a3b FEAT_B: Bottle_Filler__Activity__2000 - Correlation: 0.9999999999999999\n",
      "160: FEAT_A: 7fd1ac25 FEAT_B: Egg_Dropper__Activity__4080 - Correlation: 1.0\n",
      "161: FEAT_A: 05ad839b FEAT_B: Happy_Camel_4090 - Correlation: 0.9999999999999998\n",
      "162: FEAT_A: 5d042115 FEAT_B: Flower_Waterer__Activity__4030 - Correlation: 0.9999999999999998\n",
      "163: FEAT_A: 2c4e6db0 FEAT_B: All_Star_Sorting_2020 - Correlation: 1.0\n",
      "164: FEAT_A: 763fc34e FEAT_B: e57dd7af - Correlation: 0.9972721980394427\n",
      "165: FEAT_A: 763fc34e FEAT_B: Leaf_Leader_3120 - Correlation: 0.9972721980394427\n",
      "166: FEAT_A: 763fc34e FEAT_B: Leaf_Leader_3020 - Correlation: 1.0\n",
      "167: FEAT_A: 06372577 FEAT_B: Air_Show_2060 - Correlation: 1.0\n",
      "168: FEAT_A: 562cec5f FEAT_B: Chest_Sorter__Assessment__4025 - Correlation: 1.0\n",
      "169: FEAT_A: a52b92d5 FEAT_B: a1e4395d - Correlation: 0.9999867422000192\n",
      "170: FEAT_A: a52b92d5 FEAT_B: Mushroom_Sorter__Assessment__3010 - Correlation: 0.9999867422000192\n",
      "171: FEAT_A: a52b92d5 FEAT_B: Mushroom_Sorter__Assessment__3110 - Correlation: 1.0\n",
      "172: FEAT_A: e5c9df6f FEAT_B: 3afde5dd - Correlation: 0.9990910601838007\n",
      "173: FEAT_A: e5c9df6f FEAT_B: b012cd7f - Correlation: 0.9990885395773588\n",
      "174: FEAT_A: e5c9df6f FEAT_B: Leaf_Leader_2030 - Correlation: 0.9990885395773588\n",
      "175: FEAT_A: e5c9df6f FEAT_B: Leaf_Leader_3021 - Correlation: 0.9990910601838007\n",
      "176: FEAT_A: e5c9df6f FEAT_B: Leaf_Leader_3121 - Correlation: 1.0\n",
      "177: FEAT_A: 7d093bf9 FEAT_B: Chow_Time_2000 - Correlation: 1.0\n",
      "178: FEAT_A: b7530680 FEAT_B: e9c52111 - Correlation: 0.9988176899646241\n",
      "179: FEAT_A: b7530680 FEAT_B: Bottle_Filler__Activity__2020 - Correlation: 0.9999999999999999\n",
      "180: FEAT_A: b7530680 FEAT_B: Bottle_Filler__Activity__2030 - Correlation: 0.9988176899646241\n",
      "181: FEAT_A: a5e9da97 FEAT_B: Pan_Balance_4100 - Correlation: 1.0\n",
      "182: FEAT_A: 3edf6747 FEAT_B: Cauldron_Filler__Assessment__4035 - Correlation: 1.0\n",
      "183: FEAT_A: b80e5e84 FEAT_B: 7ab78247 - Correlation: 0.9998336590281098\n",
      "184: FEAT_A: b80e5e84 FEAT_B: Egg_Dropper__Activity__3110 - Correlation: 1.0\n",
      "185: FEAT_A: b80e5e84 FEAT_B: Egg_Dropper__Activity__3010 - Correlation: 0.9998336590281098\n",
      "186: FEAT_A: 804ee27f FEAT_B: Pan_Balance_4020 - Correlation: 1.0\n",
      "187: FEAT_A: df4940d3 FEAT_B: 67439901 - Correlation: 0.9999351626435945\n",
      "188: FEAT_A: df4940d3 FEAT_B: Bottle_Filler__Activity__3110 - Correlation: 0.9999999999999999\n",
      "189: FEAT_A: df4940d3 FEAT_B: Bottle_Filler__Activity__3010 - Correlation: 0.9999351626435945\n",
      "190: FEAT_A: cfbd47c8 FEAT_B: Chow_Time_4030 - Correlation: 0.9999999999999998\n",
      "191: FEAT_A: c2baf0bd FEAT_B: Happy_Camel_2020 - Correlation: 1.0\n",
      "192: FEAT_A: 69fdac0a FEAT_B: 8d7e386c - Correlation: 0.9996590210382701\n",
      "193: FEAT_A: 69fdac0a FEAT_B: Happy_Camel_3110 - Correlation: 1.0\n",
      "194: FEAT_A: 69fdac0a FEAT_B: Happy_Camel_3010 - Correlation: 0.9996590210382701\n",
      "195: FEAT_A: 04df9b66 FEAT_B: 5290eab1 - Correlation: 0.999819047746618\n",
      "196: FEAT_A: 04df9b66 FEAT_B: Cauldron_Filler__Assessment__3020 - Correlation: 1.0\n",
      "197: FEAT_A: 04df9b66 FEAT_B: Cauldron_Filler__Assessment__3120 - Correlation: 0.999819047746618\n",
      "198: FEAT_A: 55115cbd FEAT_B: 6f4adc4b - Correlation: 0.9997831892615416\n",
      "199: FEAT_A: 55115cbd FEAT_B: Bubble_Bath_3121 - Correlation: 1.0\n",
      "200: FEAT_A: 55115cbd FEAT_B: Bubble_Bath_3021 - Correlation: 0.9997831892615416\n",
      "201: FEAT_A: 8d84fa81 FEAT_B: Bubble_Bath_4010 - Correlation: 1.0\n",
      "202: FEAT_A: cf82af56 FEAT_B: Scrub_A_Dub_4070 - Correlation: 1.0\n",
      "203: FEAT_A: 828e68f9 FEAT_B: 795e4a37 - Correlation: 0.999212732433683\n",
      "204: FEAT_A: 828e68f9 FEAT_B: Cart_Balancer__Assessment__3110 - Correlation: 1.0\n",
      "205: FEAT_A: 828e68f9 FEAT_B: Cart_Balancer__Assessment__3010 - Correlation: 0.999212732433683\n",
      "206: FEAT_A: cdd22e43 FEAT_B: Chicken_Balancer__Activity__4035 - Correlation: 1.0\n",
      "207: FEAT_A: 36fa3ebe FEAT_B: a8a78786 - Correlation: 0.997955912062381\n",
      "208: FEAT_A: 36fa3ebe FEAT_B: c7fe2a55 - Correlation: 0.9998350234117673\n",
      "209: FEAT_A: 36fa3ebe FEAT_B: Happy_Camel_3121 - Correlation: 0.997955912062381\n",
      "210: FEAT_A: 36fa3ebe FEAT_B: Happy_Camel_3021 - Correlation: 0.9998350234117673\n",
      "211: FEAT_A: 36fa3ebe FEAT_B: Happy_Camel_2030 - Correlation: 0.9999999999999998\n",
      "212: FEAT_A: d185d3ea FEAT_B: Chow_Time_4035 - Correlation: 1.0\n",
      "213: FEAT_A: 99abe2bb FEAT_B: Bubble_Bath_2080 - Correlation: 0.9999999999999998\n",
      "214: FEAT_A: f50fc6c1 FEAT_B: Watering_Hole__Activity__4021 - Correlation: 1.0\n",
      "215: FEAT_A: 93b353f2 FEAT_B: Chest_Sorter__Assessment__4100 - Correlation: 0.9999999999999999\n",
      "216: FEAT_A: 2a512369 FEAT_B: 33505eae - Correlation: 0.9994585292841931\n",
      "217: FEAT_A: 2a512369 FEAT_B: Leaf_Leader_3010 - Correlation: 0.9994585292841931\n",
      "218: FEAT_A: 2a512369 FEAT_B: Leaf_Leader_3110 - Correlation: 1.0\n",
      "219: FEAT_A: e694a35b FEAT_B: Fireworks__Activity__4020 - Correlation: 0.9999999999999999\n",
      "220: FEAT_A: f56e0afc FEAT_B: Bird_Measurer__Assessment__2000 - Correlation: 1.0\n",
      "221: FEAT_A: 2ec694de FEAT_B: Bug_Measurer__Activity__4080 - Correlation: 1.0\n",
      "222: FEAT_A: 0330ab6a FEAT_B: 2230fab4 - Correlation: 0.9998673365188055\n",
      "223: FEAT_A: 0330ab6a FEAT_B: Chow_Time_3020 - Correlation: 1.0\n",
      "224: FEAT_A: 0330ab6a FEAT_B: Chow_Time_3120 - Correlation: 0.9998673365188055\n",
      "225: FEAT_A: 1b54d27f FEAT_B: Watering_Hole__Activity__2010 - Correlation: 1.0\n",
      "226: FEAT_A: 5b49460a FEAT_B: 155f62a4 - Correlation: 1.0\n",
      "227: FEAT_A: 5b49460a FEAT_B: Chest_Sorter__Assessment__2020 - Correlation: 1.0\n",
      "228: FEAT_A: 5b49460a FEAT_B: Chest_Sorter__Assessment__2000 - Correlation: 1.0\n",
      "229: FEAT_A: a5be6304 FEAT_B: 6c930e6e - Correlation: 0.9963210379524845\n",
      "230: FEAT_A: a5be6304 FEAT_B: Mushroom_Sorter__Assessment__2010 - Correlation: 1.0\n",
      "231: FEAT_A: a5be6304 FEAT_B: Mushroom_Sorter__Assessment__2030 - Correlation: 0.9963210379524845\n",
      "232: FEAT_A: a8efe47b FEAT_B: Chest_Sorter__Assessment__4030 - Correlation: 0.9999999999999999\n",
      "233: FEAT_A: c1cac9a2 FEAT_B: Scrub_A_Dub_2081 - Correlation: 0.9999999999999999\n",
      "234: FEAT_A: 1375ccb7 FEAT_B: bdf49a58 - Correlation: 0.9997857456466123\n",
      "235: FEAT_A: 1375ccb7 FEAT_B: Bird_Measurer__Assessment__3110 - Correlation: 0.9997857456466123\n",
      "236: FEAT_A: 1375ccb7 FEAT_B: Bird_Measurer__Assessment__3010 - Correlation: 1.0\n",
      "237: FEAT_A: 7ad3efc6 FEAT_B: 65a38bf7 - Correlation: 0.9999742946915021\n",
      "238: FEAT_A: 7ad3efc6 FEAT_B: Cart_Balancer__Assessment__2020 - Correlation: 0.9999742946915021\n",
      "239: FEAT_A: 7ad3efc6 FEAT_B: Cart_Balancer__Assessment__2000 - Correlation: 1.0\n",
      "240: FEAT_A: 884228c8 FEAT_B: Fireworks__Activity__4070 - Correlation: 1.0\n",
      "241: FEAT_A: 832735e1 FEAT_B: ab3136ba - Correlation: 0.9998637945770232\n",
      "242: FEAT_A: 832735e1 FEAT_B: Dino_Dive_3110 - Correlation: 0.9998637945770232\n",
      "243: FEAT_A: 832735e1 FEAT_B: Dino_Dive_3010 - Correlation: 1.0\n",
      "244: FEAT_A: 4c2ec19f FEAT_B: Egg_Dropper__Activity__4025 - Correlation: 1.0\n",
      "245: FEAT_A: 792530f8 FEAT_B: Dino_Drink_4030 - Correlation: 0.9999999999999998\n",
      "246: FEAT_A: ecaab346 FEAT_B: b2e5b0f1 - Correlation: 0.9998493398612228\n",
      "247: FEAT_A: ecaab346 FEAT_B: b74258a0 - Correlation: 0.9999999999999999\n",
      "248: FEAT_A: ecaab346 FEAT_B: Cart_Balancer__Assessment__2010 - Correlation: 0.9998493398612228\n",
      "249: FEAT_A: ecaab346 FEAT_B: Cart_Balancer__Assessment__3121 - Correlation: 0.9999999999999999\n",
      "250: FEAT_A: ecaab346 FEAT_B: Cart_Balancer__Assessment__2030 - Correlation: 0.9999999999999999\n",
      "251: FEAT_A: 923afab1 FEAT_B: 2dcad279 - Correlation: 0.9999294631962794\n",
      "252: FEAT_A: 923afab1 FEAT_B: Cauldron_Filler__Assessment__3110 - Correlation: 0.9999294631962794\n",
      "253: FEAT_A: 923afab1 FEAT_B: Cauldron_Filler__Assessment__3010 - Correlation: 0.9999999999999998\n",
      "254: FEAT_A: 3a4be871 FEAT_B: Flower_Waterer__Activity__4080 - Correlation: 1.0\n",
      "255: FEAT_A: 9ce586dd FEAT_B: Chest_Sorter__Assessment__4035 - Correlation: 0.9999999999999998\n",
      "256: FEAT_A: 3d0b9317 FEAT_B: Chest_Sorter__Assessment__4040 - Correlation: 1.0\n",
      "257: FEAT_A: 1c178d24 FEAT_B: 250513af - Correlation: 0.9999803429610531\n",
      "258: FEAT_A: 1c178d24 FEAT_B: cf7638f3 - Correlation: 0.9996196393003325\n",
      "259: FEAT_A: 1c178d24 FEAT_B: a592d54e - Correlation: 0.9973125883100863\n",
      "260: FEAT_A: 1c178d24 FEAT_B: Pan_Balance_3121 - Correlation: 0.9996196393003325\n",
      "261: FEAT_A: 1c178d24 FEAT_B: Pan_Balance_3021 - Correlation: 0.9999803429610531\n",
      "262: FEAT_A: 1c178d24 FEAT_B: Pan_Balance_2030 - Correlation: 0.9999999999999999\n",
      "263: FEAT_A: 1c178d24 FEAT_B: Pan_Balance_2020 - Correlation: 0.9973125883100863\n",
      "264: FEAT_A: 84b0e0c8 FEAT_B: ea321fb1 - Correlation: 0.9993007600205095\n",
      "265: FEAT_A: 84b0e0c8 FEAT_B: Chicken_Balancer__Activity__3110 - Correlation: 1.0\n",
      "266: FEAT_A: 84b0e0c8 FEAT_B: Chicken_Balancer__Activity__3010 - Correlation: 0.9993007600205095\n",
      "267: FEAT_A: 2dc29e21 FEAT_B: All_Star_Sorting_4020 - Correlation: 1.0\n",
      "268: FEAT_A: 907a054b FEAT_B: c51d8688 - Correlation: 0.9999667370361721\n",
      "269: FEAT_A: 907a054b FEAT_B: Pan_Balance_3120 - Correlation: 0.9999667370361721\n",
      "270: FEAT_A: 907a054b FEAT_B: Pan_Balance_3020 - Correlation: 1.0\n",
      "271: FEAT_A: 1beb320a FEAT_B: 8f094001 - Correlation: 0.9980774800878138\n",
      "272: FEAT_A: 1beb320a FEAT_B: Bubble_Bath_2020 - Correlation: 0.9999999999999998\n",
      "273: FEAT_A: 1beb320a FEAT_B: Bubble_Bath_4045 - Correlation: 0.9980774800878138\n",
      "274: FEAT_A: 731c0cbe FEAT_B: Bird_Measurer__Assessment__4090 - Correlation: 1.0\n",
      "275: FEAT_A: b88f38da FEAT_B: beb0a7b9 - Correlation: 0.999912517982976\n",
      "276: FEAT_A: b88f38da FEAT_B: Fireworks__Activity__3010 - Correlation: 0.999912517982976\n",
      "277: FEAT_A: b88f38da FEAT_B: Fireworks__Activity__3110 - Correlation: 0.9999999999999998\n",
      "278: FEAT_A: 8fee50e2 FEAT_B: Bird_Measurer__Assessment__4020 - Correlation: 1.0\n",
      "279: FEAT_A: df4fe8b6 FEAT_B: ea296733 - Correlation: 0.9972489515829084\n",
      "280: FEAT_A: df4fe8b6 FEAT_B: Chest_Sorter__Assessment__3020 - Correlation: 0.9972489515829084\n",
      "281: FEAT_A: df4fe8b6 FEAT_B: Chest_Sorter__Assessment__3120 - Correlation: 1.0\n",
      "282: FEAT_A: df4fe8b6 FEAT_B: afa_Chest_Sorter__Assessment_ - Correlation: 0.9972434688333263\n",
      "283: FEAT_A: 77c76bc5 FEAT_B: Cauldron_Filler__Assessment__4090 - Correlation: 1.0\n",
      "284: FEAT_A: e64e2cfd FEAT_B: Watering_Hole__Activity__2000 - Correlation: 1.0\n",
      "285: FEAT_A: 00c73085 FEAT_B: Dino_Dive_2030 - Correlation: 0.9999999999999998\n",
      "286: FEAT_A: c189aaf2 FEAT_B: Happy_Camel_2083 - Correlation: 1.0\n",
      "287: FEAT_A: 13f56524 FEAT_B: Mushroom_Sorter__Assessment__4080 - Correlation: 1.0\n",
      "288: FEAT_A: 3ccd3f02 FEAT_B: 3dcdda7f - Correlation: 0.9997581825041642\n",
      "289: FEAT_A: 3ccd3f02 FEAT_B: Chest_Sorter__Assessment__3010 - Correlation: 0.9997581825041642\n",
      "290: FEAT_A: 3ccd3f02 FEAT_B: Chest_Sorter__Assessment__3110 - Correlation: 1.0\n",
      "291: FEAT_A: 1cf54632 FEAT_B: Bubble_Bath_2000 - Correlation: 0.9999999999999999\n",
      "292: FEAT_A: 598f4598 FEAT_B: Flower_Waterer__Activity__4025 - Correlation: 1.0\n",
      "293: FEAT_A: 5c2f29ca FEAT_B: Cart_Balancer__Assessment__4020 - Correlation: 1.0\n",
      "294: FEAT_A: cb1178ad FEAT_B: Chest_Sorter__Assessment__4090 - Correlation: 1.0\n",
      "295: FEAT_A: 1cc7cfca FEAT_B: All_Star_Sorting_4030 - Correlation: 1.0\n",
      "296: FEAT_A: 7525289a FEAT_B: 45d01abe - Correlation: 0.9982095234026279\n",
      "297: FEAT_A: 7525289a FEAT_B: Bird_Measurer__Assessment__3121 - Correlation: 1.0\n",
      "298: FEAT_A: 7525289a FEAT_B: Bird_Measurer__Assessment__3021 - Correlation: 0.9982095234026279\n",
      "299: FEAT_A: 7525289a FEAT_B: ata_Bird_Measurer__Assessment_ - Correlation: 0.9982095234026279\n",
      "300: FEAT_A: f54238ee FEAT_B: Fireworks__Activity__4090 - Correlation: 1.0\n",
      "301: FEAT_A: 6f445b57 FEAT_B: Chow_Time_4080 - Correlation: 1.0\n",
      "302: FEAT_A: 7d5c30a2 FEAT_B: Dino_Dive_2060 - Correlation: 1.0\n",
      "303: FEAT_A: 4d6737eb FEAT_B: Dino_Drink_2070 - Correlation: 1.0\n",
      "304: FEAT_A: a0faea5d FEAT_B: Bubble_Bath_4070 - Correlation: 1.0\n",
      "305: FEAT_A: 4b5efe37 FEAT_B: b7dc8128 - Correlation: 0.9980151285384015\n",
      "306: FEAT_A: 4b5efe37 FEAT_B: All_Star_Sorting_4010 - Correlation: 1.0\n",
      "307: FEAT_A: 4b5efe37 FEAT_B: All_Star_Sorting_2000 - Correlation: 0.9980151285384015\n",
      "308: FEAT_A: e79f3763 FEAT_B: Bug_Measurer__Activity__4030 - Correlation: 1.0\n",
      "309: FEAT_A: bfc77bd6 FEAT_B: Chest_Sorter__Assessment__4080 - Correlation: 1.0\n",
      "310: FEAT_A: 14de4c5d FEAT_B: Air_Show_4100 - Correlation: 1.0\n",
      "311: FEAT_A: bd612267 FEAT_B: Chest_Sorter__Assessment__4070 - Correlation: 0.9999999999999998\n",
      "312: FEAT_A: 9c5ef70c FEAT_B: Pan_Balance_2000 - Correlation: 1.0\n",
      "313: FEAT_A: ecc6157f FEAT_B: Cart_Balancer__Assessment__4080 - Correlation: 0.9999999999999998\n",
      "314: FEAT_A: a8876db3 FEAT_B: Cart_Balancer__Assessment__3021 - Correlation: 0.9999999999999999\n",
      "315: FEAT_A: a8876db3 FEAT_B: ata_Cart_Balancer__Assessment_ - Correlation: 0.9999999999999999\n",
      "316: FEAT_A: 6bf9e3e1 FEAT_B: Happy_Camel_4040 - Correlation: 1.0\n",
      "317: FEAT_A: c7128948 FEAT_B: Mushroom_Sorter__Assessment__4040 - Correlation: 0.9999999999999998\n",
      "318: FEAT_A: 90ea0bac FEAT_B: 5859dfb6 - Correlation: 0.9981052472610575\n",
      "319: FEAT_A: 90ea0bac FEAT_B: Bubble_Bath_3120 - Correlation: 0.9981052472610575\n",
      "320: FEAT_A: 90ea0bac FEAT_B: Bubble_Bath_3020 - Correlation: 1.0\n",
      "321: FEAT_A: a1bbe385 FEAT_B: f28c589a - Correlation: 0.9999536797342228\n",
      "322: FEAT_A: a1bbe385 FEAT_B: Air_Show_3110 - Correlation: 1.0\n",
      "323: FEAT_A: a1bbe385 FEAT_B: Air_Show_3010 - Correlation: 0.9999536797342228\n",
      "324: FEAT_A: 46b50ba8 FEAT_B: Happy_Camel_4095 - Correlation: 1.0\n",
      "325: FEAT_A: 2b058fe3 FEAT_B: Cauldron_Filler__Assessment__2010 - Correlation: 1.0\n",
      "326: FEAT_A: 3afb49e6 FEAT_B: e4f1efe6 - Correlation: 0.9985179906980766\n",
      "327: FEAT_A: 3afb49e6 FEAT_B: Chest_Sorter__Assessment__3121 - Correlation: 0.9985179906980766\n",
      "328: FEAT_A: 3afb49e6 FEAT_B: Chest_Sorter__Assessment__3021 - Correlation: 0.9999999999999999\n",
      "329: FEAT_A: 3afb49e6 FEAT_B: ata_Chest_Sorter__Assessment_ - Correlation: 0.9996140232445464\n",
      "330: FEAT_A: 6c517a88 FEAT_B: Dino_Drink_4070 - Correlation: 1.0\n",
      "331: FEAT_A: c6971acf FEAT_B: Dino_Drink_2060 - Correlation: 1.0\n",
      "332: FEAT_A: d3f1e122 FEAT_B: Bottle_Filler__Activity__4035 - Correlation: 1.0\n",
      "333: FEAT_A: 5e812b27 FEAT_B: Sandcastle_Builder__Activity__4030 - Correlation: 1.0\n",
      "334: FEAT_A: a76029ee FEAT_B: Bird_Measurer__Assessment__4040 - Correlation: 0.9999999999999999\n",
      "335: FEAT_A: 84538528 FEAT_B: Sandcastle_Builder__Activity__4020 - Correlation: 1.0\n",
      "336: FEAT_A: 91561152 FEAT_B: Cauldron_Filler__Assessment__4025 - Correlation: 1.0\n",
      "337: FEAT_A: ecc36b7f FEAT_B: Bubble_Bath_4095 - Correlation: 1.0\n",
      "338: FEAT_A: 37db1c2f FEAT_B: Happy_Camel_4045 - Correlation: 1.0\n",
      "339: FEAT_A: fcfdffb6 FEAT_B: Flower_Waterer__Activity__4022 - Correlation: 1.0\n",
      "340: FEAT_A: 565a3990 FEAT_B: Bug_Measurer__Activity__4070 - Correlation: 0.9999999999999999\n",
      "341: FEAT_A: 6cf7d25c FEAT_B: 15f99afc - Correlation: 0.9994848234397995\n",
      "342: FEAT_A: 6cf7d25c FEAT_B: Pan_Balance_3010 - Correlation: 1.0\n",
      "343: FEAT_A: 6cf7d25c FEAT_B: Pan_Balance_3110 - Correlation: 0.9994848234397995\n",
      "344: FEAT_A: 90d848e0 FEAT_B: Cauldron_Filler__Assessment__2000 - Correlation: 1.0\n",
      "345: FEAT_A: d9c005dd FEAT_B: Happy_Camel_2000 - Correlation: 1.0\n",
      "346: FEAT_A: 9d4e7b25 FEAT_B: Cart_Balancer__Assessment__4040 - Correlation: 1.0\n",
      "347: FEAT_A: 15eb4a7d FEAT_B: 0413e89d - Correlation: 0.9997266832893084\n",
      "348: FEAT_A: 15eb4a7d FEAT_B: Bubble_Bath_3110 - Correlation: 1.0\n",
      "349: FEAT_A: 15eb4a7d FEAT_B: Bubble_Bath_3010 - Correlation: 0.9997266832893084\n",
      "350: FEAT_A: f7e47413 FEAT_B: f71c4741 - Correlation: 0.9999426890770886\n",
      "351: FEAT_A: f7e47413 FEAT_B: Scrub_A_Dub_3010 - Correlation: 0.9999426890770886\n",
      "352: FEAT_A: f7e47413 FEAT_B: Scrub_A_Dub_3110 - Correlation: 0.9999999999999999\n",
      "353: FEAT_A: 1af8be29 FEAT_B: 3bf1cf26 - Correlation: 0.9998900847287081\n",
      "354: FEAT_A: 1af8be29 FEAT_B: Happy_Camel_3120 - Correlation: 0.9998900847287081\n",
      "355: FEAT_A: 1af8be29 FEAT_B: Happy_Camel_3020 - Correlation: 0.9999999999999999\n",
      "356: FEAT_A: 9e6b7fb5 FEAT_B: Chow_Time_4095 - Correlation: 0.9999999999999999\n",
      "357: FEAT_A: e37a2b78 FEAT_B: ad2fc29c - Correlation: 0.9992191329664136\n",
      "358: FEAT_A: e37a2b78 FEAT_B: 17113b36 - Correlation: 0.9982548122198944\n",
      "359: FEAT_A: e37a2b78 FEAT_B: Bird_Measurer__Assessment__3020 - Correlation: 0.9992191329664136\n",
      "360: FEAT_A: e37a2b78 FEAT_B: Bird_Measurer__Assessment__4110 - Correlation: 0.9982548122198944\n",
      "361: FEAT_A: e37a2b78 FEAT_B: Bird_Measurer__Assessment__3120 - Correlation: 1.0\n",
      "362: FEAT_A: e37a2b78 FEAT_B: afa_Bird_Measurer__Assessment_ - Correlation: 0.9983241920747598\n",
      "363: FEAT_A: 022b4259 FEAT_B: Bug_Measurer__Activity__4025 - Correlation: 1.0\n",
      "364: FEAT_A: 4a09ace1 FEAT_B: Scrub_A_Dub_2083 - Correlation: 1.0\n",
      "365: FEAT_A: 1575e76c FEAT_B: Air_Show_2020 - Correlation: 1.0\n",
      "366: FEAT_A: a29c5338 FEAT_B: 7f0836bf - Correlation: 0.9986531654717654\n",
      "367: FEAT_A: a29c5338 FEAT_B: Dino_Drink_3010 - Correlation: 1.0\n",
      "368: FEAT_A: a29c5338 FEAT_B: Dino_Drink_3110 - Correlation: 0.9986531654717654\n",
      "369: FEAT_A: 0086365d FEAT_B: Pan_Balance_4010 - Correlation: 1.0\n",
      "370: FEAT_A: 4ef8cdd3 FEAT_B: Chow_Time_4020 - Correlation: 1.0\n",
      "371: FEAT_A: 88d4a5be FEAT_B: 160654fd - Correlation: 0.9989297172615935\n",
      "372: FEAT_A: 88d4a5be FEAT_B: Mushroom_Sorter__Assessment__3120 - Correlation: 1.0\n",
      "373: FEAT_A: 88d4a5be FEAT_B: Mushroom_Sorter__Assessment__3020 - Correlation: 0.9989297172615935\n",
      "374: FEAT_A: 88d4a5be FEAT_B: afa_Mushroom_Sorter__Assessment_ - Correlation: 0.998929367927868\n",
      "375: FEAT_A: cc5087a3 FEAT_B: Crystals_Rule_4010 - Correlation: 1.0\n",
      "376: FEAT_A: 5348fd84 FEAT_B: Cauldron_Filler__Assessment__4040 - Correlation: 1.0\n",
      "377: FEAT_A: 48349b14 FEAT_B: Crystals_Rule_2000 - Correlation: 0.9999999999999998\n",
      "378: FEAT_A: 8b757ab8 FEAT_B: 44cb4907 - Correlation: 0.9998350587947111\n",
      "379: FEAT_A: 8b757ab8 FEAT_B: Crystals_Rule_3020 - Correlation: 0.9998350587947111\n",
      "380: FEAT_A: 8b757ab8 FEAT_B: Crystals_Rule_3120 - Correlation: 1.0\n",
      "381: FEAT_A: 532a2afb FEAT_B: Cauldron_Filler__Assessment__2020 - Correlation: 0.9999999999999999\n",
      "382: FEAT_A: 01ca3a3c FEAT_B: Leaf_Leader_4080 - Correlation: 0.9999999999999999\n",
      "383: FEAT_A: 6f4bd64e FEAT_B: Air_Show_4090 - Correlation: 1.0\n",
      "384: FEAT_A: 99ea62f3 FEAT_B: Bubble_Bath_2083 - Correlation: 1.0\n",
      "385: FEAT_A: 1f19558b FEAT_B: ca11f653 - Correlation: 0.998316427341078\n",
      "386: FEAT_A: 1f19558b FEAT_B: daac11b0 - Correlation: 0.9991356096406628\n",
      "387: FEAT_A: 1f19558b FEAT_B: All_Star_Sorting_3121 - Correlation: 1.0\n",
      "388: FEAT_A: 1f19558b FEAT_B: All_Star_Sorting_2030 - Correlation: 0.998316427341078\n",
      "389: FEAT_A: 1f19558b FEAT_B: All_Star_Sorting_3021 - Correlation: 0.9991356096406628\n",
      "390: FEAT_A: 363c86c9 FEAT_B: Bug_Measurer__Activity__4035 - Correlation: 1.0\n",
      "391: FEAT_A: 9554a50b FEAT_B: Cauldron_Filler__Assessment__4080 - Correlation: 1.0\n",
      "392: FEAT_A: d38c2fd7 FEAT_B: Bird_Measurer__Assessment__4035 - Correlation: 0.9999999999999998\n",
      "393: FEAT_A: 63f13dd7 FEAT_B: Chow_Time_2020 - Correlation: 1.0\n",
      "394: FEAT_A: 7372e1a5 FEAT_B: Chow_Time_4070 - Correlation: 0.9999999999999999\n",
      "395: FEAT_A: 4901243f FEAT_B: Fireworks__Activity__2000 - Correlation: 1.0\n",
      "396: FEAT_A: 6043a2b4 FEAT_B: All_Star_Sorting_4090 - Correlation: 1.0\n",
      "397: FEAT_A: 222660ff FEAT_B: 38074c54 - Correlation: 1.0\n",
      "398: FEAT_A: 222660ff FEAT_B: Chest_Sorter__Assessment__2030 - Correlation: 1.0\n",
      "399: FEAT_A: 222660ff FEAT_B: Chest_Sorter__Assessment__2010 - Correlation: 1.0\n",
      "400: FEAT_A: 46cd75b4 FEAT_B: Chicken_Balancer__Activity__4022 - Correlation: 1.0\n",
      "401: FEAT_A: ec138c1c FEAT_B: Bird_Measurer__Assessment__2020 - Correlation: 1.0\n",
      "402: FEAT_A: 8d748b58 FEAT_B: Bug_Measurer__Activity__4090 - Correlation: 1.0\n",
      "403: FEAT_A: d02b7a8e FEAT_B: All_Star_Sorting_4035 - Correlation: 1.0\n",
      "404: FEAT_A: 499edb7c FEAT_B: Chicken_Balancer__Activity__4020 - Correlation: 1.0\n",
      "405: FEAT_A: 611485c5 FEAT_B: Fireworks__Activity__4080 - Correlation: 1.0\n",
      "406: FEAT_A: 2fb91ec1 FEAT_B: d2e9262e - Correlation: 0.9991434495208753\n",
      "407: FEAT_A: 2fb91ec1 FEAT_B: Watering_Hole__Activity__4020 - Correlation: 0.9991434495208753\n",
      "408: FEAT_A: 2fb91ec1 FEAT_B: Watering_Hole__Activity__4025 - Correlation: 1.0\n",
      "409: FEAT_A: d06f75b5 FEAT_B: c54cf6c5 - Correlation: 0.9972383579301791\n",
      "410: FEAT_A: d06f75b5 FEAT_B: 895865f3 - Correlation: 0.9991496470458583\n",
      "411: FEAT_A: d06f75b5 FEAT_B: 3bb91dda - Correlation: 0.9970877409320826\n",
      "412: FEAT_A: d06f75b5 FEAT_B: Bubble_Bath_4020 - Correlation: 0.9970877409320826\n",
      "413: FEAT_A: d06f75b5 FEAT_B: Bubble_Bath_2030 - Correlation: 0.9991496470458583\n",
      "414: FEAT_A: d06f75b5 FEAT_B: Bubble_Bath_2035 - Correlation: 1.0\n",
      "415: FEAT_A: d06f75b5 FEAT_B: Bubble_Bath_2025 - Correlation: 0.9972383579301791\n",
      "416: FEAT_A: e04fb33d FEAT_B: 7423acbc - Correlation: 0.9996289974081193\n",
      "417: FEAT_A: e04fb33d FEAT_B: Air_Show_3120 - Correlation: 1.0\n",
      "418: FEAT_A: e04fb33d FEAT_B: Air_Show_3020 - Correlation: 0.9996289974081193\n",
      "419: FEAT_A: 6aeafed4 FEAT_B: Bubble_Bath_4090 - Correlation: 1.0\n",
      "420: FEAT_A: 857f21c0 FEAT_B: Bubble_Bath_4040 - Correlation: 1.0\n",
      "421: FEAT_A: 47efca07 FEAT_B: Bottle_Filler__Activity__4090 - Correlation: 1.0\n",
      "422: FEAT_A: de26c3a6 FEAT_B: Flower_Waterer__Activity__4020 - Correlation: 1.0\n",
      "423: FEAT_A: 65abac75 FEAT_B: Air_Show_4010 - Correlation: 1.0\n",
      "424: FEAT_A: 56bcd38d FEAT_B: Chicken_Balancer__Activity__4030 - Correlation: 1.0\n",
      "425: FEAT_A: c58186bf FEAT_B: Sandcastle_Builder__Activity__4035 - Correlation: 1.0\n",
      "426: FEAT_A: c0415e5c FEAT_B: Dino_Dive_4020 - Correlation: 1.0\n",
      "427: FEAT_A: 9e34ea74 FEAT_B: Egg_Dropper__Activity__4070 - Correlation: 1.0\n",
      "428: FEAT_A: 15a43e5b FEAT_B: Bottle_Filler__Activity__4070 - Correlation: 0.9999999999999999\n",
      "429: FEAT_A: 67aa2ada FEAT_B: Leaf_Leader_4090 - Correlation: 1.0\n",
      "430: FEAT_A: fbaf3456 FEAT_B: Mushroom_Sorter__Assessment__4030 - Correlation: 0.9999999999999999\n",
      "431: FEAT_A: 9de5e594 FEAT_B: 28a4eb9a - Correlation: 0.9995923561196821\n",
      "432: FEAT_A: 9de5e594 FEAT_B: Dino_Dive_3020 - Correlation: 1.0\n",
      "433: FEAT_A: 9de5e594 FEAT_B: Dino_Dive_3120 - Correlation: 0.9995923561196821\n",
      "434: FEAT_A: 7ec0c298 FEAT_B: Chow_Time_3010 - Correlation: 0.9999999999999998\n",
      "435: FEAT_A: b2dba42b FEAT_B: 1bb5fbdb - Correlation: 0.9999521729413288\n",
      "436: FEAT_A: b2dba42b FEAT_B: Sandcastle_Builder__Activity__3010 - Correlation: 1.0\n",
      "437: FEAT_A: b2dba42b FEAT_B: Sandcastle_Builder__Activity__3110 - Correlation: 0.9999521729413288\n",
      "438: FEAT_A: 8ac7cce4 FEAT_B: Leaf_Leader_2000 - Correlation: 1.0\n",
      "439: FEAT_A: 6088b756 FEAT_B: Dino_Dive_2070 - Correlation: 0.9999999999999999\n",
      "440: FEAT_A: d2659ab4 FEAT_B: Air_Show_2075 - Correlation: 0.9999999999999998\n",
      "441: FEAT_A: 3d63345e FEAT_B: Cart_Balancer__Assessment__4035 - Correlation: 1.0\n",
      "442: FEAT_A: 16667cc5 FEAT_B: Chicken_Balancer__Activity__4080 - Correlation: 0.9999999999999999\n",
      "443: FEAT_A: eb2c19cd FEAT_B: Mushroom_Sorter__Assessment__4090 - Correlation: 1.0\n",
      "444: FEAT_A: 0d1da71f FEAT_B: Chow_Time_3110 - Correlation: 0.9999999999999998\n",
      "445: FEAT_A: b1d5101d FEAT_B: All_Star_Sorting_4095 - Correlation: 1.0\n",
      "446: FEAT_A: 7040c096 FEAT_B: Scrub_A_Dub_4010 - Correlation: 0.9999999999999999\n",
      "447: FEAT_A: 5a848010 FEAT_B: Scrub_A_Dub_2080 - Correlation: 1.0\n",
      "448: FEAT_A: 87d743c1 FEAT_B: Dino_Dive_4010 - Correlation: 1.0\n",
      "449: FEAT_A: 0ce40006 FEAT_B: Happy_Camel_4080 - Correlation: 1.0\n",
      "450: FEAT_A: 5c3d2b2f FEAT_B: Scrub_A_Dub_4020 - Correlation: 1.0\n",
      "451: FEAT_A: a16a373e FEAT_B: Bird_Measurer__Assessment__4070 - Correlation: 1.0\n",
      "452: FEAT_A: 15ba1109 FEAT_B: Air_Show_2000 - Correlation: 1.0\n",
      "453: FEAT_A: 37937459 FEAT_B: Sandcastle_Builder__Activity__4090 - Correlation: 1.0\n",
      "454: FEAT_A: 25fa8af4 FEAT_B: Mushroom_Sorter__Assessment__4100 - Correlation: 1.0\n",
      "455: FEAT_A: f93fc684 FEAT_B: Chow_Time_4010 - Correlation: 1.0\n",
      "456: FEAT_A: d3640339 FEAT_B: Dino_Dive_4090 - Correlation: 0.9999999999999999\n",
      "457: FEAT_A: 90efca10 FEAT_B: Bottle_Filler__Activity__4020 - Correlation: 1.0\n",
      "458: FEAT_A: 9ed8f6da FEAT_B: Dino_Drink_2075 - Correlation: 1.0\n",
      "459: FEAT_A: 30df3273 FEAT_B: Sandcastle_Builder__Activity__4080 - Correlation: 1.0\n",
      "460: FEAT_A: 56cd3b43 FEAT_B: bbfe0445 - Correlation: 0.9996926215355523\n",
      "461: FEAT_A: 56cd3b43 FEAT_B: Flower_Waterer__Activity__3110 - Correlation: 0.9996926215355523\n",
      "462: FEAT_A: 56cd3b43 FEAT_B: Flower_Waterer__Activity__3010 - Correlation: 1.0\n",
      "463: FEAT_A: 85d1b0de FEAT_B: Chicken_Balancer__Activity__4090 - Correlation: 1.0\n",
      "464: FEAT_A: 461eace6 FEAT_B: Egg_Dropper__Activity__4020 - Correlation: 1.0\n",
      "465: FEAT_A: ab4ec3a4 FEAT_B: Dino_Drink_4080 - Correlation: 1.0\n",
      "466: FEAT_A: c952eb01 FEAT_B: Watering_Hole__Activity__4070 - Correlation: 0.9999999999999999\n",
      "467: FEAT_A: 9b01374f FEAT_B: Flower_Waterer__Activity__2000 - Correlation: 1.0\n",
      "468: FEAT_A: 86c924c4 FEAT_B: Crystals_Rule_4020 - Correlation: 1.0\n",
      "469: FEAT_A: 3393b68b FEAT_B: Bird_Measurer__Assessment__2010 - Correlation: 1.0\n",
      "470: FEAT_A: e4d32835 FEAT_B: Pan_Balance_4080 - Correlation: 0.9999999999999998\n",
      "471: FEAT_A: a2df0760 FEAT_B: Happy_Camel_4035 - Correlation: 0.9999999999999998\n",
      "472: FEAT_A: 76babcde FEAT_B: Dino_Dive_4070 - Correlation: 0.9999999999999998\n",
      "473: FEAT_A: 8af75982 FEAT_B: Happy_Camel_4020 - Correlation: 1.0\n",
      "474: FEAT_A: 4bb2f698 FEAT_B: Chicken_Balancer__Activity__4070 - Correlation: 1.0\n",
      "475: FEAT_A: 5f0eb72c FEAT_B: Mushroom_Sorter__Assessment__4020 - Correlation: 1.0\n",
      "476: FEAT_A: 5e3ea25a FEAT_B: Crystals_Rule_4070 - Correlation: 1.0\n",
      "477: FEAT_A: 3d8c61b0 FEAT_B: Happy_Camel_4030 - Correlation: 1.0\n",
      "478: FEAT_A: 77261ab5 FEAT_B: Sandcastle_Builder__Activity__2000 - Correlation: 0.9999999999999999\n",
      "479: FEAT_A: f32856e4 FEAT_B: Leaf_Leader_2020 - Correlation: 1.0\n",
      "480: FEAT_A: 3b2048ee FEAT_B: Leaf_Leader_4095 - Correlation: 1.0\n",
      "481: FEAT_A: e5734469 FEAT_B: 89aace00 - Correlation: 0.9998406115110352\n",
      "482: FEAT_A: e5734469 FEAT_B: Dino_Drink_3020 - Correlation: 1.0\n",
      "483: FEAT_A: e5734469 FEAT_B: Dino_Drink_3120 - Correlation: 0.9998406115110352\n",
      "484: FEAT_A: 51311d7a FEAT_B: Dino_Drink_2000 - Correlation: 1.0\n",
      "485: FEAT_A: 29a42aea FEAT_B: Bubble_Bath_4080 - Correlation: 1.0\n",
      "486: FEAT_A: c7f7f0e1 FEAT_B: Bug_Measurer__Activity__2000 - Correlation: 0.9999999999999998\n",
      "487: FEAT_A: 0db6d71d FEAT_B: Chest_Sorter__Assessment__4020 - Correlation: 1.0\n",
      "488: FEAT_A: 02a42007 FEAT_B: Fireworks__Activity__4030 - Correlation: 1.0\n",
      "489: FEAT_A: 587b5989 FEAT_B: All_Star_Sorting_4070 - Correlation: 1.0\n",
      "490: FEAT_A: 29bdd9ba FEAT_B: Dino_Dive_2000 - Correlation: 0.9999999999999999\n",
      "491: FEAT_A: e7e44842 FEAT_B: Watering_Hole__Activity__4090 - Correlation: 1.0\n",
      "492: FEAT_A: 4e5fc6f5 FEAT_B: Cart_Balancer__Assessment__4090 - Correlation: 1.0\n",
      "493: FEAT_A: d122731b FEAT_B: Cart_Balancer__Assessment__4100 - Correlation: 1.0\n",
      "494: FEAT_A: bc8f2793 FEAT_B: Pan_Balance_4035 - Correlation: 1.0\n",
      "495: FEAT_A: 2a444e03 FEAT_B: Pan_Balance_4030 - Correlation: 1.0\n",
      "496: FEAT_A: 86ba578b FEAT_B: Leaf_Leader_2070 - Correlation: 1.0\n",
      "497: FEAT_A: 08ff79ad FEAT_B: Egg_Dropper__Activity__4090 - Correlation: 1.0\n",
      "498: FEAT_A: 6d90d394 FEAT_B: Scrub_A_Dub_2000 - Correlation: 1.0\n",
      "499: FEAT_A: 5be391b5 FEAT_B: Dino_Drink_4010 - Correlation: 1.0\n",
      "500: FEAT_A: 9ee1c98c FEAT_B: Sandcastle_Builder__Activity__4021 - Correlation: 1.0\n",
      "501: FEAT_A: 7da34a02 FEAT_B: Mushroom_Sorter__Assessment__4070 - Correlation: 1.0\n",
      "502: FEAT_A: 756e5507 FEAT_B: Chicken_Balancer__Activity__2000 - Correlation: 1.0\n",
      "503: FEAT_A: 7dfe6d8a FEAT_B: Leaf_Leader_4070 - Correlation: 1.0\n",
      "504: FEAT_A: dcb55a27 FEAT_B: Air_Show_4110 - Correlation: 1.0\n",
      "505: FEAT_A: 26a5a3dd FEAT_B: All_Star_Sorting_4080 - Correlation: 1.0\n",
      "506: FEAT_A: bcceccc6 FEAT_B: Air_Show_4070 - Correlation: 1.0\n",
      "507: FEAT_A: 4a4c3d21 FEAT_B: Bird_Measurer__Assessment__4025 - Correlation: 0.9999999999999999\n",
      "508: FEAT_A: acf5c23f FEAT_B: Cart_Balancer__Assessment__4070 - Correlation: 1.0\n",
      "509: FEAT_A: f6947f54 FEAT_B: Bird_Measurer__Assessment__2030 - Correlation: 1.0\n",
      "510: FEAT_A: d51b1749 FEAT_B: Happy_Camel_2080 - Correlation: 1.0\n",
      "511: FEAT_A: 47f43a44 FEAT_B: Flower_Waterer__Activity__4090 - Correlation: 1.0\n",
      "512: FEAT_A: f806dc10 FEAT_B: Dino_Drink_2020 - Correlation: 1.0\n",
      "513: FEAT_A: 93edfe2e FEAT_B: Crystals_Rule_4090 - Correlation: 1.0\n",
      "514: FEAT_A: lgt_Mushroom_Sorter__Assessment_ FEAT_B: agt_Mushroom_Sorter__Assessment_ - Correlation: 0.9961816177498037\n",
      "515: FEAT_A: var_event_id FEAT_B: var_title_event_code - Correlation: 0.9995196248621785\n",
      "516: FEAT_A: accumulated_actions FEAT_B: sum_event_code_count - Correlation: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Go through and find high correlations. Add to remove list if so\n",
    "counter = 0\n",
    "to_remove = []\n",
    "for feat_a in features:\n",
    "    for feat_b in features:\n",
    "        if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n",
    "            c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]\n",
    "            if c > 0.995:\n",
    "                counter += 1\n",
    "                to_remove.append(feat_b)\n",
    "                print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgt_Cauldron_Filler__Assessment_ 0.0006131019707591197 0.014890678565531248 0.01280386624911603\n",
      "lgt_Bird_Measurer__Assessment_ 0.001467792461199117 0.03618532176893661 0.006967017932031494\n",
      "lgt_Mushroom_Sorter__Assessment_ 0.0002720048349456899 0.004590887618656621 0.01326614870499121\n",
      "agt_Mushroom_Sorter__Assessment_ 0.0004883155304924486 0.007520193269563968 0.008830469115515913\n"
     ]
    }
   ],
   "source": [
    "# Go through again a look for features to remove\n",
    "to_exclude = [] \n",
    "ajusted_test = reduce_test.copy()\n",
    "for feature in ajusted_test.columns:\n",
    "    if feature not in (cols_to_drop + categoricals):\n",
    "        try:\n",
    "            data = reduce_train[feature]\n",
    "            train_mean = data.mean()\n",
    "            data = ajusted_test[feature] \n",
    "            test_mean = data.mean()\n",
    "            error = stract_hists(feature, train=reduce_train, test=reduce_test, adjust=True)\n",
    "            ajust_factor = train_mean / test_mean\n",
    "            if ajust_factor > 10 or ajust_factor < 0.1:# or error > 0.01:\n",
    "                to_exclude.append(feature)\n",
    "                print(feature, train_mean, test_mean, error)\n",
    "            else:\n",
    "                ajusted_test[feature] *= ajust_factor\n",
    "        except:\n",
    "            to_exclude.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final feature list removing the unwanted ones\n",
    "features = [x for x in features if x not in (to_exclude + to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial score of fold 0 is: 0.5573580366562838\n",
      "Partial score of fold 1 is: 0.5455838806449469\n",
      "Partial score of fold 2 is: 0.5345198315160676\n",
      "Partial score of fold 3 is: 0.5583900879769266\n",
      "Partial score of fold 4 is: 0.5413472117459542\n",
      "Our oof cohen kappa score is:  0.5474315123578697\n"
     ]
    }
   ],
   "source": [
    "# Random Forrest Classifier\n",
    "\n",
    "params = {'bootstrap': False, \n",
    "          'max_depth':59, \n",
    "          'max_features': 72, \n",
    "          'min_samples_leaf': 2, \n",
    "          'min_samples_split': 6, \n",
    "          'n_estimators': 100}\n",
    "\n",
    "rf_model = RF_Model(reduce_train, ajusted_test, features, params, categoricals=categoricals)\n",
    "# 0.5523468488732437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data:  0.5474315123578697\n"
     ]
    }
   ],
   "source": [
    "rf_train_pred = rf_model.oof_pred\n",
    "print('Accuracy on training data: ', qwk3(reduce_train['accuracy_group'], rf_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial score of fold 0 is: 0.47426155198022635\n",
      "Partial score of fold 1 is: 0.49343867565590827\n",
      "Partial score of fold 2 is: 0.46496425799374264\n",
      "Partial score of fold 3 is: 0.4763552841041663\n",
      "Partial score of fold 4 is: 0.48148629329824355\n",
      "Our oof cohen kappa score is:  0.4781292818058771\n"
     ]
    }
   ],
   "source": [
    "# K Nearest\n",
    "\n",
    "weights = 0.3491139618762451\n",
    "if weights >= 0 and weights < 1.0:\n",
    "    weights = 'uniform'\n",
    "else:\n",
    "    weights = 'distance'\n",
    "\n",
    "algorithm = 0.04441288498288465\n",
    "if algorithm >= 0 and algorithm < 1.0:\n",
    "    algorithm = 'ball_tree'\n",
    "elif algorithm >= 1 and algorithm < 2.0:\n",
    "    algorithm = 'kd_tree'\n",
    "elif algorithm >= 2 and algorithm < 3.0:\n",
    "    algorithm = 'brute'\n",
    "else:\n",
    "    algorithm = 'auto'\n",
    "\n",
    "params = {\n",
    "         'n_neighbors': int(19.544302888065488),\n",
    "        'weights': weights,\n",
    "        'algorithm': algorithm,\n",
    "        'leaf_size': int(29.702070879545722),\n",
    "        'p': int(2.986361352754792),\n",
    "        'n_jobs': -1\n",
    "}\n",
    "knn_model = KNN_Model(reduce_train, ajusted_test, features, params, categoricals=categoricals)\n",
    "# 0.4942484678835759"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data:  0.4781292818058771\n"
     ]
    }
   ],
   "source": [
    "knn_train_pred = knn_model.oof_pred\n",
    "print('Accuracy on training data: ', qwk3(reduce_train['accuracy_group'], knn_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:1.85917\tval-rmse:1.85916\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:1.19929\tval-rmse:1.22708\n",
      "[200]\ttrain-rmse:1.01839\tval-rmse:1.07527\n",
      "[300]\ttrain-rmse:0.948968\tval-rmse:1.03067\n",
      "[400]\ttrain-rmse:0.903107\tval-rmse:1.00609\n",
      "[500]\ttrain-rmse:0.868435\tval-rmse:0.991004\n",
      "[600]\ttrain-rmse:0.842172\tval-rmse:0.982262\n",
      "[666]\ttrain-rmse:0.827314\tval-rmse:0.978281\n",
      "Partial score of fold 0 is: 0.46595104187923664\n",
      "[0]\ttrain-rmse:1.85914\tval-rmse:1.85946\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:1.1988\tval-rmse:1.2319\n",
      "[200]\ttrain-rmse:1.01703\tval-rmse:1.07849\n",
      "[300]\ttrain-rmse:0.946626\tval-rmse:1.03248\n",
      "[400]\ttrain-rmse:0.899773\tval-rmse:1.0091\n",
      "[500]\ttrain-rmse:0.865111\tval-rmse:0.995583\n",
      "[600]\ttrain-rmse:0.838626\tval-rmse:0.988273\n",
      "[666]\ttrain-rmse:0.823719\tval-rmse:0.984801\n",
      "Partial score of fold 1 is: 0.4520240461333148\n",
      "[0]\ttrain-rmse:1.85916\tval-rmse:1.85928\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:1.20004\tval-rmse:1.22512\n",
      "[200]\ttrain-rmse:1.01897\tval-rmse:1.07161\n",
      "[300]\ttrain-rmse:0.948901\tval-rmse:1.02662\n",
      "[400]\ttrain-rmse:0.90274\tval-rmse:1.00292\n",
      "[500]\ttrain-rmse:0.867958\tval-rmse:0.989295\n",
      "[600]\ttrain-rmse:0.84192\tval-rmse:0.981728\n",
      "[666]\ttrain-rmse:0.827748\tval-rmse:0.978402\n",
      "Partial score of fold 2 is: 0.45398089425502686\n",
      "[0]\ttrain-rmse:1.85909\tval-rmse:1.85931\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:1.19896\tval-rmse:1.22531\n",
      "[200]\ttrain-rmse:1.01782\tval-rmse:1.07494\n",
      "[300]\ttrain-rmse:0.947268\tval-rmse:1.03225\n",
      "[400]\ttrain-rmse:0.900466\tval-rmse:1.00981\n",
      "[500]\ttrain-rmse:0.865825\tval-rmse:0.996489\n",
      "[600]\ttrain-rmse:0.840229\tval-rmse:0.988385\n",
      "[666]\ttrain-rmse:0.826191\tval-rmse:0.98473\n",
      "Partial score of fold 3 is: 0.46040113341747413\n",
      "[0]\ttrain-rmse:1.85905\tval-rmse:1.85945\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:1.19853\tval-rmse:1.22635\n",
      "[200]\ttrain-rmse:1.017\tval-rmse:1.076\n",
      "[300]\ttrain-rmse:0.946736\tval-rmse:1.03323\n",
      "[400]\ttrain-rmse:0.900967\tval-rmse:1.01079\n",
      "[500]\ttrain-rmse:0.866967\tval-rmse:0.997498\n",
      "[600]\ttrain-rmse:0.841788\tval-rmse:0.990359\n",
      "[666]\ttrain-rmse:0.827395\tval-rmse:0.986904\n",
      "Partial score of fold 4 is: 0.4474387177018353\n",
      "Our oof cohen kappa score is:  0.45593095935157313\n"
     ]
    }
   ],
   "source": [
    "# XG Boost\n",
    "params = {\n",
    "            'colsample_bytree': 0.2,                 \n",
    "            'learning_rate': 0.01,\n",
    "            'objective':'reg:squarederror',\n",
    "            'max_depth': 6,\n",
    "            'subsample': 1,\n",
    "            'min_child_weight': 3,\n",
    "            'gamma': 0.25,\n",
    "            'n_estimators': 1400\n",
    "         }\n",
    "\n",
    "xgb_model = Xgb_Model(reduce_train, ajusted_test, features, params, categoricals=categoricals)\n",
    "# 0.6098126526596694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data:  0.45593095935157313\n"
     ]
    }
   ],
   "source": [
    "xgb_train_pred = xgb_model.oof_pred\n",
    "print('Accuracy on training data: ', qwk3(reduce_train['accuracy_group'], xgb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 2.1992176\ttest: 2.1986935\tbest: 2.1986935 (0)\ttotal: 103ms\tremaining: 3m 11s\n",
      "100:\tlearn: 1.0075605\ttest: 1.0155215\tbest: 1.0155215 (100)\ttotal: 4.42s\tremaining: 1m 17s\n",
      "200:\tlearn: 0.9735774\ttest: 0.9946583\tbest: 0.9946583 (200)\ttotal: 8.8s\tremaining: 1m 12s\n",
      "300:\tlearn: 0.9482295\ttest: 0.9834390\tbest: 0.9834390 (300)\ttotal: 13.2s\tremaining: 1m 8s\n",
      "400:\tlearn: 0.9258153\ttest: 0.9775811\tbest: 0.9775811 (400)\ttotal: 17.5s\tremaining: 1m 3s\n",
      "500:\tlearn: 0.9077978\ttest: 0.9741636\tbest: 0.9741229 (499)\ttotal: 21.9s\tremaining: 59.5s\n",
      "600:\tlearn: 0.8923743\ttest: 0.9724816\tbest: 0.9722439 (587)\ttotal: 26.2s\tremaining: 55s\n",
      "700:\tlearn: 0.8776046\ttest: 0.9709678\tbest: 0.9708560 (680)\ttotal: 30.6s\tremaining: 50.6s\n",
      "800:\tlearn: 0.8645137\ttest: 0.9705376\tbest: 0.9705376 (800)\ttotal: 35s\tremaining: 46.2s\n",
      "900:\tlearn: 0.8518941\ttest: 0.9698601\tbest: 0.9696706 (878)\ttotal: 39.4s\tremaining: 41.9s\n",
      "1000:\tlearn: 0.8391667\ttest: 0.9699349\tbest: 0.9695947 (910)\ttotal: 43.8s\tremaining: 37.6s\n",
      "1100:\tlearn: 0.8275327\ttest: 0.9699507\tbest: 0.9695947 (910)\ttotal: 48.2s\tremaining: 33.2s\n",
      "1200:\tlearn: 0.8160875\ttest: 0.9693084\tbest: 0.9692946 (1195)\ttotal: 52.6s\tremaining: 28.9s\n",
      "1300:\tlearn: 0.8055712\ttest: 0.9693020\tbest: 0.9691602 (1229)\ttotal: 57s\tremaining: 24.5s\n",
      "1400:\tlearn: 0.7952872\ttest: 0.9691126\tbest: 0.9689408 (1332)\ttotal: 1m 1s\tremaining: 20.1s\n",
      "1500:\tlearn: 0.7857751\ttest: 0.9689535\tbest: 0.9689174 (1486)\ttotal: 1m 5s\tremaining: 15.8s\n",
      "1600:\tlearn: 0.7759907\ttest: 0.9694455\tbest: 0.9689174 (1486)\ttotal: 1m 10s\tremaining: 11.4s\n",
      "1700:\tlearn: 0.7672007\ttest: 0.9696382\tbest: 0.9689174 (1486)\ttotal: 1m 14s\tremaining: 6.97s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.9689173507\n",
      "bestIteration = 1486\n",
      "\n",
      "Shrink model to first 1487 iterations.\n",
      "Partial score of fold 0 is: 0.48906486142575023\n",
      "0:\tlearn: 2.1992474\ttest: 2.1992675\tbest: 2.1992675 (0)\ttotal: 44ms\tremaining: 1m 21s\n",
      "100:\tlearn: 1.0061325\ttest: 1.0147108\tbest: 1.0147108 (100)\ttotal: 4.33s\tremaining: 1m 15s\n",
      "200:\tlearn: 0.9691420\ttest: 0.9951897\tbest: 0.9951897 (200)\ttotal: 8.71s\tremaining: 1m 11s\n",
      "300:\tlearn: 0.9434209\ttest: 0.9883492\tbest: 0.9883492 (300)\ttotal: 13.1s\tremaining: 1m 7s\n",
      "400:\tlearn: 0.9219782\ttest: 0.9850652\tbest: 0.9850652 (400)\ttotal: 17.4s\tremaining: 1m 3s\n",
      "500:\tlearn: 0.9043579\ttest: 0.9840435\tbest: 0.9837515 (473)\ttotal: 21.8s\tremaining: 59.1s\n",
      "600:\tlearn: 0.8881861\ttest: 0.9824755\tbest: 0.9824595 (594)\ttotal: 26.1s\tremaining: 54.8s\n",
      "700:\tlearn: 0.8736035\ttest: 0.9822226\tbest: 0.9822226 (700)\ttotal: 30.6s\tremaining: 50.6s\n",
      "800:\tlearn: 0.8601899\ttest: 0.9819781\tbest: 0.9816990 (736)\ttotal: 35.3s\tremaining: 46.7s\n",
      "900:\tlearn: 0.8473426\ttest: 0.9818118\tbest: 0.9815727 (880)\ttotal: 40s\tremaining: 42.5s\n",
      "1000:\tlearn: 0.8353560\ttest: 0.9817673\tbest: 0.9815727 (880)\ttotal: 44.3s\tremaining: 38s\n",
      "1100:\tlearn: 0.8237935\ttest: 0.9817986\tbest: 0.9815727 (880)\ttotal: 48.6s\tremaining: 33.5s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.9815727265\n",
      "bestIteration = 880\n",
      "\n",
      "Shrink model to first 881 iterations.\n",
      "Partial score of fold 1 is: 0.4692643090533414\n",
      "0:\tlearn: 2.1989060\ttest: 2.1991390\tbest: 2.1991390 (0)\ttotal: 41.6ms\tremaining: 1m 17s\n",
      "100:\tlearn: 1.0071295\ttest: 1.0249488\tbest: 1.0249488 (100)\ttotal: 4.36s\tremaining: 1m 15s\n",
      "200:\tlearn: 0.9720291\ttest: 1.0057458\tbest: 1.0057458 (200)\ttotal: 8.79s\tremaining: 1m 12s\n",
      "300:\tlearn: 0.9473236\ttest: 0.9971502\tbest: 0.9971502 (300)\ttotal: 13.1s\tremaining: 1m 8s\n",
      "400:\tlearn: 0.9264397\ttest: 0.9913750\tbest: 0.9913750 (400)\ttotal: 17.5s\tremaining: 1m 3s\n",
      "500:\tlearn: 0.9080182\ttest: 0.9878263\tbest: 0.9878205 (499)\ttotal: 21.9s\tremaining: 59.3s\n",
      "600:\tlearn: 0.8927120\ttest: 0.9862513\tbest: 0.9861504 (596)\ttotal: 26.2s\tremaining: 54.8s\n",
      "700:\tlearn: 0.8774693\ttest: 0.9842228\tbest: 0.9842127 (687)\ttotal: 30.5s\tremaining: 50.5s\n",
      "800:\tlearn: 0.8645319\ttest: 0.9835201\tbest: 0.9835201 (800)\ttotal: 34.9s\tremaining: 46.1s\n",
      "900:\tlearn: 0.8517683\ttest: 0.9828299\tbest: 0.9825272 (859)\ttotal: 39.2s\tremaining: 41.8s\n",
      "1000:\tlearn: 0.8394156\ttest: 0.9827441\tbest: 0.9825272 (859)\ttotal: 43.6s\tremaining: 37.4s\n",
      "1100:\tlearn: 0.8279942\ttest: 0.9820629\tbest: 0.9819670 (1098)\ttotal: 47.9s\tremaining: 33.1s\n",
      "1200:\tlearn: 0.8169558\ttest: 0.9818568\tbest: 0.9815214 (1168)\ttotal: 52.3s\tremaining: 28.7s\n",
      "1300:\tlearn: 0.8063173\ttest: 0.9813352\tbest: 0.9812619 (1292)\ttotal: 56.7s\tremaining: 24.3s\n",
      "1400:\tlearn: 0.7964621\ttest: 0.9811702\tbest: 0.9810313 (1374)\ttotal: 1m 1s\tremaining: 20s\n",
      "1500:\tlearn: 0.7861898\ttest: 0.9805842\tbest: 0.9805842 (1500)\ttotal: 1m 5s\tremaining: 15.6s\n",
      "1600:\tlearn: 0.7764142\ttest: 0.9809157\tbest: 0.9805842 (1500)\ttotal: 1m 9s\tremaining: 11.3s\n",
      "1700:\tlearn: 0.7667956\ttest: 0.9810952\tbest: 0.9805842 (1500)\ttotal: 1m 14s\tremaining: 6.93s\n",
      "1800:\tlearn: 0.7579521\ttest: 0.9811979\tbest: 0.9805842 (1500)\ttotal: 1m 18s\tremaining: 2.57s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.980584151\n",
      "bestIteration = 1500\n",
      "\n",
      "Shrink model to first 1501 iterations.\n",
      "Partial score of fold 2 is: 0.4810844420684023\n",
      "0:\tlearn: 2.1992735\ttest: 2.1990304\tbest: 2.1990304 (0)\ttotal: 52.7ms\tremaining: 1m 37s\n",
      "100:\tlearn: 1.0071997\ttest: 1.0200655\tbest: 1.0200655 (100)\ttotal: 4.39s\tremaining: 1m 16s\n",
      "200:\tlearn: 0.9710672\ttest: 1.0021687\tbest: 1.0021687 (200)\ttotal: 8.83s\tremaining: 1m 12s\n",
      "300:\tlearn: 0.9452514\ttest: 0.9942782\tbest: 0.9942683 (296)\ttotal: 13.2s\tremaining: 1m 8s\n",
      "400:\tlearn: 0.9227075\ttest: 0.9898754\tbest: 0.9898028 (399)\ttotal: 17.6s\tremaining: 1m 4s\n",
      "500:\tlearn: 0.9050645\ttest: 0.9869482\tbest: 0.9869482 (500)\ttotal: 21.9s\tremaining: 59.5s\n",
      "600:\tlearn: 0.8886923\ttest: 0.9849277\tbest: 0.9849277 (600)\ttotal: 26.3s\tremaining: 55.1s\n",
      "700:\tlearn: 0.8743529\ttest: 0.9838205\tbest: 0.9838205 (700)\ttotal: 30.6s\tremaining: 50.7s\n",
      "800:\tlearn: 0.8604899\ttest: 0.9827161\tbest: 0.9826702 (799)\ttotal: 35s\tremaining: 46.3s\n",
      "900:\tlearn: 0.8475915\ttest: 0.9815599\tbest: 0.9815599 (900)\ttotal: 39.4s\tremaining: 41.9s\n",
      "1000:\tlearn: 0.8365132\ttest: 0.9810841\tbest: 0.9809939 (968)\ttotal: 43.7s\tremaining: 37.5s\n",
      "1100:\tlearn: 0.8249410\ttest: 0.9807253\tbest: 0.9807174 (1099)\ttotal: 48.1s\tremaining: 33.1s\n",
      "1200:\tlearn: 0.8136383\ttest: 0.9802324\tbest: 0.9802324 (1200)\ttotal: 52.5s\tremaining: 28.8s\n",
      "1300:\tlearn: 0.8037809\ttest: 0.9804205\tbest: 0.9801397 (1207)\ttotal: 56.8s\tremaining: 24.4s\n",
      "1400:\tlearn: 0.7938920\ttest: 0.9805682\tbest: 0.9801397 (1207)\ttotal: 1m 1s\tremaining: 20s\n",
      "1500:\tlearn: 0.7836272\ttest: 0.9811846\tbest: 0.9801397 (1207)\ttotal: 1m 5s\tremaining: 15.7s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.9801397437\n",
      "bestIteration = 1207\n",
      "\n",
      "Shrink model to first 1208 iterations.\n",
      "Partial score of fold 3 is: 0.4811688995207467\n",
      "0:\tlearn: 2.1990815\ttest: 2.1994323\tbest: 2.1994323 (0)\ttotal: 37.5ms\tremaining: 1m 9s\n",
      "100:\tlearn: 1.0045946\ttest: 1.0253115\tbest: 1.0253115 (100)\ttotal: 4.42s\tremaining: 1m 16s\n",
      "200:\tlearn: 0.9717796\ttest: 1.0064150\tbest: 1.0064150 (200)\ttotal: 8.79s\tremaining: 1m 12s\n",
      "300:\tlearn: 0.9465188\ttest: 0.9958988\tbest: 0.9958988 (300)\ttotal: 13.2s\tremaining: 1m 8s\n",
      "400:\tlearn: 0.9255192\ttest: 0.9903810\tbest: 0.9903689 (399)\ttotal: 17.5s\tremaining: 1m 3s\n",
      "500:\tlearn: 0.9082830\ttest: 0.9873226\tbest: 0.9873226 (500)\ttotal: 21.8s\tremaining: 59.2s\n",
      "600:\tlearn: 0.8918991\ttest: 0.9849499\tbest: 0.9849499 (600)\ttotal: 26.2s\tremaining: 55s\n",
      "700:\tlearn: 0.8773552\ttest: 0.9833374\tbest: 0.9833374 (700)\ttotal: 30.6s\tremaining: 50.6s\n",
      "800:\tlearn: 0.8635637\ttest: 0.9832971\tbest: 0.9831237 (777)\ttotal: 35s\tremaining: 46.2s\n",
      "900:\tlearn: 0.8509533\ttest: 0.9828739\tbest: 0.9827501 (897)\ttotal: 39.3s\tremaining: 41.9s\n",
      "1000:\tlearn: 0.8391673\ttest: 0.9826914\tbest: 0.9824956 (983)\ttotal: 43.8s\tremaining: 37.6s\n",
      "1100:\tlearn: 0.8282686\ttest: 0.9829091\tbest: 0.9824956 (983)\ttotal: 48.2s\tremaining: 33.2s\n",
      "1200:\tlearn: 0.8171407\ttest: 0.9826621\tbest: 0.9824956 (983)\ttotal: 52.5s\tremaining: 28.8s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.9824955678\n",
      "bestIteration = 983\n",
      "\n",
      "Shrink model to first 984 iterations.\n",
      "Partial score of fold 4 is: 0.4739557658119341\n",
      "Our oof cohen kappa score is:  0.47886933893948913\n"
     ]
    }
   ],
   "source": [
    "# Catboost\n",
    "params = {\n",
    "            'loss_function': 'MultiRMSE',\n",
    "            'task_type': \"CPU\",\n",
    "            'iterations': 1860,\n",
    "            'depth': 6,\n",
    "            'early_stopping_rounds': 300,\n",
    "            'l2_leaf_reg': 2,\n",
    "            'rsm': 1,\n",
    "            'bootstrap_type': 'Bayesian',\n",
    "            'bagging_temperature': 1,\n",
    "            'random_seed': 42,\n",
    "            'learning_rate': 0.04\n",
    "        }\n",
    "\n",
    "cat_model = Catb_Model(reduce_train, ajusted_test, features, params, categoricals=categoricals)\n",
    "# 0.6091561423979555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data:  0.47886933893948913\n"
     ]
    }
   ],
   "source": [
    "cat_train_pred = cat_model.oof_pred\n",
    "print('Accuracy on training data: ', qwk3(reduce_train['accuracy_group'], cat_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 391)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 391)]             0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 50, 391)           0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 50, 391, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 1, 7, 18)          45018     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 126)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               12700     \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_1 (Layer (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 63,119\n",
      "Trainable params: 63,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14151 samples, validate on 3539 samples\n",
      "Epoch 1/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 1.8507\n",
      "Epoch 00001: val_loss improved from inf to 1.07923, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 7s 464us/sample - loss: 1.8480 - val_loss: 1.0792\n",
      "Epoch 2/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.3926\n",
      "Epoch 00002: val_loss improved from 1.07923 to 1.04780, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 5s 375us/sample - loss: 1.3944 - val_loss: 1.0478\n",
      "Epoch 3/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.2870\n",
      "Epoch 00003: val_loss improved from 1.04780 to 1.03753, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 5s 377us/sample - loss: 1.2870 - val_loss: 1.0375\n",
      "Epoch 4/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 1.2316\n",
      "Epoch 00004: val_loss improved from 1.03753 to 1.01916, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 5s 384us/sample - loss: 1.2319 - val_loss: 1.0192\n",
      "Epoch 5/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 1.2086\n",
      "Epoch 00005: val_loss did not improve from 1.01916\n",
      "14151/14151 [==============================] - 5s 384us/sample - loss: 1.2090 - val_loss: 1.0430\n",
      "Epoch 6/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.1844\n",
      "Epoch 00006: val_loss did not improve from 1.01916\n",
      "14151/14151 [==============================] - 6s 391us/sample - loss: 1.1847 - val_loss: 1.0313\n",
      "Epoch 7/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.1618\n",
      "Epoch 00007: val_loss improved from 1.01916 to 1.00162, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 6s 411us/sample - loss: 1.1626 - val_loss: 1.0016\n",
      "Epoch 8/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.1286\n",
      "Epoch 00008: val_loss did not improve from 1.00162\n",
      "14151/14151 [==============================] - 5s 382us/sample - loss: 1.1287 - val_loss: 1.0051\n",
      "Epoch 9/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 1.1133\n",
      "Epoch 00009: val_loss did not improve from 1.00162\n",
      "14151/14151 [==============================] - 5s 377us/sample - loss: 1.1158 - val_loss: 1.0050\n",
      "Epoch 10/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.1029\n",
      "Epoch 00010: val_loss did not improve from 1.00162\n",
      "14151/14151 [==============================] - 5s 384us/sample - loss: 1.1018 - val_loss: 1.0062\n",
      "Epoch 11/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.0842\n",
      "Epoch 00011: val_loss improved from 1.00162 to 0.99109, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 5s 376us/sample - loss: 1.0838 - val_loss: 0.9911\n",
      "Epoch 12/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 1.0600\n",
      "Epoch 00012: val_loss did not improve from 0.99109\n",
      "14151/14151 [==============================] - 5s 385us/sample - loss: 1.0596 - val_loss: 1.0016\n",
      "Epoch 13/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 1.0559\n",
      "Epoch 00013: val_loss improved from 0.99109 to 0.98256, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 5s 377us/sample - loss: 1.0557 - val_loss: 0.9826\n",
      "Epoch 14/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.0522\n",
      "Epoch 00014: val_loss did not improve from 0.98256\n",
      "14151/14151 [==============================] - 5s 375us/sample - loss: 1.0517 - val_loss: 1.0158\n",
      "Epoch 15/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.0396\n",
      "Epoch 00015: val_loss improved from 0.98256 to 0.98104, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 6s 430us/sample - loss: 1.0409 - val_loss: 0.9810\n",
      "Epoch 16/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.0277\n",
      "Epoch 00016: val_loss did not improve from 0.98104\n",
      "14151/14151 [==============================] - 6s 413us/sample - loss: 1.0265 - val_loss: 0.9819\n",
      "Epoch 17/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 1.0142\n",
      "Epoch 00017: val_loss did not improve from 0.98104\n",
      "14151/14151 [==============================] - 5s 379us/sample - loss: 1.0134 - val_loss: 0.9902\n",
      "Epoch 18/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.0051\n",
      "Epoch 00018: val_loss did not improve from 0.98104\n",
      "14151/14151 [==============================] - 5s 369us/sample - loss: 1.0062 - val_loss: 0.9941\n",
      "Epoch 19/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.9890\n",
      "Epoch 00019: val_loss did not improve from 0.98104\n",
      "14151/14151 [==============================] - 5s 377us/sample - loss: 0.9886 - val_loss: 0.9827\n",
      "Epoch 20/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.9872\n",
      "Epoch 00020: val_loss improved from 0.98104 to 0.97703, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 5s 370us/sample - loss: 0.9876 - val_loss: 0.9770\n",
      "Epoch 21/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.9655\n",
      "Epoch 00021: val_loss did not improve from 0.97703\n",
      "14151/14151 [==============================] - 5s 376us/sample - loss: 0.9652 - val_loss: 0.9858\n",
      "Epoch 22/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.9620\n",
      "Epoch 00022: val_loss did not improve from 0.97703\n",
      "14151/14151 [==============================] - 6s 403us/sample - loss: 0.9627 - val_loss: 0.9947\n",
      "Epoch 23/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.9513\n",
      "Epoch 00023: val_loss did not improve from 0.97703\n",
      "14151/14151 [==============================] - 5s 374us/sample - loss: 0.9521 - val_loss: 0.9936\n",
      "Epoch 24/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 0.9427\n",
      "Epoch 00024: val_loss did not improve from 0.97703\n",
      "14151/14151 [==============================] - 5s 371us/sample - loss: 0.9434 - val_loss: 0.9835\n",
      "Epoch 25/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.9295\n",
      "Epoch 00025: val_loss improved from 0.97703 to 0.97359, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 5s 372us/sample - loss: 0.9293 - val_loss: 0.9736\n",
      "Epoch 26/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 0.9286\n",
      "Epoch 00026: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 368us/sample - loss: 0.9284 - val_loss: 0.9856\n",
      "Epoch 27/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.9115\n",
      "Epoch 00027: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 370us/sample - loss: 0.9112 - val_loss: 0.9904\n",
      "Epoch 28/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.9026\n",
      "Epoch 00028: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 371us/sample - loss: 0.9029 - val_loss: 1.0048\n",
      "Epoch 29/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.8984\n",
      "Epoch 00029: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 367us/sample - loss: 0.8975 - val_loss: 0.9978\n",
      "Epoch 30/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.8845\n",
      "Epoch 00030: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 370us/sample - loss: 0.8834 - val_loss: 1.0013\n",
      "Epoch 31/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.8792\n",
      "Epoch 00031: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 371us/sample - loss: 0.8796 - val_loss: 1.0023\n",
      "Epoch 32/100\n",
      "13984/14151 [============================>.] - ETA: 0s - loss: 0.8718\n",
      "Epoch 00032: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 376us/sample - loss: 0.8724 - val_loss: 0.9966\n",
      "Epoch 33/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.8685\n",
      "Epoch 00033: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 375us/sample - loss: 0.8690 - val_loss: 1.0045\n",
      "Epoch 34/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 0.8655\n",
      "Epoch 00034: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 374us/sample - loss: 0.8650 - val_loss: 1.0082\n",
      "Epoch 35/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.8540\n",
      "Epoch 00035: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 369us/sample - loss: 0.8536 - val_loss: 1.0203\n",
      "Epoch 36/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.8409\n",
      "Epoch 00036: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 368us/sample - loss: 0.8418 - val_loss: 1.0084\n",
      "Epoch 37/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 0.8342\n",
      "Epoch 00037: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 367us/sample - loss: 0.8334 - val_loss: 1.0130\n",
      "Epoch 38/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.8229\n",
      "Epoch 00038: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 371us/sample - loss: 0.8230 - val_loss: 1.0283\n",
      "Epoch 39/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.8178\n",
      "Epoch 00039: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 368us/sample - loss: 0.8168 - val_loss: 1.0247\n",
      "Epoch 40/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 0.8154\n",
      "Epoch 00040: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 367us/sample - loss: 0.8159 - val_loss: 1.0237\n",
      "Epoch 41/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.8090\n",
      "Epoch 00041: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 373us/sample - loss: 0.8087 - val_loss: 1.0233\n",
      "Epoch 42/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 0.8004\n",
      "Epoch 00042: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 372us/sample - loss: 0.7994 - val_loss: 1.0501\n",
      "Epoch 43/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.7857\n",
      "Epoch 00043: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 370us/sample - loss: 0.7873 - val_loss: 1.0412\n",
      "Epoch 44/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.7784\n",
      "Epoch 00044: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 375us/sample - loss: 0.7809 - val_loss: 1.0669\n",
      "Epoch 45/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.7742\n",
      "Epoch 00045: val_loss did not improve from 0.97359\n",
      "14151/14151 [==============================] - 5s 378us/sample - loss: 0.7743 - val_loss: 1.0601\n",
      "Partial score of fold 0 is: [0.48341756]\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 391)]             0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 50, 391)           0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 50, 391, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 1, 7, 18)          45018     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 126)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               12700     \n",
      "_________________________________________________________________\n",
      "layer_normalization_2 (Layer (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_3 (Layer (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 63,119\n",
      "Trainable params: 63,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.8581\n",
      "Epoch 00001: val_loss improved from inf to 1.09267, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 6s 437us/sample - loss: 1.8544 - val_loss: 1.0927\n",
      "Epoch 2/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.4138\n",
      "Epoch 00002: val_loss improved from 1.09267 to 1.03891, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 371us/sample - loss: 1.4119 - val_loss: 1.0389\n",
      "Epoch 3/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.3023\n",
      "Epoch 00003: val_loss improved from 1.03891 to 1.03263, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 369us/sample - loss: 1.3009 - val_loss: 1.0326\n",
      "Epoch 4/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.2562\n",
      "Epoch 00004: val_loss improved from 1.03263 to 1.02586, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 373us/sample - loss: 1.2574 - val_loss: 1.0259\n",
      "Epoch 5/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.2143\n",
      "Epoch 00005: val_loss did not improve from 1.02586\n",
      "14152/14152 [==============================] - 5s 388us/sample - loss: 1.2142 - val_loss: 1.0342\n",
      "Epoch 6/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1897\n",
      "Epoch 00006: val_loss improved from 1.02586 to 1.00445, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 372us/sample - loss: 1.1898 - val_loss: 1.0045\n",
      "Epoch 7/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.1603\n",
      "Epoch 00007: val_loss did not improve from 1.00445\n",
      "14152/14152 [==============================] - 5s 369us/sample - loss: 1.1608 - val_loss: 1.0210\n",
      "Epoch 8/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1367\n",
      "Epoch 00008: val_loss did not improve from 1.00445\n",
      "14152/14152 [==============================] - 5s 373us/sample - loss: 1.1365 - val_loss: 1.0168\n",
      "Epoch 9/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1279\n",
      "Epoch 00009: val_loss did not improve from 1.00445\n",
      "14152/14152 [==============================] - 5s 367us/sample - loss: 1.1284 - val_loss: 1.0264\n",
      "Epoch 10/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1032\n",
      "Epoch 00010: val_loss improved from 1.00445 to 1.00124, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 371us/sample - loss: 1.1022 - val_loss: 1.0012\n",
      "Epoch 11/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0821\n",
      "Epoch 00011: val_loss improved from 1.00124 to 0.99483, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 370us/sample - loss: 1.0834 - val_loss: 0.9948\n",
      "Epoch 12/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0733\n",
      "Epoch 00012: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 5s 370us/sample - loss: 1.0704 - val_loss: 1.0092\n",
      "Epoch 13/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0701\n",
      "Epoch 00013: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 5s 370us/sample - loss: 1.0710 - val_loss: 1.0056\n",
      "Epoch 14/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0508\n",
      "Epoch 00014: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 5s 371us/sample - loss: 1.0519 - val_loss: 1.0128\n",
      "Epoch 15/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0436\n",
      "Epoch 00015: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 5s 371us/sample - loss: 1.0425 - val_loss: 1.0016\n",
      "Epoch 16/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0254\n",
      "Epoch 00016: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 5s 372us/sample - loss: 1.0254 - val_loss: 1.0056\n",
      "Epoch 17/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0133\n",
      "Epoch 00017: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 7s 518us/sample - loss: 1.0142 - val_loss: 1.0003\n",
      "Epoch 18/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0074\n",
      "Epoch 00018: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 6s 406us/sample - loss: 1.0054 - val_loss: 1.0183\n",
      "Epoch 19/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9972\n",
      "Epoch 00019: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 6s 395us/sample - loss: 0.9964 - val_loss: 1.0020\n",
      "Epoch 20/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9828\n",
      "Epoch 00020: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 5s 382us/sample - loss: 0.9816 - val_loss: 1.0004\n",
      "Epoch 21/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9636\n",
      "Epoch 00021: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 6s 425us/sample - loss: 0.9637 - val_loss: 0.9985\n",
      "Epoch 22/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9630\n",
      "Epoch 00022: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 5s 388us/sample - loss: 0.9632 - val_loss: 1.0072\n",
      "Epoch 23/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9562\n",
      "Epoch 00023: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 5s 379us/sample - loss: 0.9561 - val_loss: 1.0042\n",
      "Epoch 24/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9377\n",
      "Epoch 00024: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 5s 379us/sample - loss: 0.9379 - val_loss: 1.0186\n",
      "Epoch 25/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9441\n",
      "Epoch 00025: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 5s 383us/sample - loss: 0.9441 - val_loss: 1.0012\n",
      "Epoch 26/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9227\n",
      "Epoch 00026: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 6s 409us/sample - loss: 0.9235 - val_loss: 1.0190\n",
      "Epoch 27/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9264\n",
      "Epoch 00027: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 6s 420us/sample - loss: 0.9275 - val_loss: 1.0389\n",
      "Epoch 28/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9192\n",
      "Epoch 00028: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 6s 438us/sample - loss: 0.9188 - val_loss: 1.0069\n",
      "Epoch 29/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.8986\n",
      "Epoch 00029: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 6s 403us/sample - loss: 0.8988 - val_loss: 1.0200\n",
      "Epoch 30/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.8953\n",
      "Epoch 00030: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 5s 386us/sample - loss: 0.8950 - val_loss: 1.0450\n",
      "Epoch 31/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.8806\n",
      "Epoch 00031: val_loss did not improve from 0.99483\n",
      "14152/14152 [==============================] - 5s 382us/sample - loss: 0.8809 - val_loss: 1.0623\n",
      "Partial score of fold 1 is: [0.46343418]\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 391)]             0         \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 50, 391)           0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 50, 391, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 7, 18)          45018     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 126)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               12700     \n",
      "_________________________________________________________________\n",
      "layer_normalization_4 (Layer (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_5 (Layer (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 63,119\n",
      "Trainable params: 63,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.9067\n",
      "Epoch 00001: val_loss improved from inf to 1.09832, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 6s 442us/sample - loss: 1.9064 - val_loss: 1.0983\n",
      "Epoch 2/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.4540\n",
      "Epoch 00002: val_loss improved from 1.09832 to 1.07548, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 383us/sample - loss: 1.4527 - val_loss: 1.0755\n",
      "Epoch 3/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.3231\n",
      "Epoch 00003: val_loss improved from 1.07548 to 1.07540, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 383us/sample - loss: 1.3231 - val_loss: 1.0754\n",
      "Epoch 4/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.2672\n",
      "Epoch 00004: val_loss improved from 1.07540 to 1.05673, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 374us/sample - loss: 1.2678 - val_loss: 1.0567\n",
      "Epoch 5/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.2243\n",
      "Epoch 00005: val_loss improved from 1.05673 to 1.05589, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 379us/sample - loss: 1.2240 - val_loss: 1.0559\n",
      "Epoch 6/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1909\n",
      "Epoch 00006: val_loss improved from 1.05589 to 1.04760, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 386us/sample - loss: 1.1918 - val_loss: 1.0476\n",
      "Epoch 7/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.1752\n",
      "Epoch 00007: val_loss improved from 1.04760 to 1.04345, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 377us/sample - loss: 1.1745 - val_loss: 1.0434\n",
      "Epoch 8/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1390\n",
      "Epoch 00008: val_loss improved from 1.04345 to 1.02781, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 377us/sample - loss: 1.1418 - val_loss: 1.0278\n",
      "Epoch 9/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.1198\n",
      "Epoch 00009: val_loss did not improve from 1.02781\n",
      "14152/14152 [==============================] - 5s 379us/sample - loss: 1.1205 - val_loss: 1.0315\n",
      "Epoch 10/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1142\n",
      "Epoch 00010: val_loss did not improve from 1.02781\n",
      "14152/14152 [==============================] - 5s 369us/sample - loss: 1.1146 - val_loss: 1.0507\n",
      "Epoch 11/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0967\n",
      "Epoch 00011: val_loss did not improve from 1.02781\n",
      "14152/14152 [==============================] - 5s 371us/sample - loss: 1.0976 - val_loss: 1.0474\n",
      "Epoch 12/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0681\n",
      "Epoch 00012: val_loss improved from 1.02781 to 1.01446, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 379us/sample - loss: 1.0663 - val_loss: 1.0145\n",
      "Epoch 13/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0602\n",
      "Epoch 00013: val_loss improved from 1.01446 to 1.00346, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 375us/sample - loss: 1.0613 - val_loss: 1.0035\n",
      "Epoch 14/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0429\n",
      "Epoch 00014: val_loss did not improve from 1.00346\n",
      "14152/14152 [==============================] - 5s 376us/sample - loss: 1.0434 - val_loss: 1.0059\n",
      "Epoch 15/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0375\n",
      "Epoch 00015: val_loss did not improve from 1.00346\n",
      "14152/14152 [==============================] - 5s 372us/sample - loss: 1.0388 - val_loss: 1.0148\n",
      "Epoch 16/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0177\n",
      "Epoch 00016: val_loss did not improve from 1.00346\n",
      "14152/14152 [==============================] - 5s 373us/sample - loss: 1.0193 - val_loss: 1.0081\n",
      "Epoch 17/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0013\n",
      "Epoch 00017: val_loss did not improve from 1.00346\n",
      "14152/14152 [==============================] - 5s 370us/sample - loss: 1.0021 - val_loss: 1.0048\n",
      "Epoch 18/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9943\n",
      "Epoch 00018: val_loss did not improve from 1.00346\n",
      "14152/14152 [==============================] - 5s 375us/sample - loss: 0.9947 - val_loss: 1.0088\n",
      "Epoch 19/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9911\n",
      "Epoch 00019: val_loss did not improve from 1.00346\n",
      "14152/14152 [==============================] - 5s 378us/sample - loss: 0.9913 - val_loss: 1.0052\n",
      "Epoch 20/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9900\n",
      "Epoch 00020: val_loss did not improve from 1.00346\n",
      "14152/14152 [==============================] - 5s 377us/sample - loss: 0.9909 - val_loss: 1.0167\n",
      "Epoch 21/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9598\n",
      "Epoch 00021: val_loss did not improve from 1.00346\n",
      "14152/14152 [==============================] - 5s 374us/sample - loss: 0.9603 - val_loss: 1.0282\n",
      "Epoch 22/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9573\n",
      "Epoch 00022: val_loss did not improve from 1.00346\n",
      "14152/14152 [==============================] - 5s 380us/sample - loss: 0.9571 - val_loss: 1.0194\n",
      "Epoch 23/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9481\n",
      "Epoch 00023: val_loss improved from 1.00346 to 0.99975, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 381us/sample - loss: 0.9496 - val_loss: 0.9997\n",
      "Epoch 24/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9384\n",
      "Epoch 00024: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 376us/sample - loss: 0.9382 - val_loss: 1.0023\n",
      "Epoch 25/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9320\n",
      "Epoch 00025: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 378us/sample - loss: 0.9326 - val_loss: 1.0056\n",
      "Epoch 26/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9081\n",
      "Epoch 00026: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 386us/sample - loss: 0.9077 - val_loss: 1.0263\n",
      "Epoch 27/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9066\n",
      "Epoch 00027: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 383us/sample - loss: 0.9075 - val_loss: 1.0178\n",
      "Epoch 28/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9047\n",
      "Epoch 00028: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 385us/sample - loss: 0.9042 - val_loss: 1.0391\n",
      "Epoch 29/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.8926\n",
      "Epoch 00029: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 386us/sample - loss: 0.8923 - val_loss: 1.0256\n",
      "Epoch 30/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8815\n",
      "Epoch 00030: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 374us/sample - loss: 0.8810 - val_loss: 1.0367\n",
      "Epoch 31/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.8802\n",
      "Epoch 00031: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 382us/sample - loss: 0.8791 - val_loss: 1.0271\n",
      "Epoch 32/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.8712\n",
      "Epoch 00032: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 382us/sample - loss: 0.8715 - val_loss: 1.0279\n",
      "Epoch 33/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8616\n",
      "Epoch 00033: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 379us/sample - loss: 0.8623 - val_loss: 1.0317\n",
      "Epoch 34/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8589\n",
      "Epoch 00034: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 373us/sample - loss: 0.8572 - val_loss: 1.0404\n",
      "Epoch 35/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8450\n",
      "Epoch 00035: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 6s 426us/sample - loss: 0.8458 - val_loss: 1.0347\n",
      "Epoch 36/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8389\n",
      "Epoch 00036: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 367us/sample - loss: 0.8385 - val_loss: 1.0625\n",
      "Epoch 37/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.8337\n",
      "Epoch 00037: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 368us/sample - loss: 0.8336 - val_loss: 1.0478\n",
      "Epoch 38/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.8254\n",
      "Epoch 00038: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 377us/sample - loss: 0.8260 - val_loss: 1.0388\n",
      "Epoch 39/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8139\n",
      "Epoch 00039: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 366us/sample - loss: 0.8135 - val_loss: 1.0591\n",
      "Epoch 40/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8136\n",
      "Epoch 00040: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 375us/sample - loss: 0.8136 - val_loss: 1.0576\n",
      "Epoch 41/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.7997\n",
      "Epoch 00041: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 374us/sample - loss: 0.7989 - val_loss: 1.0589\n",
      "Epoch 42/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.7921\n",
      "Epoch 00042: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 5s 374us/sample - loss: 0.7925 - val_loss: 1.0570\n",
      "Epoch 43/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.7837\n",
      "Epoch 00043: val_loss did not improve from 0.99975\n",
      "14152/14152 [==============================] - 6s 401us/sample - loss: 0.7846 - val_loss: 1.0652\n",
      "Partial score of fold 2 is: [0.47761037]\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 391)]             0         \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 50, 391)           0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 50, 391, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 7, 18)          45018     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 126)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               12700     \n",
      "_________________________________________________________________\n",
      "layer_normalization_6 (Layer (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_7 (Layer (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 63,119\n",
      "Trainable params: 63,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.8512\n",
      "Epoch 00001: val_loss improved from inf to 1.09536, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 6s 436us/sample - loss: 1.8506 - val_loss: 1.0954\n",
      "Epoch 2/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.3998\n",
      "Epoch 00002: val_loss improved from 1.09536 to 1.09070, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 374us/sample - loss: 1.3995 - val_loss: 1.0907\n",
      "Epoch 3/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.2795\n",
      "Epoch 00003: val_loss improved from 1.09070 to 1.05478, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 371us/sample - loss: 1.2790 - val_loss: 1.0548\n",
      "Epoch 4/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.2421\n",
      "Epoch 00004: val_loss improved from 1.05478 to 1.04116, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 380us/sample - loss: 1.2419 - val_loss: 1.0412\n",
      "Epoch 5/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1976\n",
      "Epoch 00005: val_loss improved from 1.04116 to 1.04055, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 374us/sample - loss: 1.1965 - val_loss: 1.0406\n",
      "Epoch 6/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.1699\n",
      "Epoch 00006: val_loss improved from 1.04055 to 1.03917, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 372us/sample - loss: 1.1701 - val_loss: 1.0392\n",
      "Epoch 7/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.1453\n",
      "Epoch 00007: val_loss improved from 1.03917 to 1.03723, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 371us/sample - loss: 1.1453 - val_loss: 1.0372\n",
      "Epoch 8/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1293\n",
      "Epoch 00008: val_loss improved from 1.03723 to 1.02248, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 6s 406us/sample - loss: 1.1290 - val_loss: 1.0225\n",
      "Epoch 9/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.1187\n",
      "Epoch 00009: val_loss improved from 1.02248 to 1.01358, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 373us/sample - loss: 1.1191 - val_loss: 1.0136\n",
      "Epoch 10/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0830\n",
      "Epoch 00010: val_loss improved from 1.01358 to 1.00796, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 6s 410us/sample - loss: 1.0832 - val_loss: 1.0080\n",
      "Epoch 11/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0897\n",
      "Epoch 00011: val_loss did not improve from 1.00796\n",
      "14152/14152 [==============================] - 6s 410us/sample - loss: 1.0898 - val_loss: 1.0176\n",
      "Epoch 12/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0558\n",
      "Epoch 00012: val_loss did not improve from 1.00796\n",
      "14152/14152 [==============================] - 5s 384us/sample - loss: 1.0561 - val_loss: 1.0124\n",
      "Epoch 13/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0577\n",
      "Epoch 00013: val_loss did not improve from 1.00796\n",
      "14152/14152 [==============================] - 5s 372us/sample - loss: 1.0573 - val_loss: 1.0083\n",
      "Epoch 14/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0519\n",
      "Epoch 00014: val_loss improved from 1.00796 to 1.00531, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 374us/sample - loss: 1.0511 - val_loss: 1.0053\n",
      "Epoch 15/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0365\n",
      "Epoch 00015: val_loss did not improve from 1.00531\n",
      "14152/14152 [==============================] - 5s 371us/sample - loss: 1.0369 - val_loss: 1.0165\n",
      "Epoch 16/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0214\n",
      "Epoch 00016: val_loss did not improve from 1.00531\n",
      "14152/14152 [==============================] - 5s 371us/sample - loss: 1.0210 - val_loss: 1.0259\n",
      "Epoch 17/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0145\n",
      "Epoch 00017: val_loss did not improve from 1.00531\n",
      "14152/14152 [==============================] - 5s 375us/sample - loss: 1.0138 - val_loss: 1.0058\n",
      "Epoch 18/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0053\n",
      "Epoch 00018: val_loss did not improve from 1.00531\n",
      "14152/14152 [==============================] - 5s 375us/sample - loss: 1.0050 - val_loss: 1.0088\n",
      "Epoch 19/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9914\n",
      "Epoch 00019: val_loss improved from 1.00531 to 1.00334, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 372us/sample - loss: 0.9912 - val_loss: 1.0033\n",
      "Epoch 20/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9763\n",
      "Epoch 00020: val_loss did not improve from 1.00334\n",
      "14152/14152 [==============================] - 5s 370us/sample - loss: 0.9772 - val_loss: 1.0095\n",
      "Epoch 21/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9851\n",
      "Epoch 00021: val_loss did not improve from 1.00334\n",
      "14152/14152 [==============================] - 5s 368us/sample - loss: 0.9841 - val_loss: 1.0138\n",
      "Epoch 22/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9658\n",
      "Epoch 00022: val_loss did not improve from 1.00334\n",
      "14152/14152 [==============================] - 5s 369us/sample - loss: 0.9663 - val_loss: 1.0046\n",
      "Epoch 23/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9593\n",
      "Epoch 00023: val_loss did not improve from 1.00334\n",
      "14152/14152 [==============================] - 5s 367us/sample - loss: 0.9595 - val_loss: 1.0049\n",
      "Epoch 24/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9497\n",
      "Epoch 00024: val_loss did not improve from 1.00334\n",
      "14152/14152 [==============================] - 5s 370us/sample - loss: 0.9498 - val_loss: 1.0066\n",
      "Epoch 25/100\n",
      "10112/14152 [====================>.........] - ETA: 1s - loss: 0.9408"
     ]
    }
   ],
   "source": [
    "cnn_model = Cnn_Model(reduce_train, ajusted_test, features, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data:  0.4726632103896947\n"
     ]
    }
   ],
   "source": [
    "cnn_train_pred = cnn_model.oof_pred\n",
    "print('Accuracy on training data: ', qwk3(reduce_train['accuracy_group'], cnn_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 200)               78400     \n",
      "_________________________________________________________________\n",
      "layer_normalization_10 (Laye (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_11 (Laye (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_12 (Laye (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_13 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 105,601\n",
      "Trainable params: 105,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14151 samples, validate on 3539 samples\n",
      "Epoch 1/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.9481\n",
      "Epoch 00001: val_loss improved from inf to 1.14740, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 4s 284us/sample - loss: 1.9446 - val_loss: 1.1474\n",
      "Epoch 2/100\n",
      "13920/14151 [============================>.] - ETA: 0s - loss: 1.4468\n",
      "Epoch 00002: val_loss improved from 1.14740 to 1.08765, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 196us/sample - loss: 1.4456 - val_loss: 1.0877\n",
      "Epoch 3/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.2954\n",
      "Epoch 00003: val_loss improved from 1.08765 to 1.06537, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 194us/sample - loss: 1.2948 - val_loss: 1.0654\n",
      "Epoch 4/100\n",
      "13888/14151 [============================>.] - ETA: 0s - loss: 1.2299\n",
      "Epoch 00004: val_loss improved from 1.06537 to 1.06009, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 191us/sample - loss: 1.2244 - val_loss: 1.0601\n",
      "Epoch 5/100\n",
      "13856/14151 [============================>.] - ETA: 0s - loss: 1.1851\n",
      "Epoch 00005: val_loss did not improve from 1.06009\n",
      "14151/14151 [==============================] - 3s 190us/sample - loss: 1.1843 - val_loss: 1.0652\n",
      "Epoch 6/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 1.1635\n",
      "Epoch 00006: val_loss improved from 1.06009 to 1.04798, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 194us/sample - loss: 1.1633 - val_loss: 1.0480\n",
      "Epoch 7/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 1.1365\n",
      "Epoch 00007: val_loss improved from 1.04798 to 1.02966, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 195us/sample - loss: 1.1368 - val_loss: 1.0297\n",
      "Epoch 8/100\n",
      "13952/14151 [============================>.] - ETA: 0s - loss: 1.1260\n",
      "Epoch 00008: val_loss did not improve from 1.02966\n",
      "14151/14151 [==============================] - 3s 193us/sample - loss: 1.1246 - val_loss: 1.0404\n",
      "Epoch 9/100\n",
      "13984/14151 [============================>.] - ETA: 0s - loss: 1.1070\n",
      "Epoch 00009: val_loss improved from 1.02966 to 1.02677, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 189us/sample - loss: 1.1081 - val_loss: 1.0268\n",
      "Epoch 10/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 1.0888\n",
      "Epoch 00010: val_loss did not improve from 1.02677\n",
      "14151/14151 [==============================] - 3s 186us/sample - loss: 1.0882 - val_loss: 1.0277\n",
      "Epoch 11/100\n",
      "13856/14151 [============================>.] - ETA: 0s - loss: 1.0842\n",
      "Epoch 00011: val_loss improved from 1.02677 to 1.00846, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 191us/sample - loss: 1.0820 - val_loss: 1.0085\n",
      "Epoch 12/100\n",
      "13952/14151 [============================>.] - ETA: 0s - loss: 1.0599\n",
      "Epoch 00012: val_loss improved from 1.00846 to 1.00810, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 191us/sample - loss: 1.0630 - val_loss: 1.0081\n",
      "Epoch 13/100\n",
      "13888/14151 [============================>.] - ETA: 0s - loss: 1.0626\n",
      "Epoch 00013: val_loss did not improve from 1.00810\n",
      "14151/14151 [==============================] - 3s 189us/sample - loss: 1.0623 - val_loss: 1.0124\n",
      "Epoch 14/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.0545\n",
      "Epoch 00014: val_loss did not improve from 1.00810\n",
      "14151/14151 [==============================] - 3s 199us/sample - loss: 1.0539 - val_loss: 1.0089\n",
      "Epoch 15/100\n",
      "13984/14151 [============================>.] - ETA: 0s - loss: 1.0470\n",
      "Epoch 00015: val_loss improved from 1.00810 to 1.00719, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 189us/sample - loss: 1.0469 - val_loss: 1.0072\n",
      "Epoch 16/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.0203\n",
      "Epoch 00016: val_loss improved from 1.00719 to 0.99902, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 189us/sample - loss: 1.0207 - val_loss: 0.9990\n",
      "Epoch 17/100\n",
      "13856/14151 [============================>.] - ETA: 0s - loss: 1.0270\n",
      "Epoch 00017: val_loss did not improve from 0.99902\n",
      "14151/14151 [==============================] - 3s 191us/sample - loss: 1.0241 - val_loss: 1.0007\n",
      "Epoch 18/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 1.0129\n",
      "Epoch 00018: val_loss improved from 0.99902 to 0.98535, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 192us/sample - loss: 1.0123 - val_loss: 0.9854\n",
      "Epoch 19/100\n",
      "13888/14151 [============================>.] - ETA: 0s - loss: 0.9983\n",
      "Epoch 00019: val_loss improved from 0.98535 to 0.98200, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 190us/sample - loss: 0.9996 - val_loss: 0.9820\n",
      "Epoch 20/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.9940\n",
      "Epoch 00020: val_loss did not improve from 0.98200\n",
      "14151/14151 [==============================] - 3s 195us/sample - loss: 0.9925 - val_loss: 0.9862\n",
      "Epoch 21/100\n",
      "13952/14151 [============================>.] - ETA: 0s - loss: 0.9855\n",
      "Epoch 00021: val_loss improved from 0.98200 to 0.97327, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 192us/sample - loss: 0.9869 - val_loss: 0.9733\n",
      "Epoch 22/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.9828\n",
      "Epoch 00022: val_loss did not improve from 0.97327\n",
      "14151/14151 [==============================] - 3s 186us/sample - loss: 0.9829 - val_loss: 0.9814\n",
      "Epoch 23/100\n",
      "13984/14151 [============================>.] - ETA: 0s - loss: 0.9645\n",
      "Epoch 00023: val_loss did not improve from 0.97327\n",
      "14151/14151 [==============================] - 3s 188us/sample - loss: 0.9642 - val_loss: 0.9735\n",
      "Epoch 24/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.9591\n",
      "Epoch 00024: val_loss did not improve from 0.97327\n",
      "14151/14151 [==============================] - 3s 187us/sample - loss: 0.9593 - val_loss: 0.9854\n",
      "Epoch 25/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.9412\n",
      "Epoch 00025: val_loss did not improve from 0.97327\n",
      "14151/14151 [==============================] - 3s 187us/sample - loss: 0.9432 - val_loss: 0.9824\n",
      "Epoch 26/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.9379\n",
      "Epoch 00026: val_loss did not improve from 0.97327\n",
      "14151/14151 [==============================] - 3s 187us/sample - loss: 0.9355 - val_loss: 0.9888\n",
      "Epoch 27/100\n",
      "13984/14151 [============================>.] - ETA: 0s - loss: 0.9344\n",
      "Epoch 00027: val_loss improved from 0.97327 to 0.96960, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 187us/sample - loss: 0.9352 - val_loss: 0.9696\n",
      "Epoch 28/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.9278\n",
      "Epoch 00028: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 185us/sample - loss: 0.9287 - val_loss: 0.9805\n",
      "Epoch 29/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.9215\n",
      "Epoch 00029: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 184us/sample - loss: 0.9207 - val_loss: 0.9725\n",
      "Epoch 30/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.9081\n",
      "Epoch 00030: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 187us/sample - loss: 0.9078 - val_loss: 0.9987\n",
      "Epoch 31/100\n",
      "13952/14151 [============================>.] - ETA: 0s - loss: 0.9112\n",
      "Epoch 00031: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 187us/sample - loss: 0.9100 - val_loss: 0.9796\n",
      "Epoch 32/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.9086\n",
      "Epoch 00032: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 186us/sample - loss: 0.9073 - val_loss: 0.9805\n",
      "Epoch 33/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.9028\n",
      "Epoch 00033: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 191us/sample - loss: 0.9047 - val_loss: 0.9927\n",
      "Epoch 34/100\n",
      "13856/14151 [============================>.] - ETA: 0s - loss: 0.8822\n",
      "Epoch 00034: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 188us/sample - loss: 0.8829 - val_loss: 0.9726\n",
      "Epoch 35/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.8755\n",
      "Epoch 00035: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 187us/sample - loss: 0.8750 - val_loss: 0.9749\n",
      "Epoch 36/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.8757\n",
      "Epoch 00036: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 186us/sample - loss: 0.8788 - val_loss: 0.9730\n",
      "Epoch 37/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.8698\n",
      "Epoch 00037: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 185us/sample - loss: 0.8695 - val_loss: 1.0183\n",
      "Epoch 38/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.8650\n",
      "Epoch 00038: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 186us/sample - loss: 0.8658 - val_loss: 0.9952\n",
      "Epoch 39/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.8592\n",
      "Epoch 00039: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 185us/sample - loss: 0.8582 - val_loss: 0.9832\n",
      "Epoch 40/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.8518\n",
      "Epoch 00040: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 185us/sample - loss: 0.8520 - val_loss: 1.0025\n",
      "Epoch 41/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 0.8515\n",
      "Epoch 00041: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 184us/sample - loss: 0.8523 - val_loss: 0.9781\n",
      "Epoch 42/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.8365\n",
      "Epoch 00042: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 186us/sample - loss: 0.8365 - val_loss: 0.9824\n",
      "Epoch 43/100\n",
      "13952/14151 [============================>.] - ETA: 0s - loss: 0.8268\n",
      "Epoch 00043: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 187us/sample - loss: 0.8246 - val_loss: 1.0185\n",
      "Epoch 44/100\n",
      "13984/14151 [============================>.] - ETA: 0s - loss: 0.8186\n",
      "Epoch 00044: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 185us/sample - loss: 0.8201 - val_loss: 0.9883\n",
      "Epoch 45/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.8268\n",
      "Epoch 00045: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 185us/sample - loss: 0.8268 - val_loss: 1.0049\n",
      "Epoch 46/100\n",
      "13888/14151 [============================>.] - ETA: 0s - loss: 0.8113\n",
      "Epoch 00046: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 184us/sample - loss: 0.8127 - val_loss: 0.9860\n",
      "Epoch 47/100\n",
      "13984/14151 [============================>.] - ETA: 0s - loss: 0.8175\n",
      "Epoch 00047: val_loss did not improve from 0.96960\n",
      "14151/14151 [==============================] - 3s 185us/sample - loss: 0.8187 - val_loss: 1.0155\n",
      "Partial score of fold 0 is: [0.48673967]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 200)               78400     \n",
      "_________________________________________________________________\n",
      "layer_normalization_14 (Laye (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_15 (Laye (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_16 (Laye (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_17 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 105,601\n",
      "Trainable params: 105,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 2.0572\n",
      "Epoch 00001: val_loss improved from inf to 1.11420, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 4s 277us/sample - loss: 2.0514 - val_loss: 1.1142\n",
      "Epoch 2/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.4744\n",
      "Epoch 00002: val_loss improved from 1.11420 to 1.08757, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 4s 302us/sample - loss: 1.4746 - val_loss: 1.0876\n",
      "Epoch 3/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.3262\n",
      "Epoch 00003: val_loss improved from 1.08757 to 1.06760, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 1.3245 - val_loss: 1.0676\n",
      "Epoch 4/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.2367\n",
      "Epoch 00004: val_loss improved from 1.06760 to 1.04160, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 1.2371 - val_loss: 1.0416\n",
      "Epoch 5/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.2039\n",
      "Epoch 00005: val_loss improved from 1.04160 to 1.03432, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 223us/sample - loss: 1.2047 - val_loss: 1.0343\n",
      "Epoch 6/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1651\n",
      "Epoch 00006: val_loss improved from 1.03432 to 1.02279, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 238us/sample - loss: 1.1648 - val_loss: 1.0228\n",
      "Epoch 7/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.1604\n",
      "Epoch 00007: val_loss did not improve from 1.02279\n",
      "14152/14152 [==============================] - 3s 190us/sample - loss: 1.1590 - val_loss: 1.0335\n",
      "Epoch 8/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.1299\n",
      "Epoch 00008: val_loss did not improve from 1.02279\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 1.1285 - val_loss: 1.0246\n",
      "Epoch 9/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.1180\n",
      "Epoch 00009: val_loss improved from 1.02279 to 1.01645, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 1.1172 - val_loss: 1.0164\n",
      "Epoch 10/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.1017\n",
      "Epoch 00010: val_loss did not improve from 1.01645\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 1.1024 - val_loss: 1.0177\n",
      "Epoch 11/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0792\n",
      "Epoch 00011: val_loss improved from 1.01645 to 1.01317, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 1.0806 - val_loss: 1.0132\n",
      "Epoch 12/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0712\n",
      "Epoch 00012: val_loss did not improve from 1.01317\n",
      "14152/14152 [==============================] - 3s 184us/sample - loss: 1.0707 - val_loss: 1.0465\n",
      "Epoch 13/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0613\n",
      "Epoch 00013: val_loss improved from 1.01317 to 1.00648, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 1.0605 - val_loss: 1.0065\n",
      "Epoch 14/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0430\n",
      "Epoch 00014: val_loss did not improve from 1.00648\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 1.0415 - val_loss: 1.0082\n",
      "Epoch 15/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0434\n",
      "Epoch 00015: val_loss improved from 1.00648 to 1.00074, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 192us/sample - loss: 1.0406 - val_loss: 1.0007\n",
      "Epoch 16/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0196\n",
      "Epoch 00016: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 1.0199 - val_loss: 1.0020\n",
      "Epoch 17/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0024\n",
      "Epoch 00017: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 1.0042 - val_loss: 1.0313\n",
      "Epoch 18/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0048\n",
      "Epoch 00018: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 1.0047 - val_loss: 1.0060\n",
      "Epoch 19/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9861\n",
      "Epoch 00019: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 213us/sample - loss: 0.9841 - val_loss: 1.0205\n",
      "Epoch 20/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9787\n",
      "Epoch 00020: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 198us/sample - loss: 0.9799 - val_loss: 1.0062\n",
      "Epoch 21/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9708\n",
      "Epoch 00021: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 193us/sample - loss: 0.9706 - val_loss: 1.0161\n",
      "Epoch 22/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9636\n",
      "Epoch 00022: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 198us/sample - loss: 0.9612 - val_loss: 1.0109\n",
      "Epoch 23/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9552\n",
      "Epoch 00023: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 184us/sample - loss: 0.9555 - val_loss: 1.0020\n",
      "Epoch 24/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9430\n",
      "Epoch 00024: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.9422 - val_loss: 1.0226\n",
      "Epoch 25/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9328\n",
      "Epoch 00025: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.9315 - val_loss: 1.0142\n",
      "Epoch 26/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9421\n",
      "Epoch 00026: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.9409 - val_loss: 1.0274\n",
      "Epoch 27/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9198\n",
      "Epoch 00027: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 0.9199 - val_loss: 1.0169\n",
      "Epoch 28/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9191\n",
      "Epoch 00028: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 0.9166 - val_loss: 1.0222\n",
      "Epoch 29/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9158\n",
      "Epoch 00029: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 218us/sample - loss: 0.9150 - val_loss: 1.0262\n",
      "Epoch 30/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9018\n",
      "Epoch 00030: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 4s 315us/sample - loss: 0.9012 - val_loss: 1.0118\n",
      "Epoch 31/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.8708\n",
      "Epoch 00031: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 224us/sample - loss: 0.8708 - val_loss: 1.0180\n",
      "Epoch 32/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.8888\n",
      "Epoch 00032: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 190us/sample - loss: 0.8877 - val_loss: 1.0128\n",
      "Epoch 33/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8695\n",
      "Epoch 00033: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 0.8691 - val_loss: 1.0464\n",
      "Epoch 34/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.8745\n",
      "Epoch 00034: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 0.8734 - val_loss: 1.0433\n",
      "Epoch 35/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.8640\n",
      "Epoch 00035: val_loss did not improve from 1.00074\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 0.8661 - val_loss: 1.0315\n",
      "Partial score of fold 1 is: [0.47278128]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 200)               78400     \n",
      "_________________________________________________________________\n",
      "layer_normalization_18 (Laye (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_19 (Laye (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_20 (Laye (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_21 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 105,601\n",
      "Trainable params: 105,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.6660\n",
      "Epoch 00001: val_loss improved from inf to 1.17292, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 4s 288us/sample - loss: 1.6622 - val_loss: 1.1729\n",
      "Epoch 2/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.2992\n",
      "Epoch 00002: val_loss improved from 1.17292 to 1.12835, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 1.2989 - val_loss: 1.1283\n",
      "Epoch 3/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.2297\n",
      "Epoch 00003: val_loss improved from 1.12835 to 1.07557, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 191us/sample - loss: 1.2309 - val_loss: 1.0756\n",
      "Epoch 4/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1791\n",
      "Epoch 00004: val_loss improved from 1.07557 to 1.07108, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 1.1786 - val_loss: 1.0711\n",
      "Epoch 5/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1633\n",
      "Epoch 00005: val_loss improved from 1.07108 to 1.06405, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 1.1637 - val_loss: 1.0641\n",
      "Epoch 6/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.1431\n",
      "Epoch 00006: val_loss improved from 1.06405 to 1.04822, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 191us/sample - loss: 1.1441 - val_loss: 1.0482\n",
      "Epoch 7/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.1133\n",
      "Epoch 00007: val_loss did not improve from 1.04822\n",
      "14152/14152 [==============================] - 3s 191us/sample - loss: 1.1142 - val_loss: 1.0720\n",
      "Epoch 8/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1090\n",
      "Epoch 00008: val_loss improved from 1.04822 to 1.03892, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 193us/sample - loss: 1.1092 - val_loss: 1.0389\n",
      "Epoch 9/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0965\n",
      "Epoch 00009: val_loss improved from 1.03892 to 1.03059, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 195us/sample - loss: 1.0961 - val_loss: 1.0306\n",
      "Epoch 10/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0851\n",
      "Epoch 00010: val_loss did not improve from 1.03059\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 1.0847 - val_loss: 1.0325\n",
      "Epoch 11/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0697\n",
      "Epoch 00011: val_loss did not improve from 1.03059\n",
      "14152/14152 [==============================] - 3s 191us/sample - loss: 1.0687 - val_loss: 1.0395\n",
      "Epoch 12/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0458\n",
      "Epoch 00012: val_loss did not improve from 1.03059\n",
      "14152/14152 [==============================] - 3s 192us/sample - loss: 1.0454 - val_loss: 1.0311\n",
      "Epoch 13/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0437\n",
      "Epoch 00013: val_loss did not improve from 1.03059\n",
      "14152/14152 [==============================] - 3s 192us/sample - loss: 1.0442 - val_loss: 1.0342\n",
      "Epoch 14/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0384\n",
      "Epoch 00014: val_loss improved from 1.03059 to 1.02345, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 192us/sample - loss: 1.0370 - val_loss: 1.0235\n",
      "Epoch 15/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0280\n",
      "Epoch 00015: val_loss improved from 1.02345 to 1.01356, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 195us/sample - loss: 1.0283 - val_loss: 1.0136\n",
      "Epoch 16/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0265\n",
      "Epoch 00016: val_loss did not improve from 1.01356\n",
      "14152/14152 [==============================] - 3s 190us/sample - loss: 1.0277 - val_loss: 1.0160\n",
      "Epoch 17/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9973\n",
      "Epoch 00017: val_loss did not improve from 1.01356\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 0.9990 - val_loss: 1.0145\n",
      "Epoch 18/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0005\n",
      "Epoch 00018: val_loss did not improve from 1.01356\n",
      "14152/14152 [==============================] - 3s 192us/sample - loss: 1.0010 - val_loss: 1.0178\n",
      "Epoch 19/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9797\n",
      "Epoch 00019: val_loss improved from 1.01356 to 1.00929, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 0.9791 - val_loss: 1.0093\n",
      "Epoch 20/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9762\n",
      "Epoch 00020: val_loss improved from 1.00929 to 1.00565, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 194us/sample - loss: 0.9775 - val_loss: 1.0056\n",
      "Epoch 21/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9843\n",
      "Epoch 00021: val_loss did not improve from 1.00565\n",
      "14152/14152 [==============================] - 3s 196us/sample - loss: 0.9839 - val_loss: 1.0078\n",
      "Epoch 22/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9717\n",
      "Epoch 00022: val_loss improved from 1.00565 to 0.99617, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 197us/sample - loss: 0.9712 - val_loss: 0.9962\n",
      "Epoch 23/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9520\n",
      "Epoch 00023: val_loss did not improve from 0.99617\n",
      "14152/14152 [==============================] - 3s 199us/sample - loss: 0.9515 - val_loss: 1.0097\n",
      "Epoch 24/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9454\n",
      "Epoch 00024: val_loss did not improve from 0.99617\n",
      "14152/14152 [==============================] - 3s 191us/sample - loss: 0.9455 - val_loss: 1.0058\n",
      "Epoch 25/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9375\n",
      "Epoch 00025: val_loss did not improve from 0.99617\n",
      "14152/14152 [==============================] - 3s 195us/sample - loss: 0.9366 - val_loss: 1.0354\n",
      "Epoch 26/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9317\n",
      "Epoch 00026: val_loss improved from 0.99617 to 0.99582, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 194us/sample - loss: 0.9328 - val_loss: 0.9958\n",
      "Epoch 27/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9248\n",
      "Epoch 00027: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 193us/sample - loss: 0.9225 - val_loss: 1.0016\n",
      "Epoch 28/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9108\n",
      "Epoch 00028: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 191us/sample - loss: 0.9106 - val_loss: 1.0208\n",
      "Epoch 29/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9037\n",
      "Epoch 00029: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 194us/sample - loss: 0.9042 - val_loss: 1.0276\n",
      "Epoch 30/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9001\n",
      "Epoch 00030: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 200us/sample - loss: 0.8988 - val_loss: 1.0091\n",
      "Epoch 31/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8882\n",
      "Epoch 00031: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 191us/sample - loss: 0.8879 - val_loss: 1.0081\n",
      "Epoch 32/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.8919\n",
      "Epoch 00032: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 193us/sample - loss: 0.8909 - val_loss: 1.0278\n",
      "Epoch 33/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.8794\n",
      "Epoch 00033: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 190us/sample - loss: 0.8792 - val_loss: 1.0113\n",
      "Epoch 34/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.8708\n",
      "Epoch 00034: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 196us/sample - loss: 0.8705 - val_loss: 1.0184\n",
      "Epoch 35/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.8610\n",
      "Epoch 00035: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 197us/sample - loss: 0.8617 - val_loss: 1.0232\n",
      "Epoch 36/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.8606\n",
      "Epoch 00036: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 0.8572 - val_loss: 1.0167\n",
      "Epoch 37/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8570\n",
      "Epoch 00037: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 190us/sample - loss: 0.8563 - val_loss: 1.0342\n",
      "Epoch 38/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.8320\n",
      "Epoch 00038: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 0.8328 - val_loss: 1.0295\n",
      "Epoch 39/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.8478\n",
      "Epoch 00039: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 0.8469 - val_loss: 1.0437\n",
      "Epoch 40/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8382\n",
      "Epoch 00040: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 193us/sample - loss: 0.8366 - val_loss: 1.0219\n",
      "Epoch 41/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8242\n",
      "Epoch 00041: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 192us/sample - loss: 0.8236 - val_loss: 1.0242\n",
      "Epoch 42/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.8185\n",
      "Epoch 00042: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 203us/sample - loss: 0.8167 - val_loss: 1.0396\n",
      "Epoch 43/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.8150\n",
      "Epoch 00043: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 0.8139 - val_loss: 1.0456\n",
      "Epoch 44/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.8221\n",
      "Epoch 00044: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 0.8221 - val_loss: 1.0367\n",
      "Epoch 45/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.8081\n",
      "Epoch 00045: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 0.8094 - val_loss: 1.0439\n",
      "Epoch 46/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.8022\n",
      "Epoch 00046: val_loss did not improve from 0.99582\n",
      "14152/14152 [==============================] - 3s 191us/sample - loss: 0.8036 - val_loss: 1.0306\n",
      "Partial score of fold 2 is: [0.47587116]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 200)               78400     \n",
      "_________________________________________________________________\n",
      "layer_normalization_22 (Laye (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_23 (Laye (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_24 (Laye (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_25 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 105,601\n",
      "Trainable params: 105,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.7623\n",
      "Epoch 00001: val_loss improved from inf to 1.13276, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 4s 297us/sample - loss: 1.7576 - val_loss: 1.1328\n",
      "Epoch 2/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.3283\n",
      "Epoch 00002: val_loss improved from 1.13276 to 1.11045, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 190us/sample - loss: 1.3283 - val_loss: 1.1105\n",
      "Epoch 3/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.2500\n",
      "Epoch 00003: val_loss improved from 1.11045 to 1.07881, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 190us/sample - loss: 1.2485 - val_loss: 1.0788\n",
      "Epoch 4/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.1957\n",
      "Epoch 00004: val_loss improved from 1.07881 to 1.05979, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 196us/sample - loss: 1.1948 - val_loss: 1.0598\n",
      "Epoch 5/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1674\n",
      "Epoch 00005: val_loss did not improve from 1.05979\n",
      "14152/14152 [==============================] - 3s 199us/sample - loss: 1.1691 - val_loss: 1.0638\n",
      "Epoch 6/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1532\n",
      "Epoch 00006: val_loss improved from 1.05979 to 1.05586, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 190us/sample - loss: 1.1519 - val_loss: 1.0559\n",
      "Epoch 7/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.1242\n",
      "Epoch 00007: val_loss improved from 1.05586 to 1.04083, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 1.1271 - val_loss: 1.0408\n",
      "Epoch 8/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1182\n",
      "Epoch 00008: val_loss improved from 1.04083 to 1.04073, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 1.1196 - val_loss: 1.0407\n",
      "Epoch 9/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1000\n",
      "Epoch 00009: val_loss did not improve from 1.04073\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 1.1013 - val_loss: 1.0477\n",
      "Epoch 10/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0987\n",
      "Epoch 00010: val_loss improved from 1.04073 to 1.03994, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 192us/sample - loss: 1.0964 - val_loss: 1.0399\n",
      "Epoch 11/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0784\n",
      "Epoch 00011: val_loss did not improve from 1.03994\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 1.0756 - val_loss: 1.0472\n",
      "Epoch 12/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0647\n",
      "Epoch 00012: val_loss improved from 1.03994 to 1.02471, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 191us/sample - loss: 1.0650 - val_loss: 1.0247\n",
      "Epoch 13/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0635\n",
      "Epoch 00013: val_loss did not improve from 1.02471\n",
      "14152/14152 [==============================] - 3s 192us/sample - loss: 1.0641 - val_loss: 1.0337\n",
      "Epoch 14/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0467\n",
      "Epoch 00014: val_loss improved from 1.02471 to 1.02078, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 192us/sample - loss: 1.0503 - val_loss: 1.0208\n",
      "Epoch 15/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0347\n",
      "Epoch 00015: val_loss improved from 1.02078 to 1.01942, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 191us/sample - loss: 1.0329 - val_loss: 1.0194\n",
      "Epoch 16/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0451\n",
      "Epoch 00016: val_loss improved from 1.01942 to 1.01905, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 191us/sample - loss: 1.0490 - val_loss: 1.0190\n",
      "Epoch 17/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0220\n",
      "Epoch 00017: val_loss did not improve from 1.01905\n",
      "14152/14152 [==============================] - 3s 197us/sample - loss: 1.0223 - val_loss: 1.0235\n",
      "Epoch 18/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9973\n",
      "Epoch 00018: val_loss improved from 1.01905 to 1.01592, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 198us/sample - loss: 0.9959 - val_loss: 1.0159\n",
      "Epoch 19/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0090\n",
      "Epoch 00019: val_loss did not improve from 1.01592\n",
      "14152/14152 [==============================] - 3s 194us/sample - loss: 1.0068 - val_loss: 1.0209\n",
      "Epoch 20/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9883\n",
      "Epoch 00020: val_loss did not improve from 1.01592\n",
      "14152/14152 [==============================] - 3s 192us/sample - loss: 0.9877 - val_loss: 1.0258\n",
      "Epoch 21/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9784\n",
      "Epoch 00021: val_loss did not improve from 1.01592\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.9805 - val_loss: 1.0407\n",
      "Epoch 22/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9757\n",
      "Epoch 00022: val_loss did not improve from 1.01592\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.9757 - val_loss: 1.0222\n",
      "Epoch 23/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9608\n",
      "Epoch 00023: val_loss did not improve from 1.01592\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.9607 - val_loss: 1.0198\n",
      "Epoch 24/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9530\n",
      "Epoch 00024: val_loss did not improve from 1.01592\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.9531 - val_loss: 1.0640\n",
      "Epoch 25/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9532\n",
      "Epoch 00025: val_loss did not improve from 1.01592\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 0.9533 - val_loss: 1.0359\n",
      "Epoch 26/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9456\n",
      "Epoch 00026: val_loss did not improve from 1.01592\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.9468 - val_loss: 1.0199\n",
      "Epoch 27/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9263\n",
      "Epoch 00027: val_loss did not improve from 1.01592\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.9271 - val_loss: 1.0189\n",
      "Epoch 28/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9228\n",
      "Epoch 00028: val_loss did not improve from 1.01592\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.9235 - val_loss: 1.0195\n",
      "Epoch 29/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9124\n",
      "Epoch 00029: val_loss did not improve from 1.01592\n",
      "14152/14152 [==============================] - 3s 184us/sample - loss: 0.9139 - val_loss: 1.0327\n",
      "Epoch 30/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9003\n",
      "Epoch 00030: val_loss improved from 1.01592 to 1.00581, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.8996 - val_loss: 1.0058\n",
      "Epoch 31/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9036\n",
      "Epoch 00031: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.9042 - val_loss: 1.0330\n",
      "Epoch 32/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8915\n",
      "Epoch 00032: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.8915 - val_loss: 1.0462\n",
      "Epoch 33/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8983\n",
      "Epoch 00033: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.8968 - val_loss: 1.0377\n",
      "Epoch 34/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.8848\n",
      "Epoch 00034: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 224us/sample - loss: 0.8849 - val_loss: 1.0371\n",
      "Epoch 35/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8782\n",
      "Epoch 00035: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 230us/sample - loss: 0.8776 - val_loss: 1.0352\n",
      "Epoch 36/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.8566\n",
      "Epoch 00036: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 184us/sample - loss: 0.8569 - val_loss: 1.0184\n",
      "Epoch 37/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.8675\n",
      "Epoch 00037: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.8680 - val_loss: 1.0439\n",
      "Epoch 38/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.8585\n",
      "Epoch 00038: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.8595 - val_loss: 1.0124\n",
      "Epoch 39/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.8503\n",
      "Epoch 00039: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.8507 - val_loss: 1.0372\n",
      "Epoch 40/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.8383\n",
      "Epoch 00040: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 184us/sample - loss: 0.8398 - val_loss: 1.0458\n",
      "Epoch 41/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.8331\n",
      "Epoch 00041: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.8325 - val_loss: 1.0535\n",
      "Epoch 42/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.8395\n",
      "Epoch 00042: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 0.8400 - val_loss: 1.0314\n",
      "Epoch 43/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.8323\n",
      "Epoch 00043: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 0.8306 - val_loss: 1.0254\n",
      "Epoch 44/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8182\n",
      "Epoch 00044: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.8175 - val_loss: 1.0671\n",
      "Epoch 45/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8192\n",
      "Epoch 00045: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.8194 - val_loss: 1.0527\n",
      "Epoch 46/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8193\n",
      "Epoch 00046: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.8190 - val_loss: 1.0631\n",
      "Epoch 47/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8184\n",
      "Epoch 00047: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 210us/sample - loss: 0.8181 - val_loss: 1.0536\n",
      "Epoch 48/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.8040\n",
      "Epoch 00048: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 194us/sample - loss: 0.8024 - val_loss: 1.0577\n",
      "Epoch 49/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.7896\n",
      "Epoch 00049: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 0.7898 - val_loss: 1.0785\n",
      "Epoch 50/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.7914\n",
      "Epoch 00050: val_loss did not improve from 1.00581\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.7914 - val_loss: 1.0593\n",
      "Partial score of fold 3 is: [0.4680403]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 200)               78400     \n",
      "_________________________________________________________________\n",
      "layer_normalization_26 (Laye (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_27 (Laye (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_28 (Laye (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_29 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 105,601\n",
      "Trainable params: 105,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14153 samples, validate on 3537 samples\n",
      "Epoch 1/100\n",
      "14080/14153 [============================>.] - ETA: 0s - loss: 1.9447\n",
      "Epoch 00001: val_loss improved from inf to 1.16255, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 4s 276us/sample - loss: 1.9426 - val_loss: 1.1626\n",
      "Epoch 2/100\n",
      "14112/14153 [============================>.] - ETA: 0s - loss: 1.4144\n",
      "Epoch 00002: val_loss improved from 1.16255 to 1.12452, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 186us/sample - loss: 1.4136 - val_loss: 1.1245\n",
      "Epoch 3/100\n",
      "14016/14153 [============================>.] - ETA: 0s - loss: 1.2942\n",
      "Epoch 00003: val_loss improved from 1.12452 to 1.10310, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 187us/sample - loss: 1.2935 - val_loss: 1.1031\n",
      "Epoch 4/100\n",
      "14144/14153 [============================>.] - ETA: 0s - loss: 1.2446\n",
      "Epoch 00004: val_loss improved from 1.10310 to 1.09479, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 186us/sample - loss: 1.2446 - val_loss: 1.0948\n",
      "Epoch 5/100\n",
      "14080/14153 [============================>.] - ETA: 0s - loss: 1.2053\n",
      "Epoch 00005: val_loss improved from 1.09479 to 1.07474, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 186us/sample - loss: 1.2061 - val_loss: 1.0747\n",
      "Epoch 6/100\n",
      "14048/14153 [============================>.] - ETA: 0s - loss: 1.1688\n",
      "Epoch 00006: val_loss improved from 1.07474 to 1.07445, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 187us/sample - loss: 1.1688 - val_loss: 1.0744\n",
      "Epoch 7/100\n",
      "14080/14153 [============================>.] - ETA: 0s - loss: 1.1563\n",
      "Epoch 00007: val_loss improved from 1.07445 to 1.04881, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 186us/sample - loss: 1.1558 - val_loss: 1.0488\n",
      "Epoch 8/100\n",
      "13984/14153 [============================>.] - ETA: 0s - loss: 1.1347\n",
      "Epoch 00008: val_loss improved from 1.04881 to 1.04572, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 188us/sample - loss: 1.1373 - val_loss: 1.0457\n",
      "Epoch 9/100\n",
      "13952/14153 [============================>.] - ETA: 0s - loss: 1.1146\n",
      "Epoch 00009: val_loss did not improve from 1.04572\n",
      "14153/14153 [==============================] - 3s 211us/sample - loss: 1.1176 - val_loss: 1.0549\n",
      "Epoch 10/100\n",
      "14112/14153 [============================>.] - ETA: 0s - loss: 1.0974\n",
      "Epoch 00010: val_loss did not improve from 1.04572\n",
      "14153/14153 [==============================] - 3s 220us/sample - loss: 1.0989 - val_loss: 1.0541\n",
      "Epoch 11/100\n",
      "13984/14153 [============================>.] - ETA: 0s - loss: 1.0838\n",
      "Epoch 00011: val_loss improved from 1.04572 to 1.03405, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 222us/sample - loss: 1.0842 - val_loss: 1.0341\n",
      "Epoch 12/100\n",
      "13888/14153 [============================>.] - ETA: 0s - loss: 1.0728\n",
      "Epoch 00012: val_loss improved from 1.03405 to 1.03187, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 208us/sample - loss: 1.0713 - val_loss: 1.0319\n",
      "Epoch 13/100\n",
      "14048/14153 [============================>.] - ETA: 0s - loss: 1.0580\n",
      "Epoch 00013: val_loss did not improve from 1.03187\n",
      "14153/14153 [==============================] - 3s 185us/sample - loss: 1.0576 - val_loss: 1.0465\n",
      "Epoch 14/100\n",
      "14080/14153 [============================>.] - ETA: 0s - loss: 1.0453\n",
      "Epoch 00014: val_loss improved from 1.03187 to 1.02761, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 185us/sample - loss: 1.0454 - val_loss: 1.0276\n",
      "Epoch 15/100\n",
      "14112/14153 [============================>.] - ETA: 0s - loss: 1.0406\n",
      "Epoch 00015: val_loss improved from 1.02761 to 1.01831, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 186us/sample - loss: 1.0397 - val_loss: 1.0183\n",
      "Epoch 16/100\n",
      "13856/14153 [============================>.] - ETA: 0s - loss: 1.0337\n",
      "Epoch 00016: val_loss did not improve from 1.01831\n",
      "14153/14153 [==============================] - 3s 183us/sample - loss: 1.0327 - val_loss: 1.0642\n",
      "Epoch 17/100\n",
      "13920/14153 [============================>.] - ETA: 0s - loss: 1.0181\n",
      "Epoch 00017: val_loss did not improve from 1.01831\n",
      "14153/14153 [==============================] - 3s 188us/sample - loss: 1.0191 - val_loss: 1.0404\n",
      "Epoch 18/100\n",
      "14048/14153 [============================>.] - ETA: 0s - loss: 1.0083\n",
      "Epoch 00018: val_loss did not improve from 1.01831\n",
      "14153/14153 [==============================] - 3s 186us/sample - loss: 1.0078 - val_loss: 1.0206\n",
      "Epoch 19/100\n",
      "14048/14153 [============================>.] - ETA: 0s - loss: 0.9955\n",
      "Epoch 00019: val_loss did not improve from 1.01831\n",
      "14153/14153 [==============================] - 3s 191us/sample - loss: 0.9953 - val_loss: 1.0236\n",
      "Epoch 20/100\n",
      "14016/14153 [============================>.] - ETA: 0s - loss: 0.9916\n",
      "Epoch 00020: val_loss improved from 1.01831 to 1.01174, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 196us/sample - loss: 0.9921 - val_loss: 1.0117\n",
      "Epoch 21/100\n",
      "13984/14153 [============================>.] - ETA: 0s - loss: 0.9675\n",
      "Epoch 00021: val_loss did not improve from 1.01174\n",
      "14153/14153 [==============================] - 3s 187us/sample - loss: 0.9701 - val_loss: 1.0244\n",
      "Epoch 22/100\n",
      "13952/14153 [============================>.] - ETA: 0s - loss: 0.9625\n",
      "Epoch 00022: val_loss did not improve from 1.01174\n",
      "14153/14153 [==============================] - 3s 188us/sample - loss: 0.9635 - val_loss: 1.0275\n",
      "Epoch 23/100\n",
      "14144/14153 [============================>.] - ETA: 0s - loss: 0.9627\n",
      "Epoch 00023: val_loss improved from 1.01174 to 1.01060, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 194us/sample - loss: 0.9624 - val_loss: 1.0106\n",
      "Epoch 24/100\n",
      "14080/14153 [============================>.] - ETA: 0s - loss: 0.9488\n",
      "Epoch 00024: val_loss did not improve from 1.01060\n",
      "14153/14153 [==============================] - 3s 184us/sample - loss: 0.9496 - val_loss: 1.0257\n",
      "Epoch 25/100\n",
      "13984/14153 [============================>.] - ETA: 0s - loss: 0.9453\n",
      "Epoch 00025: val_loss did not improve from 1.01060\n",
      "14153/14153 [==============================] - 3s 188us/sample - loss: 0.9449 - val_loss: 1.0112\n",
      "Epoch 26/100\n",
      "13984/14153 [============================>.] - ETA: 0s - loss: 0.9374\n",
      "Epoch 00026: val_loss did not improve from 1.01060\n",
      "14153/14153 [==============================] - 3s 192us/sample - loss: 0.9390 - val_loss: 1.0221\n",
      "Epoch 27/100\n",
      "14016/14153 [============================>.] - ETA: 0s - loss: 0.9275\n",
      "Epoch 00027: val_loss improved from 1.01060 to 1.00814, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 188us/sample - loss: 0.9273 - val_loss: 1.0081\n",
      "Epoch 28/100\n",
      "13888/14153 [============================>.] - ETA: 0s - loss: 0.9204\n",
      "Epoch 00028: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 188us/sample - loss: 0.9182 - val_loss: 1.0183\n",
      "Epoch 29/100\n",
      "13920/14153 [============================>.] - ETA: 0s - loss: 0.9131\n",
      "Epoch 00029: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 190us/sample - loss: 0.9138 - val_loss: 1.0191\n",
      "Epoch 30/100\n",
      "13984/14153 [============================>.] - ETA: 0s - loss: 0.9076\n",
      "Epoch 00030: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 193us/sample - loss: 0.9072 - val_loss: 1.0307\n",
      "Epoch 31/100\n",
      "13856/14153 [============================>.] - ETA: 0s - loss: 0.8947\n",
      "Epoch 00031: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 191us/sample - loss: 0.8928 - val_loss: 1.0204\n",
      "Epoch 32/100\n",
      "14112/14153 [============================>.] - ETA: 0s - loss: 0.8987\n",
      "Epoch 00032: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 188us/sample - loss: 0.8991 - val_loss: 1.0168\n",
      "Epoch 33/100\n",
      "14016/14153 [============================>.] - ETA: 0s - loss: 0.8793\n",
      "Epoch 00033: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 197us/sample - loss: 0.8797 - val_loss: 1.0315\n",
      "Epoch 34/100\n",
      "13984/14153 [============================>.] - ETA: 0s - loss: 0.8823\n",
      "Epoch 00034: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 187us/sample - loss: 0.8802 - val_loss: 1.0203\n",
      "Epoch 35/100\n",
      "14112/14153 [============================>.] - ETA: 0s - loss: 0.8673\n",
      "Epoch 00035: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 186us/sample - loss: 0.8671 - val_loss: 1.0470\n",
      "Epoch 36/100\n",
      "14144/14153 [============================>.] - ETA: 0s - loss: 0.8524\n",
      "Epoch 00036: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 190us/sample - loss: 0.8524 - val_loss: 1.0405\n",
      "Epoch 37/100\n",
      "13952/14153 [============================>.] - ETA: 0s - loss: 0.8514\n",
      "Epoch 00037: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 188us/sample - loss: 0.8518 - val_loss: 1.0122\n",
      "Epoch 38/100\n",
      "14144/14153 [============================>.] - ETA: 0s - loss: 0.8510\n",
      "Epoch 00038: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 185us/sample - loss: 0.8507 - val_loss: 1.0383\n",
      "Epoch 39/100\n",
      "14112/14153 [============================>.] - ETA: 0s - loss: 0.8383\n",
      "Epoch 00039: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 184us/sample - loss: 0.8386 - val_loss: 1.0346\n",
      "Epoch 40/100\n",
      "14080/14153 [============================>.] - ETA: 0s - loss: 0.8293\n",
      "Epoch 00040: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 185us/sample - loss: 0.8292 - val_loss: 1.0330\n",
      "Epoch 41/100\n",
      "14112/14153 [============================>.] - ETA: 0s - loss: 0.8289\n",
      "Epoch 00041: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 185us/sample - loss: 0.8281 - val_loss: 1.0537\n",
      "Epoch 42/100\n",
      "13984/14153 [============================>.] - ETA: 0s - loss: 0.8144\n",
      "Epoch 00042: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 188us/sample - loss: 0.8140 - val_loss: 1.0498\n",
      "Epoch 43/100\n",
      "13952/14153 [============================>.] - ETA: 0s - loss: 0.8174\n",
      "Epoch 00043: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 186us/sample - loss: 0.8172 - val_loss: 1.0536\n",
      "Epoch 44/100\n",
      "13856/14153 [============================>.] - ETA: 0s - loss: 0.8171\n",
      "Epoch 00044: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 183us/sample - loss: 0.8179 - val_loss: 1.0518\n",
      "Epoch 45/100\n",
      "14080/14153 [============================>.] - ETA: 0s - loss: 0.8070\n",
      "Epoch 00045: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 186us/sample - loss: 0.8075 - val_loss: 1.0454\n",
      "Epoch 46/100\n",
      "13952/14153 [============================>.] - ETA: 0s - loss: 0.8067\n",
      "Epoch 00046: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 187us/sample - loss: 0.8080 - val_loss: 1.0716\n",
      "Epoch 47/100\n",
      "14144/14153 [============================>.] - ETA: 0s - loss: 0.7979\n",
      "Epoch 00047: val_loss did not improve from 1.00814\n",
      "14153/14153 [==============================] - 3s 184us/sample - loss: 0.7980 - val_loss: 1.0349\n",
      "Partial score of fold 4 is: [0.47351095]\n",
      "Our oof cohen kappa score is:  0.47542251448992634\n"
     ]
    }
   ],
   "source": [
    "nn_model = Nn_Model(reduce_train, ajusted_test, features, {}, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data:  0.47542251448992634\n"
     ]
    }
   ],
   "source": [
    "nn_train_pred = nn_model.oof_pred\n",
    "print('Accuracy on training data: ', qwk3(reduce_train['accuracy_group'], nn_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.976487\tvalid_1's rmse: 1.00548\n",
      "[200]\ttraining's rmse: 0.919112\tvalid_1's rmse: 0.977682\n",
      "[300]\ttraining's rmse: 0.884238\tvalid_1's rmse: 0.970223\n",
      "[400]\ttraining's rmse: 0.855904\tvalid_1's rmse: 0.967784\n",
      "[500]\ttraining's rmse: 0.831457\tvalid_1's rmse: 0.966618\n",
      "[600]\ttraining's rmse: 0.809202\tvalid_1's rmse: 0.966159\n",
      "[700]\ttraining's rmse: 0.789133\tvalid_1's rmse: 0.966233\n",
      "Early stopping, best iteration is:\n",
      "[621]\ttraining's rmse: 0.804807\tvalid_1's rmse: 0.966064\n",
      "Partial score of fold 0 is: 0.48229140401280723\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.974967\tvalid_1's rmse: 1.01076\n",
      "[200]\ttraining's rmse: 0.916861\tvalid_1's rmse: 0.987067\n",
      "[300]\ttraining's rmse: 0.881238\tvalid_1's rmse: 0.982396\n",
      "[400]\ttraining's rmse: 0.853414\tvalid_1's rmse: 0.980568\n",
      "[500]\ttraining's rmse: 0.829455\tvalid_1's rmse: 0.980077\n",
      "[600]\ttraining's rmse: 0.808354\tvalid_1's rmse: 0.980139\n",
      "Early stopping, best iteration is:\n",
      "[562]\ttraining's rmse: 0.816324\tvalid_1's rmse: 0.979601\n",
      "Partial score of fold 1 is: 0.47117684084152567\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.975298\tvalid_1's rmse: 1.01256\n",
      "[200]\ttraining's rmse: 0.918069\tvalid_1's rmse: 0.986424\n",
      "[300]\ttraining's rmse: 0.882844\tvalid_1's rmse: 0.980657\n",
      "[400]\ttraining's rmse: 0.85473\tvalid_1's rmse: 0.978576\n",
      "[500]\ttraining's rmse: 0.830941\tvalid_1's rmse: 0.977176\n",
      "[600]\ttraining's rmse: 0.809259\tvalid_1's rmse: 0.97639\n",
      "Early stopping, best iteration is:\n",
      "[569]\ttraining's rmse: 0.815902\tvalid_1's rmse: 0.976333\n",
      "Partial score of fold 2 is: 0.4778147250549062\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.975278\tvalid_1's rmse: 1.01174\n",
      "[200]\ttraining's rmse: 0.916604\tvalid_1's rmse: 0.986198\n",
      "[300]\ttraining's rmse: 0.880877\tvalid_1's rmse: 0.981016\n",
      "[400]\ttraining's rmse: 0.853275\tvalid_1's rmse: 0.979647\n",
      "[500]\ttraining's rmse: 0.829028\tvalid_1's rmse: 0.979085\n",
      "[600]\ttraining's rmse: 0.806794\tvalid_1's rmse: 0.978747\n",
      "[700]\ttraining's rmse: 0.787115\tvalid_1's rmse: 0.978786\n",
      "Early stopping, best iteration is:\n",
      "[658]\ttraining's rmse: 0.795205\tvalid_1's rmse: 0.978583\n",
      "Partial score of fold 3 is: 0.4803942113004269\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.976495\tvalid_1's rmse: 1.01545\n",
      "[200]\ttraining's rmse: 0.91821\tvalid_1's rmse: 0.989176\n",
      "[300]\ttraining's rmse: 0.88276\tvalid_1's rmse: 0.981539\n",
      "[400]\ttraining's rmse: 0.85507\tvalid_1's rmse: 0.979307\n",
      "[500]\ttraining's rmse: 0.830808\tvalid_1's rmse: 0.97851\n",
      "[600]\ttraining's rmse: 0.809599\tvalid_1's rmse: 0.978085\n",
      "[700]\ttraining's rmse: 0.790081\tvalid_1's rmse: 0.97785\n",
      "Early stopping, best iteration is:\n",
      "[645]\ttraining's rmse: 0.80049\tvalid_1's rmse: 0.977658\n",
      "Partial score of fold 4 is: 0.4670205261157443\n",
      "Our oof cohen kappa score is:  0.47571855331164636\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "params = {'n_estimators': int(5055.501312496299),\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'subsample': 0.6367826323790282,\n",
    "            'subsample_freq': int(0.2658232429370657),\n",
    "            'learning_rate': 0.017195259555759495,\n",
    "            'feature_fraction': 0.8494137043130827,\n",
    "            'max_depth': int(19.86367634095852),\n",
    "            'lambda_l1': 1.9348461159493258,  \n",
    "            'lambda_l2':1.33218451609903384,\n",
    "            'early_stopping_rounds': 100\n",
    "            }\n",
    "\n",
    "lgb_model = Lgb_Model(reduce_train, ajusted_test, features, params, categoricals=categoricals)\n",
    "# 0.6160951388044382"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data:  0.47571855331164636\n"
     ]
    }
   ],
   "source": [
    "lgb_train_pred = lgb_model.oof_pred\n",
    "print('Accuracy on training data: ', qwk3(reduce_train['accuracy_group'], lgb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_weights(rf, knn, xgb, cat, cnn, nn, lgb):\n",
    "    weights = {'rf': rf, 'knn': knn, 'xgb': xgb, 'cat': cat, 'cnn': cnn, 'nn': nn, 'lgb': lgb}\n",
    "\n",
    "    final_pred = (rf_train_pred * weights['rf']) + \\\n",
    "        (knn_train_pred * weights['knn']) + \\\n",
    "        (xgb_train_pred * weights['xgb']) + \\\n",
    "        (cat_train_pred * weights['cat']) + \\\n",
    "        (cnn_train_pred * weights['cnn']) + \\\n",
    "        (nn_train_pred * weights['nn']) + \\\n",
    "        (lgb_train_pred * weights['lgb'])\n",
    "\n",
    "    return cohen_kappa_score(reduce_train['accuracy_group'],final_pred.astype(int), weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_points = 100\n",
    "n_iter = 200\n",
    "\n",
    "bounds_weights = {'rf': (0, 1), \n",
    "           'knn': (0, 1),  \n",
    "           'xgb': (0, 1), \n",
    "           'cat': (0, 1),  \n",
    "           'cnn': (0, 1),\n",
    "           'nn': (0, 1),\n",
    "           'lgb': (0, 1)\n",
    "          }\n",
    "\n",
    "#Weights_BO = BayesianOptimization(find_weights, bounds_weights, random_state=1029)\n",
    "\n",
    "#with warnings.catch_warnings():\n",
    "#    warnings.filterwarnings('ignore')\n",
    "#    Weights_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Weights_BO.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cappa score: 0.6088190350056795\n",
      "Accuracy: 0.526455624646693\n"
     ]
    }
   ],
   "source": [
    "weights = {'cat': 0.6319722842278973, 'cnn': 0.1481760339510487, 'knn': 0.0039029911977264383, 'lgb': 0.003848860524615473, 'nn': 0.26120804791868485, 'rf': 0.2107114405167529, 'xgb': 0.006934110357386136}\n",
    "\n",
    "final_pred = (rf_train_pred * weights['rf']) + \\\n",
    "    (knn_train_pred * weights['knn']) + \\\n",
    "    (xgb_train_pred * weights['xgb']) + \\\n",
    "    (cat_train_pred * weights['cat']) + \\\n",
    "    (cnn_train_pred * weights['cnn']) + \\\n",
    "    (nn_train_pred * weights['nn']) + \\\n",
    "    (lgb_train_pred * weights['lgb'])\n",
    "\n",
    "print('Cappa score:', cohen_kappa_score(reduce_train['accuracy_group'],final_pred.astype(int), weights='quadratic'))\n",
    "print('Accuracy:', accuracy_score(reduce_train['accuracy_group'],final_pred.astype(int)))\n",
    "\n",
    "# 0.6123981493146219 Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make final predictions\n",
    "rf_test_pred = rf_model.y_pred\n",
    "knn_test_pred = knn_model.y_pred\n",
    "xgb_test_pred = xgb_model.y_pred\n",
    "cat_test_pred = cat_model.y_pred\n",
    "cnn_test_pred =cnn_model.y_pred\n",
    "nn_test_pred = nn_model.y_pred\n",
    "lgb_test_pred = lgb_model.y_pred\n",
    "\n",
    "final_pred = (rf_test_pred * weights['rf']) + \\\n",
    "    (knn_test_pred * weights['knn']) + \\\n",
    "    (xgb_test_pred * weights['xgb']) + \\\n",
    "    (cat_test_pred * weights['cat']) + \\\n",
    "    (nn_test_pred * weights['cnn']) + \\\n",
    "    (nn_test_pred * weights['nn']) + \\\n",
    "    (lgb_test_pred * weights['lgb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    0.500\n",
       "0    0.239\n",
       "1    0.136\n",
       "2    0.125\n",
       "Name: accuracy_group, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['installation_id'] = ajusted_test['installation_id']\n",
    "submission['accuracy_group'] = get_class_pred(final_pred, reduce_train)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission['accuracy_group'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0418b7276c89483d960ede9ffb0716b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "16593266151e4e27987840c83b77785e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1ac839e91a814c7fb8b1061772761f13": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "1b82c06802f04cc2ab50d201e4cf7bee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_62642a49cc9041478984600ab07c7678",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4510a49a48aa4551af618630d966b818",
       "value": 1000
      }
     },
     "25ee2fa878b84264ae4346dddf5fc90a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bc319cad79574db6a96df9c505dae406",
       "placeholder": "​",
       "style": "IPY_MODEL_0418b7276c89483d960ede9ffb0716b3",
       "value": " 3614/3614 [07:19&lt;00:00,  8.22it/s]"
      }
     },
     "2b4267f19b9e4af58a5ed93534a67921": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1b82c06802f04cc2ab50d201e4cf7bee",
        "IPY_MODEL_93f65d03e2214d3185e93bcd91bb74c1"
       ],
       "layout": "IPY_MODEL_bafa5a4c2e4c4d89ba06463a1f105f00"
      }
     },
     "3deb51edc1a74972aed7aa02f5124095": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cc3d7bf9a72040499fdaacef4d3f1fd8",
       "max": 3614,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1ac839e91a814c7fb8b1061772761f13",
       "value": 3614
      }
     },
     "3e44517fb04a47d089074e970f3c8ccb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4510a49a48aa4551af618630d966b818": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "62642a49cc9041478984600ab07c7678": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "93f65d03e2214d3185e93bcd91bb74c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f1b03424cbe04edcb5db80533f6ea2c3",
       "placeholder": "​",
       "style": "IPY_MODEL_16593266151e4e27987840c83b77785e",
       "value": " 1000/1000 [01:07&lt;00:00, 14.88it/s]"
      }
     },
     "9fd2398a2d5c49c89919c9a8b5fe0312": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3deb51edc1a74972aed7aa02f5124095",
        "IPY_MODEL_25ee2fa878b84264ae4346dddf5fc90a"
       ],
       "layout": "IPY_MODEL_3e44517fb04a47d089074e970f3c8ccb"
      }
     },
     "bafa5a4c2e4c4d89ba06463a1f105f00": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bc319cad79574db6a96df9c505dae406": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cc3d7bf9a72040499fdaacef4d3f1fd8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f1b03424cbe04edcb5db80533f6ea2c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
